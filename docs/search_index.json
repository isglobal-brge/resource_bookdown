[
["index.html", "Orchestrating privacy-protected big data analyses of data from different resources with R and DataSHIELD Welcome", " Orchestrating privacy-protected big data analyses of data from different resources with R and DataSHIELD 2021-01-19 Welcome This is the website for a book that provides users with some common workflows for the non-disclosive analysis of biomedical data with R and DataSHIELD from different resources. This book will teach you how to use the resourcer R package to perform any statistcal analysis from different studies having data in different formats (e.g., CSV, SPSS, R class, …). In particular, we focus on illustrating how to deal with Big Data by providing several examples from omic and geographical settings. To this end, we use cutting-edge Bioconductor tools to perform transcriptomic, epigenomic and genomic data analyses. Serveral R packages are used to perform analysis of geospatial data. We also provide examples of how performing non-disclosive analyses using secure SHELL commands by allowing the use of specific software that properly deals with Big Data outside R. This material serves as an online companion for the manuscript “Orchestrating non-disclosive big data analyses of shared data from different resources with R and DataSHIELD”. While we focus here in genomic and geospatial data, dozens of data analysis aplications interested in performing non-disclosive analysis having data in any specific format could be carried out. By learning the grammar of DataSHIELD workflows, we hope to provide you a starting point for the exploration of your own data, whether it be omic, geospatial or otherwise. This book is organized into five parts. In the Preamble, we introduce the book, provides a tutorial for key data infrastructure useful for omic and geospatial data and a general overview for learning how Opal and DataSHIELD allows performing non-disclosive data analyses from multiple studies simultaneously. So far, DataSHIELD uses tables from repository data in Opal which have some limitations to perfom Big Data analyses. The second part, Focus Topic, dive into information for non-disclosive analyses using any type of resource which is one of the key advances provided in this work. It allow, among others, perform big data analyses using genomic or geospatial information where thousand of sensitive data have to be managed. The third part, Resources Extensions, provides examples illustrating how to extend existing resources. We describe how to create functions to deal with omic data in Variant Calling Format (VCF files) which specifies the format of a text file used in bioinformatics for storing gene sequence variations. It also shows how to perform genomic data analysis using secure shell commands. This can be consider as an illustrative example for using shell programs to perform Big Data analyses that could be extend to other frameworks of Big Data such as “Apache Spark”. The fourth part, Workflows, provides primarily code detailing the analysis of various datasets throughout the book. Initially, we provide examples from transcriptomic, epigenomic and genomic association studies, as well as examples linking geospatial data where data confidentiality is an important issue. Exposome data analysis has also been added as another example of using resources. The fifth part, Developers, provides information about how to develop DataSHIELD packages using specific R packages devoted to Big Data analyses as well as using shell programs. It also provides some useful trips and trick that developers or end users may face when using our proposed infrastructure. Technical questions raised by one of the reviewers when submitting the paper to Plos Comp Biol are also available in another chapter. Finally, the Appendix highlights our contributors. "],
["citation.html", "Citation", " Citation If you would like to cite this work, please use the reference “Yannick Marcon, Tom Bishop, Demetris Avraam, Xavier Escribà-Montagut, Patricia Ryser-Welch, Stuart Wheater, Paul Burton, Juan R González. Orchestrating privacy-protected big data analyses of data from different resources with R and DataSHIELD. Plos Comp Biol. 2020”. "],
["introduction.html", "1 Introduction 1.1 What you will learn 1.2 Preliminaries 1.3 Acknowledgments", " 1 Introduction 1.1 What you will learn The goal of this book is to provide a solid foundation to non-disclosive data analysis using R and DataSHIELD/Opal through the resourcer R package. We illustrate how to preform such data analyses in two settings (’omics and geospatial) where the use of resources allows users to handle Big Data problems. We also present workflows of how to perform statistical analyses using data in formats other than simple tables. We aim to tackle key concepts covered in the manuscript, “Orchestrating non-disclosive big data analyses of shared data from different resources with R and DataSHIELD”, with each workflow covering these in varying detail, as well as essential preliminaries that are important for following along with the workflows on your own. 1.2 Preliminaries For those unfamiliar with R (and those looking to learn more), in the R section we provide some links to R books. Also there are several R on-line courses such as this one at DataCamp or this at CodeAcademy that can help to introduce the main R concepts. Nonetheless, we assume that the readers of this book are already familiar with R. For those interested in ’omic data analyses we recommend the book Omic association analysis with R and Bioconductor which provides a global overview of how to perform genomic, transcriptomic, epigenomic and multi-omic data analyses using R and Bioconductor packages. Bioconductor support provides the primary way to contact both Bioconductor developers and users and it is a great way to search for answers to your questions. Bioconductor courses provide excellent material to learn most of the Bioconductor basics as well as other advanced methods and R related topics. [Omic data infrastructure][Bioconductor data infrastructures] deserves a mention here since understanding common data containers is an essential part of Bioconductor workflows that are used in our DataSHIELD packages designed for ’omic data analyses. This enables interoperability across packages, allowing for “plug and play” usage of cutting-edge methods. Geospatial data also requires specific data managment that is described the [Geospatial Workflow] {#GIS} 1.3 Acknowledgments Firstly, we would like to thank OBiBa and DataSHIELD developers for providing such an impressive framework for non-disclosive data analysis. We would also like to thank all Bioconductor contributors for giving access to their packages for dealing with different omic and performing state-of-the-art data analyses. This has allowed us to create DataSHIELD packages easily without the need to re-program most of the ’omic association analyses. We also thank R package developers since their work will allow the community to implement other DataSHIELD packages specifically designed to address other biomedical, epidemiological and social science problems as we did with geospatial data. Finally, we would like to thank the Bioconductor core team for inspiring us to write this book. We are happy to follow the two succesful works describing how to orchestrate High-Throughput Genomic Analysis and Single Cell Analysis with Bioconductor. "],
["learnR.html", "2 Learning R", " 2 Learning R R is a free, open-source software and programming language developed in 1995 at the University of Auckland as an environment for statistical computing and graphics (Ikaha and Gentleman, 1996). Since then R has become one of the dominant software environments for data analysis and is used by a variety of scientific disciplines, including biomedicine, environmental epidemiology, social sciences, ecology, genetics and geoinformatics among others. CRAN Tasks provides an excellent overview of existing R packages for a given discipline (see for instance Genetics Task View ; Envirometrics Task View; Spatial Task View). R offers numerous advantages, such as: Free and Open source Reproducible Research repeatable: code + output in a single document easier the re-analyses scalable: applicable to small or large datasets extensible: several Getting help Numerous Discipline Specific R Groups Numerous Local R User Groups (including R-Ladies Groups) Stack Overflow Learning Resources R books (Free Online) R Books While some people find the use of a command line environment daunting, it is becoming a necessary skill for scientists as the volume and variety of data has grown. Additionally, GUI interfaces can easily be implemented in R (see this review) being Shiny a widely used R package that makes it easy to build interactive web apps straight from R. "],
["BioC.html", "3 Bioconductor Data Structures 3.1 SNP Array Data 3.2 Expression Sets 3.3 Genomic Ranges 3.4 Summarized Experiments 3.5 Ranged Summarized Experiments 3.6 Multi Data Set", " 3 Bioconductor Data Structures Bioconductor promotes the statistical analysis and comprehension of current and emerging high-throughput biological assays. Bioconductor is based on packages written primarily in the R programming language. Bioconductor is committed to open source, collaborative, distributed software development and literate, reproducible research. Most Bioconductor components are distributed as R packages. The functional scope of Bioconductor packages includes the analysis of DNA microarray, sequence, flow, SNP, and other data. Bioconductor provides several data infrastructures for efficiently managing omic data. See this paper for a global overview. Here we provide a quick introduction for the most commonly used ones. We also recommend to learn how to deal with GenomicRanges which helps to efficiently manage genomic data information. 3.1 SNP Array Data SNP array data are normally stored in PLINK format (or VCF for NGS data). PLINK data are normally stored in three files .ped, .bim, .fam. The advantage is that SNP data are stored in binary format in the BED file (Homozygous normal 01, Heterozygous 02, Homozygous variant 03, missing 00). FAM file: one row per individual - identification information: Family ID, Individual ID Paternal ID, Maternal ID, Sex (1=male; 2=female; other=unknown), Phenotype. BIM file: one row per SNP (rs id, chromosome, position, allele 1, allele 2). BED file: one row per individual. Genotypes in columns. Data are easily loaded into R by using read.plink function require(snpStats) snps &lt;- read.plink(&quot;data/obesity&quot;) # there are three files obesity.fam, obesity.bim, obesity.bed names(snps) [1] &quot;genotypes&quot; &quot;fam&quot; &quot;map&quot; Genotypes is a snpMatrix object geno &lt;- snps$genotypes geno A SnpMatrix with 2312 rows and 100000 columns Row names: 100 ... 998 Col names: MitoC3993T ... rs28600179 Annotation is a data.frame object annotation &lt;- snps$map head(annotation) chromosome snp.name cM position allele.1 allele.2 MitoC3993T NA MitoC3993T NA 3993 T C MitoG4821A NA MitoG4821A NA 4821 A G MitoG6027A NA MitoG6027A NA 6027 A G MitoT6153C NA MitoT6153C NA 6153 C T MitoC7275T NA MitoC7275T NA 7275 T C MitoT9699C NA MitoT9699C NA 9699 C T 3.2 Expression Sets The ExpressionSet is a fundamental data container in Bioconductor Alt ExpressionSet Description Biobase is part of the Bioconductor project and contains standardized data structures to represent genomic data. The ExpressionSet class is designed to combine several different sources of information into a single convenient structure. An ExpressionSet can be manipulated (e.g., subsetted, copied) conveniently, and is the input or output from many Bioconductor functions. The data in an ExpressionSet consists of expression data from microarray experiments, `meta-data’ describing samples in the experiment, annotations and meta-data about the features on the chip and information related to the protocol used for processing each sample Print library(tweeDEseqCountData) data(pickrell) pickrell.eset ExpressionSet (storageMode: lockedEnvironment) assayData: 52580 features, 69 samples element names: exprs protocolData: none phenoData sampleNames: NA18486 NA18498 ... NA19257 (69 total) varLabels: num.tech.reps population study gender varMetadata: labelDescription featureData featureNames: ENSG00000000003 ENSG00000000005 ... LRG_99 (52580 total) fvarLabels: gene fvarMetadata: labelDescription experimentData: use &#39;experimentData(object)&#39; Annotation: Get experimental data (e.g., gene expression) genes &lt;- exprs(pickrell.eset) genes[1:4,1:4] NA18486 NA18498 NA18499 NA18501 ENSG00000000003 0 0 0 0 ENSG00000000005 0 0 0 0 ENSG00000000419 22 105 40 55 ENSG00000000457 22 100 107 53 Get phenotypic data (e.g. covariates, disease status, outcomes, …) pheno &lt;- pData(pickrell.eset) head(pheno) num.tech.reps population study gender NA18486 2 YRI Pickrell male NA18498 2 YRI Pickrell male NA18499 2 YRI Pickrell female NA18501 2 YRI Pickrell male NA18502 2 YRI Pickrell female NA18504 2 YRI Pickrell male pheno$gender [1] male male female male female male female male female male female male female male female [16] male female female male female male female female male female male female female male female [31] female male female male female female female female male female male male female female male [46] female female male female female male female male male female female male female male female [61] male female female male female female female male female Levels: female male This also works pickrell.eset$gender [1] male male female male female male female male female male female male female male female [16] male female female male female male female female male female male female female male female [31] female male female male female female female female male female male male female female male [46] female female male female female male female male male female female male female male female [61] male female female male female female female male female Levels: female male Subsetting (everything is synchronized) eSet.male &lt;- pickrell.eset[, pickrell.eset$gender==&quot;male&quot;] eSet.male ExpressionSet (storageMode: lockedEnvironment) assayData: 52580 features, 29 samples element names: exprs protocolData: none phenoData sampleNames: NA18486 NA18498 ... NA19239 (29 total) varLabels: num.tech.reps population study gender varMetadata: labelDescription featureData featureNames: ENSG00000000003 ENSG00000000005 ... LRG_99 (52580 total) fvarLabels: gene fvarMetadata: labelDescription experimentData: use &#39;experimentData(object)&#39; Annotation: Finally, the fData function gets the probes’ annotation in a data.frame. Let us first illustrate how to provide an annotation to an ExpressionSet object require(Homo.sapiens) geneSymbols &lt;- rownames(genes) annot &lt;- select(Homo.sapiens, geneSymbols, columns=c(&quot;TXCHROM&quot;, &quot;SYMBOL&quot;), keytype=&quot;ENSEMBL&quot;) annotation(pickrell.eset) &lt;- &quot;Homo.sapiens&quot; fData(pickrell.eset) &lt;- annot pickrell.eset ExpressionSet (storageMode: lockedEnvironment) assayData: 52580 features, 69 samples element names: exprs protocolData: none phenoData sampleNames: NA18486 NA18498 ... NA19257 (69 total) varLabels: num.tech.reps population study gender varMetadata: labelDescription featureData featureNames: 1 2 ... 59351 (59351 total) fvarLabels: ENSEMBL SYMBOL TXCHROM fvarMetadata: labelDescription experimentData: use &#39;experimentData(object)&#39; Annotation: Homo.sapiens probes &lt;- fData(pickrell.eset) head(probes) ENSEMBL SYMBOL TXCHROM 1 ENSG00000000003 TSPAN6 chrX 2 ENSG00000000005 TNMD chrX 3 ENSG00000000419 DPM1 chr20 4 ENSG00000000457 SCYL3 chr1 5 ENSG00000000460 C1orf112 chr1 6 ENSG00000000938 FGR chr1 3.3 Genomic Ranges The GenomicRanges package serves as the foundation for representing genomic locations within the Bioconductor project. GRanges(): genomic coordinates to represent annotations (exons, genes, regulatory marks, …) and data (called peaks, variants, aligned reads) GRangesList(): genomic coordinates grouped into list elements (e.g., paired-end reads; exons grouped by transcript) Operations intra-range: act on each range independently e.g., shift() inter-range: act on all ranges in a GRanges object or GRangesList element - e.g., reduce(); disjoin() between-range: act on two separate GRanges or GRangesList objects - e.g., findOverlaps(), nearest() gr &lt;- GRanges(&quot;chr1&quot;, IRanges(c(10, 20, 22), width=5), &quot;+&quot;) gr GRanges object with 3 ranges and 0 metadata columns: seqnames ranges strand &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; [1] chr1 10-14 + [2] chr1 20-24 + [3] chr1 22-26 + ------- seqinfo: 1 sequence from an unspecified genome; no seqlengths # shift move all intervals 3 base pair towards the end shift(gr, 3) GRanges object with 3 ranges and 0 metadata columns: seqnames ranges strand &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; [1] chr1 13-17 + [2] chr1 23-27 + [3] chr1 25-29 + ------- seqinfo: 1 sequence from an unspecified genome; no seqlengths # inter-range range(gr) GRanges object with 1 range and 0 metadata columns: seqnames ranges strand &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; [1] chr1 10-26 + ------- seqinfo: 1 sequence from an unspecified genome; no seqlengths # two Granges: knowing the intervals that overlap with a targeted region snps &lt;- GRanges(&quot;chr1&quot;, IRanges(c(11, 17), width=1)) snps GRanges object with 2 ranges and 0 metadata columns: seqnames ranges strand &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; [1] chr1 11 * [2] chr1 17 * ------- seqinfo: 1 sequence from an unspecified genome; no seqlengths gr.ov &lt;- findOverlaps(snps, gr) gr.ov Hits object with 1 hit and 0 metadata columns: queryHits subjectHits &lt;integer&gt; &lt;integer&gt; [1] 1 1 ------- queryLength: 2 / subjectLength: 3 # recover the overlapping intervals gr[subjectHits(gr.ov)] GRanges object with 1 range and 0 metadata columns: seqnames ranges strand &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; [1] chr1 10-14 + ------- seqinfo: 1 sequence from an unspecified genome; no seqlengths #coverage: summarizes the times each base is covered by an interval coverage(gr) RleList of length 1 $chr1 integer-Rle of length 26 with 6 runs Lengths: 9 5 5 2 3 2 Values : 0 1 0 1 2 1 # get counts countOverlaps(gr, snps) [1] 1 0 0 This table shows the common operations of GenomicRanges 3.4 Summarized Experiments The SummarizedExperiment container encapsulates one or more assays, each represented by a matrix-like object of numeric or other mode. The rows typically represent genomic ranges of interest and the columns represent samples. Alt SummarizedExperiment Comprehensive data structure that can be used to store expression and methylation data from microarrays or read counts from RNA-seq experiments, among others. Can contain slots for one or more omic datasets, feature annotation (e.g. genes, transcripts, SNPs, CpGs), individual phenotypes and experimental details, such as laboratory and experimental protocols. As in an ExpressionSet a SummarizedExperiment, the rows of omic data are features and columns are subjects. Coordinate feature x sample ‘assays’ with row (feature) and column (sample) descriptions. ‘assays’ (similar to ‘exprs’ in ExpressionSetobjects) can be any matrix-like object, including very large on-disk representations such as HDF5Array ‘assays’ are annotated using GenomicRanges It is being deprecated 3.5 Ranged Summarized Experiments SummarizedExperiment is extended to RangedSummarizedExperiment, a child class that contains the annotation data of the features in a GenomicRanges object. An example dataset, stored as a RangedSummarizedExperiment is available in the airway package. This data represents an RNA sequencing experiment. library(airway) data(airway) airway class: RangedSummarizedExperiment dim: 64102 8 metadata(1): &#39;&#39; assays(1): counts rownames(64102): ENSG00000000003 ENSG00000000005 ... LRG_98 LRG_99 rowData names(0): colnames(8): SRR1039508 SRR1039509 ... SRR1039520 SRR1039521 colData names(9): SampleName cell ... Sample BioSample Some aspects of the object are very similar to ExpressionSet, although with slightly different names and types. colData contains phenotype (sample) information, like pData for ExpressionSet. It returns a DataFrame instead of a data.frame: colData(airway) DataFrame with 8 rows and 9 columns SampleName cell dex albut Run avgLength Experiment Sample BioSample &lt;factor&gt; &lt;factor&gt; &lt;factor&gt; &lt;factor&gt; &lt;factor&gt; &lt;integer&gt; &lt;factor&gt; &lt;factor&gt; &lt;factor&gt; SRR1039508 GSM1275862 N61311 untrt untrt SRR1039508 126 SRX384345 SRS508568 SAMN02422669 SRR1039509 GSM1275863 N61311 trt untrt SRR1039509 126 SRX384346 SRS508567 SAMN02422675 SRR1039512 GSM1275866 N052611 untrt untrt SRR1039512 126 SRX384349 SRS508571 SAMN02422678 SRR1039513 GSM1275867 N052611 trt untrt SRR1039513 87 SRX384350 SRS508572 SAMN02422670 SRR1039516 GSM1275870 N080611 untrt untrt SRR1039516 120 SRX384353 SRS508575 SAMN02422682 SRR1039517 GSM1275871 N080611 trt untrt SRR1039517 126 SRX384354 SRS508576 SAMN02422673 SRR1039520 GSM1275874 N061011 untrt untrt SRR1039520 101 SRX384357 SRS508579 SAMN02422683 SRR1039521 GSM1275875 N061011 trt untrt SRR1039521 98 SRX384358 SRS508580 SAMN02422677 You can still use $ to get a particular column: airway$cell [1] N61311 N61311 N052611 N052611 N080611 N080611 N061011 N061011 Levels: N052611 N061011 N080611 N61311 The measurement data are accessed by assay and assays. A SummarizedExperiment can contain multiple measurement matrices (all of the same dimension). You get all of them by assays and you select a particular one by assay(OBJECT, NAME) where you can see the names when you print the object or by using assayNames. In this case there is a single matrix called counts: assayNames(airway) [1] &quot;counts&quot; assays(airway) List of length 1 names(1): counts head(assay(airway, &quot;counts&quot;)) SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 SRR1039521 ENSG00000000003 679 448 873 408 1138 1047 770 572 ENSG00000000005 0 0 0 0 0 0 0 0 ENSG00000000419 467 515 621 365 587 799 417 508 ENSG00000000457 260 211 263 164 245 331 233 229 ENSG00000000460 60 55 40 35 78 63 76 60 ENSG00000000938 0 0 2 0 1 0 0 0 Annotation is a GenomicRanges object rowRanges(airway) GRangesList object of length 64102: $ENSG00000000003 GRanges object with 17 ranges and 2 metadata columns: seqnames ranges strand | exon_id exon_name &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;integer&gt; &lt;character&gt; [1] X 99883667-99884983 - | 667145 ENSE00001459322 [2] X 99885756-99885863 - | 667146 ENSE00000868868 [3] X 99887482-99887565 - | 667147 ENSE00000401072 [4] X 99887538-99887565 - | 667148 ENSE00001849132 [5] X 99888402-99888536 - | 667149 ENSE00003554016 ... ... ... ... . ... ... [13] X 99890555-99890743 - | 667156 ENSE00003512331 [14] X 99891188-99891686 - | 667158 ENSE00001886883 [15] X 99891605-99891803 - | 667159 ENSE00001855382 [16] X 99891790-99892101 - | 667160 ENSE00001863395 [17] X 99894942-99894988 - | 667161 ENSE00001828996 ------- seqinfo: 722 sequences (1 circular) from an unspecified genome ... &lt;64101 more elements&gt; Subset for only rows (e.g. features) which are in the interval 1 to 1Mb of chromosome 1 roi &lt;- GRanges(seqnames=&quot;1&quot;, ranges=IRanges(start=1, end=1e6)) subsetByOverlaps(airway, roi) class: RangedSummarizedExperiment dim: 79 8 metadata(1): &#39;&#39; assays(1): counts rownames(79): ENSG00000177757 ENSG00000185097 ... ENSG00000272512 ENSG00000273443 rowData names(0): colnames(8): SRR1039508 SRR1039509 ... SRR1039520 SRR1039521 colData names(9): SampleName cell ... Sample BioSample 3.6 Multi Data Set MultiDataSet is designed for integrating multi omic datasets. Alt MultiDataSet Designed to encapsulate different types of datasets (including all classes in Bioconductor) It properly deals with non-complete cases situations Subsetting is easily performed in both: samples and features (using GenomicRanges) It allows to: – perform integration analysis with third party packages; – create new methods and functions for omic data integration; – encapsule new unimplemented data from any biological experiment MultiAssayExperiment is another infrastructure container (created by BioC developers) that can be used to manage multi-omic data "],
["opal.html", "4 Opal 4.1 Introduction 4.2 Data Management 4.3 Security 4.4 R Integration 4.5 Opal demo site", " 4 Opal 4.1 Introduction Opal is OBiBa’s core database application for epidemiological studies. Participant data, collected by questionnaires, medical instruments, sensors, administrative databases etc. can be integrated and stored in a central data repository under a uniform model. Opal is a web application that can import, process, copy data and has advanced features for cataloging the data (fully described, annotated and searchable data dictionaries) as recommended by the Maelstrom Research group at McGill University, Canada. Opal is typically used by a research center to analyze the data acquired from assessment centres. Its ultimate purpose is to achieve seamless data-sharing among epidemiological studies. Opal is the reference implementation of the DataSHIELD infrastructure. More information on Opal can be found in the Opal description on OBiBa. Opal provides the following main features: Use of MongoDB, Mysql, MariaDB and/or PostgreSQL as database software backends, Import of data from various file formats (CSV, SPSS, SAS, Stata etc.) and from SQL databases, Export of data to various file formats (CSV, SPSS, SAS, Stata etc.) and to SQL databases, Plugin architecture to extend import/export capabilities, for instance by connecting to data source software such as REDCap, LimeSurvey etc., Storage of data about any type of “entity”, such as subject, sample, geographic area, etc., Storage of data of any type (e.g., texts, numbers, geo-localisation, images, videos, etc.), Advanced authentication and authorization features, Reporting using R markdown, Data analysis plugins using R, Web services can be accessed using R, Python, Java, Javascript, DataSHIELD middleware reference implementation (configuration, access controls, R session management). 4.2 Data Management In Opal the data sets are represented by tables, which are grouped by projects. A table has variables (columns) and entity values (rows). Opal also has the concept of views, which are logical tables where the variables are derived from physical tables via scripts. The storage of the data and of the meta-data (data dictionaries) is managed in a database (for example, a SQL database such as MySQL, MariaDB or PostgreSQL, or a document-oriented database such as MongoDB). Detailed concepts and tutorials for tables can be found here: Variables and Data Identifiers Mappings Data Harmonization 4.3 Security All Opal operations are accessible through web services that require authentication and proper authorization. The permissions can be granted to a specific user or a group of users, can be applied to a project or to a table and have different levels: read-only meta-data (access to the data dictionary without access to the individual-level data), read-only, or write permissions. The programmatic authentication can make use of username/password credentials, token or 2-way SSL authentication methods. Opal can also integrate with the hosting institution’s users registry using the OpenID Connect standard. 4.4 R Integration Opal connects to a R server to perform different kind of operations: data import/export (using R packages), data analysis (by transfering data from Opal’s database into a R server session and using R packages). The R server is based on the Rserve R package. The user R sessions that are running in this R server are managed by Opal. This Opal/R integration works well for small to mid-size datasets (usually less than 10M data points). For bigger datasets, extracting and transferring data from the database to the R server is time, CPU and memory intensive. In this work, we will present a more flexible data description paradigm called resources that enables Opal to manage access to Big Data sets, complex data structures and computation units for analysis purpose, while still having the security and the analysis features provided by Opal. 4.5 Opal demo site We have set up an Opal demo site to illustrate how to perform some basic analyses using DataSHIELD as well as how to deal with different resources for ’omic data. The Opal server can be accessed with the credentials: username: administrator password: password In this figure we can see all the projects available. Opal demo site available projects This vignette will mainly make use of the resources available at RSRC project Resources available at Opal demo site of RSRC project In order to make the reader familiar with Opal we recommend visiting the Opal online documentation. "],
["datashield.html", "5 DataSHIELD 5.1 Introduction 5.2 DataSHIELD R Interface (DSI) 5.3 DataSHIELD/Opal Implementation 5.4 Demo", " 5 DataSHIELD 5.1 Introduction Some research projects require pooling data from several studies to obtain sample sizes large and diverse enough for detecting interactions. Unfortunately, important ethico-legal constraints often prevent or impede the sharing of individual-level data across multiple studies. DataSHIELD aims to address this issue. DataSHIELD is a method that enables advanced statistical analysis of individual-level data from several sources without actually pooling the data from these sources together. DataSHIELD facilitates important research in settings where: a co-analysis of individual-level data from several studies is scientifically necessary but governance restrictions prevent the release or sharing of some of the required data, and/or render data access unacceptably slow, equivalent governance concerns prevent or hinder access to a single dataset, a research group wishes to actively share the information held in its data with others but does not wish to cede control of the governance of those data and/or the intellectual property they represent by physically handing over the data themselves, a dataset which is to be remotely analysed, or included in a multi-study co-analysis, contains data objects (e.g. images) which are too large to be physically transferred to the analysis location. A typical DataSHIELD infrastructure (see Figure 5.1) is composed of one analysis node (the DataSHIELD client) connected to one or several data analysis nodes (the DataSHIELD servers). In each of these server nodes, there is an R server application which can only be accessed through a DataSHIELD-compliant middleware application. This middleware application acts as a broker for managing R server sessions in a multi-user environment, assigning data and launching analysis on the R server. The analysis execution environment is then fully controlled: users must be authenticated, must have the proper permissions to access the data of interest and can only perform some predefined assignment and aggregation operations. Importantly, the operations that are permitted are designed to prevent the user having access to individual data items while still allowing useful work to be done with the data. For example, users can fit a generalised linear model to a dataset and receive information about the model coefficients, but are not given the residuals, as these could be used to reconstruct the original data.The reference implementation of this DataSHIELD infrastructure is based on the Opal data repository. Figure 5.1: Typical DataSHIELD infrastructure, including one central analysis node (client) and several data nodes (servers). The client node interacts programmatically in R with the server nodes using the DataSHIELD Interface implemented as the DSI R package. The DSI defines prototype functions to authenticate the user and to perform assignment and aggregation operations in each of the R servers sitting in the server nodes. The reference implementation of DSI is the DSOpal R package. An alternate implementation of DSI is DSLite, an R package targetted at DataSHIELD developers by offering a lightweight, pure R implementation of the whole DataSHIELD infrastructure. 5.2 DataSHIELD R Interface (DSI) The DataSHIELD Interface (DSI) defines a set of S4 classes and generic methods that can be implemented for accessing a data repository supporting the DataSHIELD infrastructure: controlled R commands to be executed on the server side that ensure only non disclosive information is returned to client side. 5.2.1 Class Structures The DSI S4 classes are: Class Description DSObject A common base class for all DSI, DSDriver A class to drive the creation of a connection object, DSConnection Allows the interaction with the remote server; DataSHIELD operations such as aggregation and assignment return a result object; DataSHIELD setup status check can be performed (dataset access, configuration comparision), DSResult Wraps access to the result, which can be fetched either synchronously or asynchronously depending on the capabilities of the data repository server. All classes are virtual: they cannot be instantiated directly and instead must be subclassed. See DSOpal for a reference implementation of DSI based on the Opal data repository. These S4 classes and generic methods are meant to be used for implementing connections to a DataSHIELD-aware data repository. 5.2.2 Higher Level Functions In addition to these S4 classes, DSI provides functions to handle a list of remote data repository servers: Functions Description datashield.login Create DSConnection objects for the data repositories, using the DSDriver specification. datashield.logout Destroy the DSConnections objects. datashield.aggregate, datashield.assign Typical DataSHIELD operations on DSConnection objects; results can be fetched through DSResult objects. datashield.connections, datashield.connections_default, datashield.connections_find Management of the list of DSConnection objects that can be discovered and used by the client-side analytic functions. datashield.workspaces, datashield.workspace_save, datashield.workspace_rm Manage R images of the remote DataSHIELD sessions (to speed up restoration of data analysis sessions). datashield.symbols, datashield.symbol_rm Minimalistic management of the R symbols living in the remote DataSHIELD sessions. datashield.tables, datashield.table_status List the tables and their accessibility across a set of data repositories. datashield.resources, datashield.resource_status List the resources and their accessibility across a set of data repositories. datashield.pkg_status, datashield.method_status, datashield.methods Utility functions to explore the DataSHIELD setup across a set of data repositories. These datashield.* functions are meant to be used by DataSHIELD packages developers and users. 5.2.3 Options Some options can be set to modify the behavior of the DSI: Option Description datashield.env The R environment in which the DSConnection object list is to be looked for. Default value is the Global Environment: globalenv(). datashield.progress A logical to enable visibility of progress bars. Default value is TRUE. datashield.progress.clear A logical to make the progress bar disappear after it has been completed. Default value is FALSE. datashield.error.stop A logical to alter error handling behavior: if TRUE an error is raised when at least one server has failed, otherwise a warning message is issued. Default value is TRUE. 5.3 DataSHIELD/Opal Implementation Opal is a web application that is accessible through web services. It implements DataSHIELD methods thanks to the following built-in features: integration with an R server, where the DataSHIELD operations will take place, secure data management, with fine-grained permissions (to restrict access to individual level data), web services API, that allows Opal operations to be run from an R script. In addition to these features, Opal manages the DataSHIELD configuration which consists of declaring the set of permitted aggregation/assignment R functions and some R options. 5.3.1 Client The opalr R package is a general purpose Opal connection R library (authentication is required) that is used to perform various operations (authorization may be required). The DSOpal R package is an implementation of the DSI, built on top of opalr. All the DataSHIELD operations are transparently applied to one or more Opal server using the DSI higher-level functions. Opal also supports asynchronous function calls (submission of a R operation, then later retrieval of the result) which allows operations on several DataSHIELD analysis nodes in parallel. 5.3.2 Server On the R server managed by Opal, some DataSHIELD-compliant R packages can be managed using the Opal’s web interface: installation, removal of DataSHIELD-compliant R packages and automatic DataSHIELD configuration discovery. Opal guarantees that only the allowed functions can be called. The DataSHIELD-compliant R package guarantees that only aggregated results are returned to the client. The term ‘aggregated’ here means that the data in the R server will go through a function that summarizes individual-level data into a non-disclosive form. For example, obtaining the length of a vector, or obtaining the summary statistics of a vector (min, max, mean, etc.). These DataSHIELD functions are customisable. That is, administrators of the Opal server can add, remove, modify or create completely custom aggregating methods that are proposed to DataSHIELD clients. Figure 5.2: DataSHIELD configuration in Opal When performing a DataSHIELD analysis session, a typical workflow on a single Opal analysis node is the following: authentication of the user (requires authorization to use DataSHIELD service), creation and initialization of an R server session, assignment of Opal-managed data into the R server session (requires data access authorization), processing incoming R operation requests (aggregation and assignment function calls requires authorization) that are forwarded to the R server session; non-disclosive aggregated result is then returned to the R client. 5.4 Demo Readers can read the DataSHIELD page in Opal documentation to have a global overview about how to use DataSHIELD functions. It describes how to perform basic statistical analyses, linear and generalized linear models and some data visualization. A complete description of how DataSHIELD works, with lots of materials, examples, courses and real data analyses can be obtained in the DataSHIELD Wiki. The following is a simple illustration of how to analyze some data available in the Opal demo site. The Projects page gives access to the different projects avaialble in this Opal server. If we select the SURVIVAL project we see that there are three tables: Figure 5.3: Tables available in the SURVIVAL project from our Opal example (https://opal-demo.obiba.org/) First we build a connection object with user credentials and the location of the server for each of the study: library(DSOpal) builder &lt;- newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;) builder$append(server = &quot;study2&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;) builder$append(server = &quot;study3&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;) logindata &lt;- builder$build() logindata server url table resource driver user password token options 1 study1 https://opal-demo.obiba.org OpalDriver dsuser password 2 study2 https://opal-demo.obiba.org OpalDriver dsuser password 3 study3 https://opal-demo.obiba.org OpalDriver dsuser password Then we perform login into each of the analysis servers and assign for each of them a different table with the same R symbol name: conns &lt;- datashield.login(logindata) datashield.assign.table(conns, symbol = &quot;D&quot;, table = list(study1 = &quot;SURVIVAL.EXPAND_WITH_MISSING1&quot;, study2 = &quot;SURVIVAL.EXPAND_WITH_MISSING2&quot;, study3 = &quot;SURVIVAL.EXPAND_WITH_MISSING3&quot;)) datashield.symbols(conns) $study1 [1] &quot;D&quot; $study2 [1] &quot;D&quot; $study3 [1] &quot;D&quot; Then we use the DataSHIELD functions from the dsBaseClient package to analyse the data and get non-disclosive summary statistics. The package should be first loaded library(dsBaseClient) Then, for example, we can get the column names and the dimension of the dataset from each study: ds.colnames(&quot;D&quot;) $study1 [1] &quot;id&quot; &quot;study.id&quot; &quot;time.id&quot; &quot;starttime&quot; &quot;endtime&quot; &quot;survtime&quot; &quot;cens&quot; &quot;age.60&quot; [9] &quot;female&quot; &quot;noise.56&quot; &quot;pm10.16&quot; &quot;bmi.26&quot; $study2 [1] &quot;id&quot; &quot;study.id&quot; &quot;time.id&quot; &quot;starttime&quot; &quot;endtime&quot; &quot;survtime&quot; &quot;cens&quot; &quot;age.60&quot; [9] &quot;female&quot; &quot;noise.56&quot; &quot;pm10.16&quot; &quot;bmi.26&quot; $study3 [1] &quot;id&quot; &quot;study.id&quot; &quot;time.id&quot; &quot;starttime&quot; &quot;endtime&quot; &quot;survtime&quot; &quot;cens&quot; &quot;age.60&quot; [9] &quot;female&quot; &quot;noise.56&quot; &quot;pm10.16&quot; &quot;bmi.26&quot; we can get the dimension of the datasetes: ds.dim(&quot;D&quot;) $`dimensions of D in study1` [1] 2060 12 $`dimensions of D in study2` [1] 1640 12 $`dimensions of D in study3` [1] 2688 12 $`dimensions of D in combined studies` [1] 6388 12 the summary of a variable which includes its class, length, quantiles and mean: ds.summary(&quot;D$age.60&quot;) $study1 $study1$class [1] &quot;numeric&quot; $study1$length [1] 2060 $study1$`quantiles &amp; mean` 5% 10% 25% 50% 75% 90% 95% Mean -27.000000 -21.000000 -13.000000 -3.000000 7.000000 15.000000 20.000000 -3.167883 $study2 $study2$class [1] &quot;numeric&quot; $study2$length [1] 1640 $study2$`quantiles &amp; mean` 5% 10% 25% 50% 75% 90% 95% Mean -27.000000 -22.000000 -15.000000 -4.000000 7.000000 14.000000 20.000000 -4.008637 $study3 $study3$class [1] &quot;numeric&quot; $study3$length [1] 2688 $study3$`quantiles &amp; mean` 5% 10% 25% 50% 75% 90% 95% Mean -25.700000 -21.000000 -13.000000 -3.000000 7.000000 14.000000 20.000000 -3.089989 we can get contigency table between categorical variables: ds.table(&quot;D$female&quot;, &quot;D$cens&quot;) Data in all studies were valid Study 1 : No errors reported from this study Study 2 : No errors reported from this study Study 3 : No errors reported from this study $output.list $output.list$TABLE.STUDY.1_row.props D$cens D$female 0 1 NA 0 0.726 0.261 0.01280 1 0.833 0.158 0.00901 NA 0.750 0.250 0.00000 $output.list$TABLE.STUDY.1_col.props D$cens D$female 0 1 NA 0 0.42000 0.57700 0.545 1 0.57300 0.41400 0.455 NA 0.00743 0.00946 0.000 $output.list$TABLE.STUDY.2_row.props D$cens D$female 0 1 NA 0 0.759 0.239 0.00286 1 0.860 0.139 0.00107 NA 0.800 0.200 0.00000 $output.list$TABLE.STUDY.2_col.props D$cens D$female 0 1 NA 0 0.39700 0.56000 0.667 1 0.60000 0.43600 0.333 NA 0.00299 0.00336 0.000 $output.list$TABLE.STUDY.3_row.props D$cens D$female 0 1 NA 0 0.714 0.274 0.01190 1 0.828 0.164 0.00802 NA 0.706 0.294 0.00000 $output.list$TABLE.STUDY.3_col.props D$cens D$female 0 1 NA 0 0.40100 0.56300 0.538 1 0.59300 0.42800 0.462 NA 0.00574 0.00874 0.000 $output.list$TABLES.COMBINED_all.sources_row.props D$cens D$female 0 1 NA 0 0.729 0.261 0.00997 1 0.838 0.155 0.00650 NA 0.737 0.263 0.00000 $output.list$TABLES.COMBINED_all.sources_col.props D$cens D$female 0 1 NA 0 0.40600 0.56700 0.549 1 0.58800 0.42500 0.451 NA 0.00555 0.00773 0.000 $output.list$TABLE_STUDY.1_counts D$cens D$female 0 1 NA 0 678 244 12 1 925 175 10 NA 12 4 0 $output.list$TABLE_STUDY.2_counts D$cens D$female 0 1 NA 0 531 167 2 1 804 130 1 NA 4 1 0 $output.list$TABLE_STUDY.3_counts D$cens D$female 0 1 NA 0 839 322 14 1 1239 245 12 NA 12 5 0 $output.list$TABLES.COMBINED_all.sources_counts D$cens D$female 0 1 NA 0 2048 733 28 1 2968 550 23 NA 28 10 0 $validity.message [1] &quot;Data in all studies were valid&quot; or we can fit a generalized linear model among many other analyses: ds.glm(formula=&quot;cens~female+bmi.26+pm10.16&quot;, data=&quot;D&quot;, family=&quot;binomial&quot;) $Nvalid [1] 6169 $Nmissing [1] 219 $Ntotal [1] 6388 $disclosure.risk RISK OF DISCLOSURE study1 0 study2 0 study3 0 $errorMessage ERROR MESSAGES study1 &quot;No errors&quot; study2 &quot;No errors&quot; study3 &quot;No errors&quot; $nsubs [1] 6169 $iter [1] 6 $family Family: binomial Link function: logit $formula [1] &quot;cens ~ female + bmi.26 + pm10.16&quot; $coefficients Estimate Std. Error z-value p-value low0.95CI.LP high0.95CI.LP P_OR low0.95CI.P_OR (Intercept) -0.9465551 0.046427292 -20.38790 2.141278e-92 -1.0375510 -0.8555593 0.2795781 0.2616228 female1 -0.7240154 0.067603806 -10.70968 9.167462e-27 -0.8565164 -0.5915143 0.4848017 0.4246388 bmi.26 0.1239566 0.007308207 16.96129 1.588245e-64 0.1096328 0.1382804 1.1319667 1.1158682 pm10.16 0.6620624 0.037059614 17.86479 2.217477e-71 0.5894269 0.7346979 1.9387867 1.8029548 high0.95CI.P_OR (Intercept) 0.2982680 female1 0.5534885 bmi.26 1.1482975 pm10.16 2.0848521 $dev [1] 5544.853 $df [1] 6165 $output.information [1] &quot;SEE TOP OF OUTPUT FOR INFORMATION ON MISSING DATA AND ERROR MESSAGES&quot; For a full list of the dsBaseClient functions you can use the following command: ds.listClientsideFunctions() ### Full search path [1] &quot;.GlobalEnv&quot; &quot;package:dsBaseClient&quot; [3] &quot;package:airway&quot; &quot;package:SummarizedExperiment&quot; [5] &quot;package:DelayedArray&quot; &quot;package:matrixStats&quot; [7] &quot;package:Homo.sapiens&quot; &quot;package:TxDb.Hsapiens.UCSC.hg19.knownGene&quot; [9] &quot;package:org.Hs.eg.db&quot; &quot;package:GO.db&quot; [11] &quot;package:OrganismDbi&quot; &quot;package:GenomicFeatures&quot; [13] &quot;package:AnnotationDbi&quot; &quot;package:tweeDEseqCountData&quot; [15] &quot;package:forcats&quot; &quot;package:stringr&quot; [17] &quot;package:dplyr&quot; &quot;package:purrr&quot; [19] &quot;package:readr&quot; &quot;package:tidyr&quot; [21] &quot;package:tibble&quot; &quot;package:ggplot2&quot; [23] &quot;package:tidyverse&quot; &quot;package:kableExtra&quot; [25] &quot;package:BiocStyle&quot; &quot;package:GenomicRanges&quot; [27] &quot;package:GenomeInfoDb&quot; &quot;package:IRanges&quot; [29] &quot;package:S4Vectors&quot; &quot;package:stats4&quot; [31] &quot;package:snpStats&quot; &quot;package:Matrix&quot; [33] &quot;package:survival&quot; &quot;package:Biobase&quot; [35] &quot;package:BiocGenerics&quot; &quot;package:parallel&quot; [37] &quot;package:DSOpal&quot; &quot;package:DSI&quot; [39] &quot;package:R6&quot; &quot;package:progress&quot; [41] &quot;package:opalr&quot; &quot;package:httr&quot; [43] &quot;tools:rstudio&quot; &quot;package:stats&quot; [45] &quot;package:graphics&quot; &quot;package:grDevices&quot; [47] &quot;package:utils&quot; &quot;package:datasets&quot; [49] &quot;package:methods&quot; &quot;Autoloads&quot; [51] &quot;package:base&quot; ### userDefinedClient functions [1] &quot;No clientside functions in this repository&quot; ### dsBaseClient functions [1] &quot;ds.asCharacter&quot; &quot;ds.asDataMatrix&quot; &quot;ds.asFactor&quot; [4] &quot;ds.asInteger&quot; &quot;ds.asList&quot; &quot;ds.asLogical&quot; [7] &quot;ds.asMatrix&quot; &quot;ds.asNumeric&quot; &quot;ds.assign&quot; [10] &quot;ds.Boole&quot; &quot;ds.c&quot; &quot;ds.cbind&quot; [13] &quot;ds.changeRefGroup&quot; &quot;ds.class&quot; &quot;ds.colnames&quot; [16] &quot;ds.completeCases&quot; &quot;ds.contourPlot&quot; &quot;ds.cor&quot; [19] &quot;ds.corTest&quot; &quot;ds.cov&quot; &quot;ds.dataFrame&quot; [22] &quot;ds.dataFrameFill&quot; &quot;ds.dataFrameSort&quot; &quot;ds.dataFrameSubset&quot; [25] &quot;ds.densityGrid&quot; &quot;ds.dim&quot; &quot;ds.exists&quot; [28] &quot;ds.exp&quot; &quot;ds.glm&quot; &quot;ds.glmerSLMA&quot; [31] &quot;ds.glmSLMA&quot; &quot;ds.heatmapPlot&quot; &quot;ds.histogram&quot; [34] &quot;ds.isNA&quot; &quot;ds.isValid&quot; &quot;ds.length&quot; [37] &quot;ds.levels&quot; &quot;ds.lexis&quot; &quot;ds.list&quot; [40] &quot;ds.listClientsideFunctions&quot; &quot;ds.listDisclosureSettings&quot; &quot;ds.listOpals&quot; [43] &quot;ds.listServersideFunctions&quot; &quot;ds.lmerSLMA&quot; &quot;ds.log&quot; [46] &quot;ds.look&quot; &quot;ds.ls&quot; &quot;ds.make&quot; [49] &quot;ds.matrix&quot; &quot;ds.matrixDet&quot; &quot;ds.matrixDet.report&quot; [52] &quot;ds.matrixDiag&quot; &quot;ds.matrixDimnames&quot; &quot;ds.matrixInvert&quot; [55] &quot;ds.matrixMult&quot; &quot;ds.matrixTranspose&quot; &quot;ds.mean&quot; [58] &quot;ds.meanByClass&quot; &quot;ds.meanSdGp&quot; &quot;ds.merge&quot; [61] &quot;ds.message&quot; &quot;ds.names&quot; &quot;ds.numNA&quot; [64] &quot;ds.quantileMean&quot; &quot;ds.rbind&quot; &quot;ds.rBinom&quot; [67] &quot;ds.recodeLevels&quot; &quot;ds.recodeValues&quot; &quot;ds.rep&quot; [70] &quot;ds.replaceNA&quot; &quot;ds.reShape&quot; &quot;ds.rm&quot; [73] &quot;ds.rNorm&quot; &quot;ds.rowColCalc&quot; &quot;ds.rPois&quot; [76] &quot;ds.rUnif&quot; &quot;ds.sample&quot; &quot;ds.scatterPlot&quot; [79] &quot;ds.seq&quot; &quot;ds.setDefaultOpals&quot; &quot;ds.setSeed&quot; [82] &quot;ds.subset&quot; &quot;ds.subsetByClass&quot; &quot;ds.summary&quot; [85] &quot;ds.table&quot; &quot;ds.table1D&quot; &quot;ds.table2D&quot; [88] &quot;ds.tapply&quot; &quot;ds.tapply.assign&quot; &quot;ds.testObjExists&quot; [91] &quot;ds.unList&quot; &quot;ds.var&quot; &quot;ds.vectorCalc&quot; If you cannot see one or more of the clientside functions you expected to find please see above for the full search path. If one of the paths is a possible clientside repository issue the R command ls(pos=&#39;package:dsPackageName&#39;) where &#39;package:dsPackageName&#39; is the full name stated in the search path [1] &quot;No clientside functions in this repository&quot; &quot;ds.asCharacter&quot; [3] &quot;ds.asDataMatrix&quot; &quot;ds.asFactor&quot; [5] &quot;ds.asInteger&quot; &quot;ds.asList&quot; [7] &quot;ds.asLogical&quot; &quot;ds.asMatrix&quot; [9] &quot;ds.asNumeric&quot; &quot;ds.assign&quot; [11] &quot;ds.Boole&quot; &quot;ds.c&quot; [13] &quot;ds.cbind&quot; &quot;ds.changeRefGroup&quot; [15] &quot;ds.class&quot; &quot;ds.colnames&quot; [17] &quot;ds.completeCases&quot; &quot;ds.contourPlot&quot; [19] &quot;ds.cor&quot; &quot;ds.corTest&quot; [21] &quot;ds.cov&quot; &quot;ds.dataFrame&quot; [23] &quot;ds.dataFrameFill&quot; &quot;ds.dataFrameSort&quot; [25] &quot;ds.dataFrameSubset&quot; &quot;ds.densityGrid&quot; [27] &quot;ds.dim&quot; &quot;ds.exists&quot; [29] &quot;ds.exp&quot; &quot;ds.glm&quot; [31] &quot;ds.glmerSLMA&quot; &quot;ds.glmSLMA&quot; [33] &quot;ds.heatmapPlot&quot; &quot;ds.histogram&quot; [35] &quot;ds.isNA&quot; &quot;ds.isValid&quot; [37] &quot;ds.length&quot; &quot;ds.levels&quot; [39] &quot;ds.lexis&quot; &quot;ds.list&quot; [41] &quot;ds.listClientsideFunctions&quot; &quot;ds.listDisclosureSettings&quot; [43] &quot;ds.listOpals&quot; &quot;ds.listServersideFunctions&quot; [45] &quot;ds.lmerSLMA&quot; &quot;ds.log&quot; [47] &quot;ds.look&quot; &quot;ds.ls&quot; [49] &quot;ds.make&quot; &quot;ds.matrix&quot; [51] &quot;ds.matrixDet&quot; &quot;ds.matrixDet.report&quot; [53] &quot;ds.matrixDiag&quot; &quot;ds.matrixDimnames&quot; [55] &quot;ds.matrixInvert&quot; &quot;ds.matrixMult&quot; [57] &quot;ds.matrixTranspose&quot; &quot;ds.mean&quot; [59] &quot;ds.meanByClass&quot; &quot;ds.meanSdGp&quot; [61] &quot;ds.merge&quot; &quot;ds.message&quot; [63] &quot;ds.names&quot; &quot;ds.numNA&quot; [65] &quot;ds.quantileMean&quot; &quot;ds.rbind&quot; [67] &quot;ds.rBinom&quot; &quot;ds.recodeLevels&quot; [69] &quot;ds.recodeValues&quot; &quot;ds.rep&quot; [71] &quot;ds.replaceNA&quot; &quot;ds.reShape&quot; [73] &quot;ds.rm&quot; &quot;ds.rNorm&quot; [75] &quot;ds.rowColCalc&quot; &quot;ds.rPois&quot; [77] &quot;ds.rUnif&quot; &quot;ds.sample&quot; [79] &quot;ds.scatterPlot&quot; &quot;ds.seq&quot; [81] &quot;ds.setDefaultOpals&quot; &quot;ds.setSeed&quot; [83] &quot;ds.subset&quot; &quot;ds.subsetByClass&quot; [85] &quot;ds.summary&quot; &quot;ds.table&quot; [87] &quot;ds.table1D&quot; &quot;ds.table2D&quot; [89] &quot;ds.tapply&quot; &quot;ds.tapply.assign&quot; [91] &quot;ds.testObjExists&quot; &quot;ds.unList&quot; [93] &quot;ds.var&quot; &quot;ds.vectorCalc&quot; Finaly we clean up the R server sessions: datashield.logout(conns) "],
["resources.html", "6 The Resources 6.1 Concept 6.2 Types 6.3 Definition", " 6 The Resources Developing and implementing new algorithms to perform advanced data analyses under the DataSHIELD framework is a current active line of research. However, the analysis of big data within DataSHIELD has some limitations. Some of them are related to how data is managed in Opal’s database and others are related to how to perform statistical analyses of big data within the R environment. Opal databases are for general purpose and do not manage large amounts of information properly and, second, it requires data to be moved from original repositories into Opal which is inefficient (this is a time, CPU and memory intensive operation) and is difficult to maintain when data are updated. We have overcome the problem related to DataSHIELD big data management by developing a new data infrastructure within Opal: the resources. 6.1 Concept Resources are datasets or computation units whose location is described by a URL and access is protected by credentials. When assigned to a R/DataSHIELD server session, remote big/complex datasets or high performance computers are made accessible to data analysts. Instead of storing the data in Opal’s database, only the way to access it is defined: the datasets are kept in their original format and location (a SQL database, a SPSS file, R object, etc.) and are read directly from the R/DataSHIELD server-side session. Then as soon as there is a R reader for the dataset or a connector for the analysis services, a resource can be defined. Opal takes care of the DataSHIELD permissions (a DataSHIELD user cannot see the resource’s credentials) and of the resources assignment to a R/DataSHIELD session (see Figure 6.1) Figure 6.1: Resources: a new DataSHIELD infrastructure 6.2 Types The data format refers to the intrinsic structure of the data. A very common family of data formats is the tabular format which is made of rows (entities, records, observations etc.) and columns (variables, fields, vectors etc.). Examples of tabular formats are the delimiter-separated values formats (CSV, TSV etc.), the spreadsheet data formats (Microsoft Excel, LibreOffice Calc, Google Sheets etc.), some proprietary statistical software data formats (SPSS, SAS, Stata etc.), the database tables that can be stored in structured database management systems that are row-oriented (MySQL, MariaDB, PostgreSQL, Oracle, SQLite etc.) or column-oriented (Apache Cassandra, Apache Parquet, MariaDB ColumnStore, BigTable etc.), or in semi-structured database management systems such as the documented-oriented databases (MongoDB, Redis, CouchDB, Elasticsearch etc.). When the data model is more complex (data types and objects relationships), a domain-specific data format is sometimes designed to handle this complexity so that statistical analysis and data retrieval can be executed as efficiently as possible. Examples of domain-specific data formats that are encountered in the ’omic or geospatial fields of research that are described in the Workflows section: Omic and Geospatial. A data format can also include some additional features such as data compression, encoding or encryption. Each data format requires an appropriate reader software library or application to extract the information or perform data aggregation or filtering operations. We have prepared a demo environment, with the Opal implementation of Resources and an appropriate R/DataSHIELD configuration that is available at: opal-demo.obiba.org in a project called RSRC. This figure illustrate the resources which are available for this project and can serve as a starting example of the different types of resources that can be dealt with Figure 6.2: Resources from a test enviroment (project called RSRC) available at https://opal-demo.obiba.org As shown in this example, the data storage can simply be a file accessed directly from the host’s file system or downloaded from a remote location. More advanced data storage systems are software applications that expose an interface to query, extract or analyse the data. These applications can make use of a standard programming interface (e.g. SQL) or expose specific web services (e.g. based on the HTTP communication protocol) or provide a software library (in different programming languages) to access the data. These different ways of accessing the data are not exclusive from each other. In some cases the micro-data cannot be extracted, only computation services that return aggregated data are provided. The data storage system can also apply security rules, requiring authentication and proper authorisations to access or analyse the data. 6.3 Definition We define a resource to be a data or computation access description. A resource will have the following properties: the location of the data or of the computation services, the data format (if this information cannot be inferred from the location property), the access credentials (if some apply). The resource location description will make use of the web standard described in the RFC 3986 “Uniform Resource Identifier (URI): Generic Syntax”. More specifically, the Uniform Resource Locator (URL) specification is what we need for defining the location of the data or computation resource: the term Uniform allows to describe the resource the same way, independently of its type, location and usage context; the term Resource does not limit the scope of what might be a resource, e.g. a document, a service, a collection of resources, or even abstract concepts (operations, relationships, etc.); the term Locator both identifies the resource and provides a means of locating it by describing its access mechanism (e.g. the network location). The URL syntax is composed of several parts: a scheme, that describes how to access the resource, e.g. the communication protocols “https” (secured HTTP communication), “ssh” (secured shell, for issuing commands on a remote server), or “s3” (for accessing Amazon Web Service S3 file store services), an authority (optional), e.g. a server name address, a path that identifies/locates the resource in a hierarchical way and that can be altered by query parameters. The resource’s data format might be inferred from the path part of the URL, by using the file name suffix for instance. Nevertheless, sometimes it is not possible to identify the data format because the path might only descripbe the data storage system, for example when a file store designates a document using an obfuscated string identifier or when a text-based data format is compressed as a zip archive. The format property can provide this information. Although the authority section of the URL can contain some user information (such as the username and password), it is discouraged to use this capability for security considerations. The resource’s credentials property will be used instead, and will be composed of an identifier sub-property and a secret sub-property, which can be used for authenticating with a username/password, or an access token, or any other credentials encoded string. The advantage of separating the credentials property from the resource location property is that a user with limited permissions could have access to the resource’s location information while the credentials are kept secret. Once a resource has been formally defined, it should be possible to build programmatically a connection object that will make use of the data or computation services described. This resource description is not bound to a specific programmatic language (the URL property is a web standard, other properties are simple strings) and does not enforce the use of a specific software application for building, storing and interpreting a resource object. Next Section describes the resourcer package which is an R implementation of the data and computation resources description and connection. There the reader can see some examples of how dealing with different resources in DataSHIELD. "],
["resourcer.html", "7 The resourcer Package 7.1 Introduction 7.2 File Resources 7.3 Database Resources 7.4 Computation Resources 7.5 Interacting with R Resources 7.6 Extending Resources 7.7 Resource Forms 7.8 Examples of Resources 7.9 Using Resources with DataSHIELD", " 7 The resourcer Package 7.1 Introduction The resourcer package is an R implementation of the data and computation resources description and connection. It resuses many existing R packages for reading various data formats and connecting to external data storage or computation servers. The resourcer package’s role is to interpret a resource description object to build the appropriate resource connection object. Because the bestiary of resources is very wide, the resourcer package provides a framework for dynamically extending the interpretation capabilities to new types of resources. This framework uses the object-oriented paradigm provided by the R6 library. The purpose of the package is to allow access resources identified by a URL in a uniform way whether it references a dataset (stored in a file, a SQL table, a MongoDB collection etc.) or a computation unit (system commands, web services etc.). Usually some credentials will be defined, and an additional data format information can be provided to help dataset coercing to a data.frame object. The main concepts are: Class Description resource Access to a resource (dataset or computation unit) is described by an object with URL, optional credentials and optional data format properties. ResourceResolver A ResourceClient factory based on the URL scheme and available in a resolvers registry. ResourceClient Realizes the connection with the dataset or the computation unit described by a resource. FileResourceGetter Connect to a file described by a resource. DBIResourceConnector Establish a DBI connection. 7.2 File Resources These are resources describing a file. If the file is in a remote location, it must be downloaded before being read. The data format specification of the resource helps to find the appropriate file reader. 7.2.1 File Getter The file locations supported by default are: file, local file system, http(s), web address, basic authentication, gridfs, MongoDB file store, scp, file copy through SSH, opal, Opal file store. This can be easily applied to other file locations by extending the FileResourceGetter class. An instance of the new file resource getter is to be registered so that the FileResourceResolver can operate as expected. resourcer::registerFileResourceGetter(MyFileLocationResourceGetter$new()) 7.2.2 File Data Format The data format specified within the resource object, helps with finding the appropriate file reader. Currently supported data formats are: the data formats that have a reader in tidyverse: readr (csv, csv2, tsv, ssv, delim), haven (spss, sav, por, dta, stata, sas, xpt), readxl (excel, xls, xlsx). This can be easily applied to other data file formats by extending the FileResourceClient class. the R data format that can be loaded in a child R environment from which object of interest will be retrieved. Usage example that reads a local SPSS file: # make a SPSS file resource res &lt;- resourcer::newResource( name = &quot;CNSIM1&quot;, url = &quot;file:///data/CNSIM1.sav&quot;, format = &quot;spss&quot; ) # coerce the csv file in the opal server to a data.frame df &lt;- as.data.frame(res) To support other file data format, extend the FileResourceClient class with the new data format reader implementation. The associated factory class, an extension of the ResourceResolver class, is also to be implemented and registered. resourcer::registerResourceResolver(MyFileFormatResourceResolver$new()) 7.3 Database Resources 7.3.1 DBI Connectors DBI is a set of virtual classes that are are used to abstract the SQL database connections and operations within R. Thus any DBI implementation can be used to access to a SQL table. Which DBI connector to be used is an information that can be extracted from the scheme part of the resource’s URL. For instance a resource URL starting with postgres:// will require the RPostgres driver. To separate the DBI connector instantiation from the DBI interface interactions in the SQLResourceClient, a DBIResourceConnector registry is to be populated. The currently supported SQL database connectors are: mariadb MariaDB connector, mysql MySQL connector, postgres or postgresql Postgres connector, presto, presto+http or presto+https Presto connector, spark, spark+http or spark+https Spark connector. To support another SQL database with a DBI driver, extend the DBIResourceConnector class and register it: resourcer::registerDBIResourceConnector(MyDBResourceConnector$new()) 7.3.2 Use dplyr Having the data stored in the database allows to handle large (common SQL databases) to big (PrestoDB, Spark) datasets using dplyr which will delegate as many operations as possible to the database. 7.3.3 Document Databases NoSQL databases can be described by a resource. The nodbi can be used here. Currently only connection to MongoDB database is supported using URL scheme mongodb or mongodb+srv. 7.4 Computation Resources Computation resources are resources on which tasks/commands can be triggered and from which resulting data can be retrieved. Example of computation resource that connects to a server through SSH: # make an application resource on a ssh server res &lt;- resourcer::newResource( name = &quot;supercomp1&quot;, url = &quot;ssh://server1.example.org/work/dir?exec=plink,ls&quot;, identity = &quot;sshaccountid&quot;, secret = &quot;sshaccountpwd&quot; ) # get ssh client from resource object client &lt;- resourcer::newResourceClient(res) # does a ssh::ssh_connect() # execute commands files &lt;- client$exec(&quot;ls&quot;) # exec &#39;cd /work/dir &amp;&amp; ls&#39; # release connection client$close() # does ssh::ssh_disconnect(session) 7.5 Interacting with R Resources As the ResourceClient is just a connector to a resource, to make this useful some data conversion functions are defined by default: R data.frame, which is the most common representation of tabular data in R. A data frame, as defined in R base, is an object stored in memory that may be not suitable for large to big datasets. dplyr tbl, which is another representation of tabular data that nicely integrates with the DBI: filtering, mutation and aggregation operations can be delegated to the underlying SQL database, reducing the R memory and computation footprint. Useful functions are also provided to perform join operations on relational datasets. A data.frame can be accessed as a tbl and vice versa. In the case when the resource is a R object, the RDataFileResourceClient offers the ability to get the internal raw data object. Then complex data structures, optimized for a specific research domain, can be accessed with the most appropriate tools. When the resource is a computation service provider, the interaction with the resource client will consist of issuing commands/requests with parameters and getting the result from it either as a response object or as a file to be downloaded. Another way of interacting with a resource is to get the internal connection object (a database connector, a SSH connector etc.) from the ResourceClient and then apply any kind of operations that are defined for it. The general purpose of a resource is not to substitute itself to the underlying library, it is to facilitate the access to the related data and services. 7.6 Extending Resources There are several ways to extend the Resources handling. These are based on different R6 classes having a isFor(resource) function: If the resource is a file located at a place not already handled, write a new FileResourceGetter subclass and register an instance of it with the function registerFileResourceGetter(). If the resource is a SQL engine having a DBI connector defined, write a new DBIResourceConnector subclass and register an instance of it with the function registerDBIResourceConnector(). If the resource is in a domain specific web application or database, write a new ResourceResolver subclass and register an instance of it with the function registerResourceResolver(). This ResourceResolver object will create the appropriate ResourceClient object that matches your needs. The design of the URL that will describe your new resource should not overlap an existing one, otherwise the different registries will return the first instance for which the isFor(resource) is TRUE. In order to distinguish resource locations, the URL’s scheme can be extended, for instance the scheme for accessing a file in a Opal server is opal+https so that the credentials be applied as needed by Opal. In order to simplify the usage of these resource client classes, the resourcer package combines several software design patterns: The factory pattern, a classic creational design pattern. It is realized by the ResourceResolver R6 class which is responsible for making a ResourceClient object matching a resource object provided. The registry pattern, “a well-known object that other objects can use to find common objects and services” as described by M. Fowler. It is basically a global list of objects to iterate through to find the appropriate one. In the resourcer package there are several registries: (1) the registry of ResourceResolver objects (the ResourceClient factories), (2) the registry of FileResourceGetter objects and (3) the registry of DBIResourceConnector objects. The self-registration pattern, which consists of delegating the registration of new services to their provider. The package event mechanism of R is used so that an R package self-registers its resource components in the registries previously mentioned when the package is loaded at runtime. For implementing ResourceClient factories the resourcer package provides different ResourceResolver classes (see Figure 7.1): for file resources, which will discriminate the resources based on the URL property (checking if any FileResourceGetter exists for that resource) and the data format property (for getting additional information about how to read the data). The FileResourceGetter can be extended to new file locations: as an example, the s3.resourcer R package is able to get resource files from Amazon Web Service S3 file stores. for database resources, which will discriminate the resources based on the scheme part of the URL (checking if any DBIResourceConnector can be found or whether a nodbi connector can be created). The DBIResourceConnector can be extended to new DBI implementations. For instance, using bigrquery R package, it would be easy to implement access to a resource stored in a Google’s BigQuery database. for command-based computation resources, which will discriminate the resources based on the scheme part of the URL, indicating how to issue commands (local shell or secure remote shell). Figure 7.1: ResourceResolver class diagram Additional ResourceResolver and ResourceClient extensions could be implemented for accessing domain specific applications which would expose data extraction and/or analysis services, the only requirement is that an R connection API exists for the considered application. The process of creating a new ResourceClient instance from a resource object consists of iterating over a registry of ResourceResolver instances and finding the one that can handle the resource object by inspecting its properties (URL, data format etc.). The URL property inspection can imply a lookup in the additional registries: if the resource is a data file, the file resource resolvers will check in the FileResourceGetter registry whether there is one that applies; if the resource is a DBI-compatible database, the SQL resource resolver will check in the DBIResourceConnector registry whether there is one that applies. This workflow is simply triggered by a resourcer’s function call. 7.7 Resource Forms As the definition a new resource can be error prone, when a URL is complex, or when there is a limited choice of formats or when credentials can be on different types, it is recommended to declare the resources forms and factory functions within the R package. This resource declaration is to be done in JavaScript, as this is a very commonly used language for building graphical user interfaces. These files are expected to be installed at the root of the package folder (then in the source code of the R package, they will be declared in the inst/resources folder), so that an external application can lookup the packages statically having declared some resources. The configuration file inst/resources/resource.js is a javascript file which contains an object with the properties: settings, a JSON object that contains the description and the documentation of the web forms (based on the json-schema specification). asResource, a javascript function that will convert the data captured from one of the declared web forms into a data structure representing the resource object. As an example (see also resourcer’s resource.js): var myPackage = { settings: { &quot;title&quot;: &quot;MyPackage resources&quot;, &quot;description&quot;: &quot;MyPackage resources are for etc.&quot;, &quot;web&quot;: &quot;https://github.com/org/myPackage&quot;, &quot;categories&quot;: [ { &quot;name&quot;: &quot;my-format&quot;, &quot;title&quot;: &quot;My data format&quot;, &quot;description&quot;: &quot;Data are files in my format, that will be read by myPackage etc.&quot; } ], &quot;types&quot;: [ { &quot;name&quot;: &quot;my-format-http&quot;, &quot;title&quot;: &quot;My data format - HTTP&quot;, &quot;description&quot;: &quot;Data are files in my format, that will be downloaded from a HTTP server etc.&quot;, &quot;tags&quot;: [&quot;my-format&quot;, &quot;http&quot;], &quot;parameters&quot;: {}, &quot;credentials&quot;: {} } ] }, asResource: function(type, name, params, credentials) { // make a resource object from arguments, using type to drive // what params/credentials properties are to be used // a basic example of resource object: return { &quot;name&quot;: name, &quot;url&quot;: params.url, &quot;format&quot;: params.format, &quot;identity&quot;: credentials.username, &quot;secret&quot;: credentials.password }; } } The specifications for the settings object is the following: Property Type Description title string The title of the set of resources. description string The description of the set of resources. web string A web link that describes the resources. categories array of object A list of category objects which are used to categorize the declared resources in terms of resource location, format, usage etc. types array of object A list of type objects which contains a description of the parameters and credentials forms for each type of resource. Where the category object is: Property Type Description name string The name of the category that will be applied to each resource type, must be unique. title string The title of the category. description string The description of the category. And the type object is: Property Type Description name string The identifying name of the resource, must be unique. title string The title of the resource. description string The description of the resource form. tags array of string The tag names that are applied to the resource form. parameters object The form that will be used to capture the parameters to build the url and the format properties of the resource (based on the json-schema specification). Some specific fields can be used: _package to capture the R package name or _packages to capture an array of R package names to be loaded prior to the resource assignment. credentials object The form that will be used to capture the access credentials to build the identity and the secret properties of the resource (based on the json-schema specification). The asResource function is a javascript function which signature is function(type, name, params, credentials) where: type, the form name used to capture the resource parameters and credentials, name, the name to apply to the resource, params, the captured parameters, credentials, the captured credentials. The name of the root object must follow the pattern: &lt;R package&gt; (note that any dots (.) in the R package name are to be replaced by underscores (_)). A real example of how to create this file for the `{r Githubpkg(“isglobal-brge”, “dsOmics”)} package (described in this Section) can be found here. 7.8 Examples of Resources The resourcer package provides some common resources, out of the box. Additional resource extensions are also available as separate R packages. The following table lists some known resource implementations. Resource R Package Description Tidy data files resourcer Data files that have a reader from the tidyverse ecosystem. R data files resourcer Any R objects stored in files in Rdata or RDS formats. SQL databases resourcer Main open source SQL databases vendors are supported: MySQL, MariaDB and PostgreSQL Apache Spark resourcer Big data analytics system, accessible as a SQL database, with also additional computation capabilities (machine leaning API). SSH server resourcer Command based remote computation service. MongoDB resourcer A NoSQL database. HL7 FHIR resource fhir.resourcer Data extractions from HL7 FIHR compliant servers, the standard for clinical data management systems. Dremio dataset odbc.resourcer Dremio is a data lake engine, tailored for integrating big data. Dremio is accessible as a SQL database. GDS data files dsOmics Reads omics data stored in Genomic Data Structure (GDS) format or in a format that can be converted to GDS (such as VCF). Whenever a data access format or protocol is defined by a standard API specification, there is most likely an R implementation of this API and then a corresponding resourcer extension can be easily implemented. For instance developing a resourcer extension to access genomic data stored in a GA4GH system should be possible and would be very powerful. 7.9 Using Resources with DataSHIELD Let us illustrate how to deal with different types of resources within DataSHIELD. To this end, let use our Opal demo example (RSRC project) available at https://opal-demo.obiba.org which has the following resources Figure 7.2: Resources from a demo enviroment available at https://opal-demo.obiba.org 7.9.1 TSV File into a tibble or data.frame We start by illustrating how to get a simple TSV file (brge.txt) into the R server. This file is located at a GitHub repository: https://raw.githubusercontent.com/isglobal-brge/brgedata/master/inst/extdata/brge.txt and it is not necesary to be moved from there. This is one of the main strenght of the resources implementation. This code describes how to get the resource (a TSV file) as a data.frame into the R Server. Note that this is secure access since a user name and password must be provided library(DSOpal) library(dsBaseClient) # access to the &#39;brge&#39; resource (NOTE: RSRC.brge is need since the project # is called RSRC) builder &lt;- newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.brge&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() # the resource is loaded into R as the object &#39;res&#39; conns &lt;- datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) # the resource is assigned to a data.frame # Assign to the original R class (e.g ExpressionSet) datashield.assign.expr(conns, symbol = &quot;dat&quot;, expr = quote(as.resource.data.frame(res))) ds.class(&quot;dat&quot;) $study1 [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; # logout the connection datashield.logout(conns) 7.9.2 R Data File into a R object Now let us describe how to get a specific type of R object into the R server. Our Opal demo contains a resource called GSE80970 which is located on a local machine (see Figure 7.2). The resource is an R object of class ExpressionSet which is normally used to jointly encapsulate gene expression, metadata and annotation. In general, we can retrieve any R object in its original format and if a method to coerce the specific object into a data.frame exists, we can also retrieve it as a tibble/data.frame. # prepare login data and resource to assign builder &lt;- newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.GSE80970&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() # login and assign resource (to &#39;res&#39; symbol) conns &lt;- datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) # coerce ResourceClient objects to a data.frame called &#39;DF&#39; # NOTE: as.data.frame exists for `ExpressionSet` objects datashield.assign.expr(conns, symbol = &quot;DF&quot;, expr = quote(as.resource.data.frame(res))) ds.class(&quot;DF&quot;) $study1 [1] &quot;data.frame&quot; # we can also coerce ResourceClient objects to their original format. # This will allow the analyses with specific R/Bioconductor packages datashield.assign.expr(conns, symbol = &quot;ES&quot;, expr = quote(as.resource.object(res))) ds.class(&quot;ES&quot;) $study1 [1] &quot;ExpressionSet&quot; attr(,&quot;package&quot;) [1] &quot;Biobase&quot; # logout the connection datashield.logout(conns) "],
["omic-extension.html", "8 Extension to VCF files to peform GWAS with Bioconductor", " 8 Extension to VCF files to peform GWAS with Bioconductor Genomic data can be stored in different formats. PLINK and VCF files are commonly used in genetic epidemiology studies. In order to deal with this type of data, we have extended the resources available at the resourcer package to VCF files. NOTE: PLINK files can be translated into VCF files using different pipelines. In R you can use SeqArray to get VCF files. We use the Genomic Data Storage (GDS) format which efficiently manages VCF files in the R environment. This extension requires creation of a Client and a Resolver function for the resourcer that are located in the dsOmics package. The client function uses snpgdsVCF2GDS function implemented in SNPrelate to coerce the VCF file to a GDS object. Then the GDS object is loaded into R as an object of class GdsGenotypeReader from GWASTools package that facilitate downstream analyses. The Opal server API allows us to incorporate this new type of resource as illustrated in Figure 8.1. Figure 8.1: Description of how a VCF file can be added to the opal resources It is important to notice that the URL should contain the tag method=biallelic.only&amp;snpfirstdim=TRUE since these are required parameters of the snpgdsVCF2GDS function. This is an example: https://raw.githubusercontent.com/isglobal-brge/scoreInvHap/master/inst/extdata/example.vcf?method=biallelic.only&amp;snpfirstdim=TRUE In this case we indicate that only biallelic SNPs are considered (‘method=biallelic.only’) and that genotypes are stored in the individual-major mode, (i.e., list all SNPs for the first individual, and then list all SNPs for the second individual, etc) (‘snpfirstdim=TRUE’). "],
["shell-extension.html", "9 Extension to secure shell programs: GWAS with PLINK", " 9 Extension to secure shell programs: GWAS with PLINK GWAS can also be performed using programs that are executed using shell commands. This is the case for PLINK, one of the state-of-the-art programs to run GWAS and other genomic data analyses such gene-enviroment interactions or polygenic risc score analyses that require efficient and scalable pipelines. Resources also allow the use of secure SSH service to run programs on a remote server accessible through SSH containing data and analysis tools where R is just used for launching the analyses and aggregating results. This feature allows us to create functions to analyze data using specific shell programs. Here we describe how the PLINK program can be used to perform GWAS. In this case, the resource describes that access is given via SSH, the credentials required to connect, and the commands that can be run (of which one is plink). We use the following code to illustrate how analyses should be performed using the resourcer package. This code could be considered as the base code for creating a server-side DataSHIELD package as performed in the plinkDS() function implemented in the dsOmics package We access the ssh resource called brge_plink (Figure 7.2) using the resourcer package as follows: library(resourcer) brge_plink &lt;- resourcer::newResource(url=&quot;ssh://plink-demo.obiba.org:2222/home/master/brge?exec=ls,plink,plink1&quot;, identity = &quot;master&quot;, secret = &quot;nSATfSy9Y8JAo5zK&quot;) client &lt;- resourcer::newResourceClient(brge_plink) This creates an object of this class: class(client) [1] &quot;SshResourceClient&quot; &quot;CommandResourceClient&quot; &quot;ResourceClient&quot; &quot;R6&quot; These are the actions we can do with an SshResourceClient object names(client) [1] &quot;.__enclos_env__&quot; &quot;clone&quot; &quot;close&quot; &quot;exec&quot; &quot;removeTempDir&quot; [6] &quot;tempDir&quot; &quot;uploadFile&quot; &quot;downloadFile&quot; &quot;getConnection&quot; &quot;getAllowedCommands&quot; [11] &quot;initialize&quot; &quot;asTbl&quot; &quot;asDataFrame&quot; &quot;getResource&quot; For this specific resource (e.g. PLINK) we can execute these shell commands client$getAllowedCommands() [1] &quot;ls&quot; &quot;plink&quot; &quot;plink1&quot; For instance client$exec(&quot;ls&quot;, &quot;-la&quot;) $status [1] 0 $output [1] &quot;total 92992&quot; [2] &quot;dr-xr-xr-x 2 master master 4096 Apr 29 2020 .&quot; [3] &quot;drwxr-xr-x 7 master master 4096 Jan 4 18:41 ..&quot; [4] &quot;-r--r--r-- 1 master master 57800003 Apr 29 2020 brge.bed&quot; [5] &quot;-r--r--r-- 1 master master 2781294 Apr 29 2020 brge.bim&quot; [6] &quot;-r--r--r-- 1 master master 45771 Apr 29 2020 brge.fam&quot; [7] &quot;-r--r--r-- 1 master master 34442346 Apr 29 2020 brge.gds&quot; [8] &quot;-r--r--r-- 1 master master 59802 Apr 29 2020 brge.phe&quot; [9] &quot;-r--r--r-- 1 master master 72106 Apr 29 2020 brge.txt&quot; $error character(0) $command [1] &quot;cd /home/master/brge &amp;&amp; ls -la&quot; attr(,&quot;class&quot;) [1] &quot;resource.exec&quot; Then, to avoid multiple accesses to the resource, it is recommended to create a temporary directory to save our results tempDir &lt;- client$tempDir() tempDir [1] &quot;/tmp/ssh-3451&quot; client$exec(&quot;ls&quot;, tempDir) $status [1] 0 $output character(0) $error character(0) $command [1] &quot;cd /home/master/brge &amp;&amp; ls /tmp/ssh-3451&quot; attr(,&quot;class&quot;) [1] &quot;resource.exec&quot; Then, we can use R to launch the shell commands client$exec(&#39;plink1&#39;, c(&#39;--bfile&#39;, &#39;brge&#39;, &#39;--freq&#39;, &#39;--out&#39;, paste0(tempDir, &#39;/out&#39;), &#39;--noweb&#39;)) $status [1] 0 $output [1] &quot;&quot; [2] &quot;@----------------------------------------------------------@&quot; [3] &quot;| PLINK! | v1.07 | 10/Aug/2009 |&quot; [4] &quot;|----------------------------------------------------------|&quot; [5] &quot;| (C) 2009 Shaun Purcell, GNU General Public License, v2 |&quot; [6] &quot;|----------------------------------------------------------|&quot; [7] &quot;| For documentation, citation &amp; bug-report instructions: |&quot; [8] &quot;| http://pngu.mgh.harvard.edu/purcell/plink/ |&quot; [9] &quot;@----------------------------------------------------------@&quot; [10] &quot;&quot; [11] &quot;Skipping web check... [ --noweb ] &quot; [12] &quot;Writing this text to log file [ /tmp/ssh-3451/out.log ]&quot; [13] &quot;Analysis started: Tue Jan 12 14:41:06 2021&quot; [14] &quot;&quot; [15] &quot;Options in effect:&quot; [16] &quot;\\t--bfile brge&quot; [17] &quot;\\t--freq&quot; [18] &quot;\\t--out /tmp/ssh-3451/out&quot; [19] &quot;\\t--noweb&quot; [20] &quot;&quot; [21] &quot;Reading map (extended format) from [ brge.bim ] &quot; [22] &quot;100000 markers to be included from [ brge.bim ]&quot; [23] &quot;Reading pedigree information from [ brge.fam ] &quot; [24] &quot;2312 individuals read from [ brge.fam ] &quot; [25] &quot;2312 individuals with nonmissing phenotypes&quot; [26] &quot;Assuming a disease phenotype (1=unaff, 2=aff, 0=miss)&quot; [27] &quot;Missing phenotype value is also -9&quot; [28] &quot;725 cases, 1587 controls and 0 missing&quot; [29] &quot;1097 males, 1215 females, and 0 of unspecified sex&quot; [30] &quot;Reading genotype bitfile from [ brge.bed ] &quot; [31] &quot;Detected that binary PED file is v1.00 SNP-major mode&quot; [32] &quot;Before frequency and genotyping pruning, there are 100000 SNPs&quot; [33] &quot;2312 founders and 0 non-founders found&quot; [34] &quot;6009 heterozygous haploid genotypes; set to missing&quot; [35] &quot;Writing list of heterozygous haploid genotypes to [ /tmp/ssh-3451/out.hh ]&quot; [36] &quot;7 SNPs with no founder genotypes observed&quot; [37] &quot;Warning, MAF set to 0 for these SNPs (see --nonfounders)&quot; [38] &quot;Writing list of these SNPs to [ /tmp/ssh-3451/out.nof ]&quot; [39] &quot;Writing allele frequencies (founders-only) to [ /tmp/ssh-3451/out.frq ] &quot; [40] &quot;&quot; [41] &quot;Analysis finished: Tue Jan 12 14:41:10 2021&quot; [42] &quot;&quot; $error character(0) $command [1] &quot;cd /home/master/brge &amp;&amp; plink1 --bfile brge --freq --out /tmp/ssh-3451/out --noweb&quot; attr(,&quot;class&quot;) [1] &quot;resource.exec&quot; The results can be retrieved as an R object outs &lt;- client$exec(&#39;ls&#39;, tempDir)$output outs [1] &quot;out.frq&quot; &quot;out.hh&quot; &quot;out.log&quot; &quot;out.nof&quot; client$downloadFile(paste0(tempDir, &#39;/out.frq&#39;)) NULL ans &lt;- readr::read_table(&quot;out.frq&quot;) ans # A tibble: 100,000 x 6 CHR SNP A1 A2 MAF NCHROBS &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 MitoC3993T T C 0.0149 4572 2 0 MitoG4821A A G 0.00175 4564 3 0 MitoG6027A A G 0.00434 4614 4 0 MitoT6153C C T 0.0106 4616 5 0 MitoC7275T T C 0.000866 4618 6 0 MitoT9699C C T 0.0732 4604 7 0 MitoA10045G G A 0.00780 4616 8 0 MitoG10311A A G 0.00261 4604 9 0 MitoA11252G G A 0.182 4518 10 0 MitoT11900C C T 0.000868 4608 # ... with 99,990 more rows Finally temporary directories are removed and the session closed client$removeTempDir() client$close() "],
["extension-to-ga4gh-and-ega-bamvcf-files.html", "10 Extension to GA4GH and EGA (BAM/VCF files) 10.1 Upload GA4GH and EGA resources into OPAL 10.2 Adding a genomic resource 10.3 Descriptive omic data analysis", " 10 Extension to GA4GH and EGA (BAM/VCF files) The Global Alliance for Genomics and Health (GA4GH) has a wide toolkit focused on many aspects of the study of the genome. One of the aspects is the distribution of data, which ensures safe transfer protocols for data providers to grant access to their data. The htsget API is a standard developed by GA4GH which focuses on the streaming of BAM (Binary version of a SAM file) and VCF (Variant Call Format) files. In order to connect to open data servers that have the htsget API implemented, a Client and a Resolver have been located in the dsOmics package. In order to integrate this resource into the dsOmics analysis pipeline, the BAM and VCF files are treated using the functions snpgdsVCF2GDS and BAM2VCF to obtain a Genomic Data Storage (GDS) object. The generic resource implemented for the GA4GH htsget API allows the connection to open databases, federated databases that have the same protocol implemented rely on different authentication processes in order to allow the streaming of data files, for that reason they have to be implemented on a case to case basis. The European Genome-Phenome Archive (EGA) is a federated database that has the htsget API implemented. In order to stream data, an authentication using OAuth2.0 is required. Following the general htsget resource implementation, a Client and a Resolver have been located in the dsOmics package. The difference to use this resource is that it does not require an URL, as the EGA-htsget endpoint is static, and that a user and password has to be provided to perform the authentication to EGA. 10.1 Upload GA4GH and EGA resources into OPAL 10.1.1 Manually 10.1.1.1 GA4GH To upload a GA4GH htsget resource into OPAL, the first steps to follow are already described here. The particularities of GA4GH htsget resources come to the last step, when inputing the resource parameters. The resource category to select is GA4GH htsget. The resouce has to be named and an optional description can be provided if desired (Illustrated on the Figure 10.1). Figure 10.1: Naming a GA4GH htsget resource Once named, some additional paramters have to be inputed to define the resource (all of them are mandatory): Host: URL of the server to which stream data. File ID: Name of the VCF/BAM file on the server. Chromosome: Name of the chromosome to filter the selected file. The enconding has to match the selected file, if the file uses UCSC enconding use “chr1” if it uses NCBI use “1”. Contact the data provider if in doubt. Start: The start position of the filter. End: The end position of the filter. Format: Format of the file, this field can take two values VCF: To select a filtered VCF file BAM: To select a filtered BAM file This parameters are illustrated on the Figure 10.2 Figure 10.2: Selecting the parameters on a GA4GH htsget resource 10.1.1.2 EGA To upload a EGA htsget resource into OPAL, the first steps to follow are already described here. The particularities of EGA htsget resources come to the last step, when inputing the resource parameters. The resource category to select is EGA database. The resouce has to be named and an optional description can be provided if desired (Illustrated on the Figure 10.3). Figure 10.3: Naming a EGA database resource Once named, some additional paramters have to be inputed to define the resource (all of them are mandatory): File ID: Name of the VCF/BAM file on EGA. Chromosome: Name of the chromosome to filter the selected file. The enconding has to match the selected file, if the file uses UCSC enconding use “chr1” if it uses NCBI use “1”. Contact the data provider if in doubt. Start: The start position of the filter. End: The end position of the filter. Format: Format of the file, this field can take two values VCF: To select a filtered VCF file BAM: To select a filtered BAM file This parameters are illustrated on the Figure 10.4 Figure 10.4: Selecting the parameters on a EGA database resource Access to the EGA database is controlled via a user and password combination that grants access to different files on the server. For that, they are mandatory to create the resource. They have to be inputed on the Credentials tab, illustrated on the Figure 10.5. Figure 10.5: Credentials tab for EGA database resource 10.1.2 Using R 10.1.2.1 GA4GH A GA4GH htsget can be added to a given OPAL project by a simple function call. This process requires to exactly write the the URL. To automate this, a function exists on dsOmicsClient that creates the URLs, this function takes the following parameters: url: URL of the server id: ID of the file on the server chr: Chromosome to filter (encoding is server dependant) start: Start position to filter end: End position to filter type: Type of file (VCF/BAM) library(dsOmicsClient) # BAM FILES bam_url &lt;- get_ga4gh_url(url = &quot;https://htsget.ga4gh.org&quot;, id = &quot;giab.NA12878.NIST7035.2&quot;, chr = &quot;chr21&quot;, start = 1, end = 20000000, type = &quot;BAM&quot;) bam_url [1] &quot;https://htsget.ga4gh.org/reads/giab.NA12878.NIST7035.2?format=BAM&amp;referenceName=chr21&amp;start=1&amp;end=20000000&quot; # VCF FILES vcf_url &lt;- get_ga4gh_url(url = &quot;https://htsget.ga4gh.org&quot;, id = &quot;1000genomes.phase1.chr1&quot;, chr = &quot;1&quot;, start = 15000, end = 16000, type = &quot;VCF&quot;) vcf_url [1] &quot;https://htsget.ga4gh.org/variants/1000genomes.phase1.chr1?format=VCF&amp;referenceName=1&amp;start=15000&amp;end=16000&quot; Then, the R code to create this resource from the VCF file in OPAL is: library(opalr) o &lt;- opal.login(&quot;administrator&quot;,&quot;password&quot;, url=&quot;https://opal-demo.obiba.org&quot;) opal.resource_create(o, &quot;RSRC&quot;, &quot;ga4gh_1000g&quot;, url = vcf_url format = &quot;GA4GHVCF&quot;) opal.resource_create(o, &quot;RSRC&quot;, &quot;giab&quot;, url = bam_url format = &quot;GA4GHBAM&quot;) # to test the resource assignment opal.assign.resource(o, &quot;client&quot;, &quot;RSRC.ga4gh_1000g&quot;) opal.assign.resource(o, &quot;client&quot;, &quot;RSRC.giab&quot;) opal.execute(o, &quot;class(client)&quot;) opal.logout(o) 10.1.2.2 EGA A EGA database can be added to a given OPAL project by a simple function call. This process requires to exactly write the the URL where the genomic data file is located. To automate this, a function exists on dsOmicsClient that creates the URLs, this function takes the following parameters: id: ID of the EGA file chr: Chromosome to filter (encoding is file dependant) start: Start position to filter end: End position to filter library(dsOmicsClient) # Same URL style for BAM and VCF ega_url &lt;- get_EGA_url(&quot;EGAF00001753756&quot;, &quot;chr21&quot;, 1, 100) ega_url [1] &quot;https://ega.ebi.ac.uk:8052/elixir/tickets/tickets/EGAF00001753756?referenceName=chr21&amp;start=1&amp;end=100&quot; Then, the R code to create this resource in OPAL is: library(opalr) o &lt;- opal.login(&quot;administrator&quot;,&quot;password&quot;, url=&quot;https://opal-demo.obiba.org&quot;) opal.resource_create(o, &quot;RSRC&quot;, &quot;EGA&quot;, url = ega_url format = &quot;EGAhtsgetBAM&quot;, # format = &quot;EGAhtsgetVCF&quot; for VCF files identity = &quot;ega-test-data@ebi.ac.uk&quot;, # This is the test user secret = &quot;egarocks&quot; # This is the test password ) # to test the resource assignment opal.assign.resource(o, &quot;client&quot;, &quot;RSRC.EGA&quot;) opal.execute(o, &quot;class(client)&quot;) opal.logout(o) 10.2 Adding a genomic resource To load the presented resources into the study servers, the standard procedure already presented on this Bookdown has to be followed (see here). library(DSI) library(DSOpal) library(dsBaseClient) library(dsOmicsClient) builder &lt;- newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.ga4gh_1000g&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() conns &lt;- datashield.login(logins = logindata, assign = TRUE, symbol = &quot;ga4gh&quot;) datashield.assign.expr(conns, symbol = &quot;ga4gh_gds&quot;, expr = quote(as.resource.object(ga4gh))) Now we can see that the resource is loaded in the study server as a GDS ds.class(&quot;ga4gh_gds&quot;) $study1 [1] &quot;GdsGenotypeReader&quot; attr(,&quot;package&quot;) [1] &quot;GWASTools&quot; 10.3 Descriptive omic data analysis Since the presented resources load GDS objects into the study servers, the same procedures introduced on this same bookdown can be applied. As an example, a PCA can be performed on the loaded data from a GA4GH htsget server that holds the 1000 Genome dataset. This example resource contains SNPs of the first human chromosome ranging from the start of it to the position 100000. ds.PCASNPS(&quot;ga4gh_gds&quot;) Figure 10.6: PCA over the 1000 genomes dataset available at GA4GH For further descriptive analysis such as Hardy-Weinberg Equilibrium testing and Allelic frequency, a phenotypes (or covariates) table has to be provided and bundled through the ds.GenotypeData function, this table has to have at least information about the gender of the individuals. This closes the access to the resources datashield.logout(conns) "],
["setup.html", "11 Setup 11.1 Providing DataSHIELD packages in the opal server 11.2 Required R Packages in the client site (e.g. local machine)", " 11 Setup As described in a previous Chapter the resourcer R package allows users to deal with the main data sources (using tidyverse, DBI, dplyr, sparklyr, MongoDB, AWS S3, SSH etc.) and is easily extensible to new ones including specific data infrastructure in R or Bioconductor. So far ExpressionSet and RangedSummarizedExperiment objects saved in .RData files are accesible through the resourcer package. The dsOmics package contains a new extension that deals with VCF (Variant Calling Format) files which are coerced to a GDS (Genomic Data Storage) format (VCF2GDS). In order to achieve this resourcer extension, two R6 classes have been implemented: GDSFileResourceResolver class which handles file-base resources with data in GDS or VCF formats. This class is responsible for creating a GDSFileResourceClient object instance from an assigned resource. GDSFileResourceClient class which is responsible for getting the referenced file and making a connection (created by GWASTools) to the GDS file (will also convert the VCF file to a GDS file on the fly, using SNPRelate). For the subsequent analysis, it is this connection handle to the GDS file that will be used. 11.1 Providing DataSHIELD packages in the opal server The required DataSHIELD packages must be uploaded to Opal through the Administration site by accessing to DataSHIELD tab. In our case, both dsBase and dsOmics and resourcer packages must be installed as is illustrated in the figure. Figure 11.1: Installed packages in the demo opal server The tab +Add package can be used to install a new package. The figure depicts how dsOmics was intalled on Opal Figure 11.2: Description how dsOmics package was intalled into the demo opal server For reproducing this book the following packages must be installed on Opal From CRAN: - resourcer From Github: - datashield/dsBase - datashield/dsGeo (tombisho/dsGeo) - isglobal-brge/dsOmics Note that the dsGeo package imports the sp, rgeos and rgdal R packages. rgeos and rgdal in turn require some additional libraries which can be installed as follows (on Ubuntu systems - see the notes in rgeos and rgdal for other operating systems): sudo apt-get update sudo apt-get install libgdal-dev libproj-dev libgeos++dev 11.2 Required R Packages in the client site (e.g. local machine) Using DataSHIELD also requires some R packages to be installed on the client site. So far, the following R packages must be installed (in their development version): install.packages(&quot;DSOpal&quot;, dependencies = TRUE) install.packages(&quot;dsBaseClient&quot;, repos = c(&quot;https://cloud.r-project.org&quot;, &quot;https://cran.obiba.org&quot;), dependencies = TRUE) devtools::install_github(&quot;isglobal-brge/dsOmicsClient&quot;, dependencies = TRUE) devtools::install_github(&quot;tombisho/dsGeoClient&quot;, dependencies = TRUE) The package dependencies are then loaded as follows: library(DSOpal) library(dsBaseClient) library(dsOmicsClient) library(dsGeoClient) "],
["basic-statistical-analyses.html", "12 Basic statistical analyses 12.1 Analysis from a single study 12.2 Analysis from a multiple studies", " 12 Basic statistical analyses Let us start by illustrating how to peform simple statistical data analyses using different resources. Here, we will use data from three studies that are available in our Opal demo repository. The three databases are called CNSIM1, CNSIM2, CNSIM3 and are available as three different resources: mySQL database, SPSS file and CSV file (see Figure 6.2). This example mimics real situations where different hospitals or research centers manage their own databases containing harmonized data. Data correspond to three simulated datasets with different numbers of observations of 11 harmonized variables. They contain synthetic data based on a model derived from the participants of the 1958 Birth Cohort, as part of an obesity methodological development project. This dataset does contain some NA values. The available variables are: Variable Description Type Note LAB_TSC Total Serum Cholesterol numeric mmol/L LAB_TRIG Triglycerides numeric mmol/L LAB_HDL HDL Cholesterol numeric mmol/L LAB_GLUC_ADJUSTED Non-Fasting Glucose numeric mmol/L PM_BMI_CONTINUOUS Body Mass Index (continuous) numeric kg/m2 DIS_CVA History of Stroke factor 0 = Never had stroke; 1 = Has had stroke MEDI_LPD Current Use of Lipid Lowering Medication (from categorical assessment item) factor 0 = Not currently using lipid lowering medication; 1 = Currently using lipid lowering medication DIS_DIAB History of Diabetes factor 0 = Never had diabetes; 1 = Has had diabetes DIS_AMI History of Myocardial Infarction factor 0 = Never had myocardial infarction; 1 = Has had myocardial infarction GENDER Gender factor 0 = Female PM_BMI_CATEGORICAL Body Mass Index (categorical) factor 1 = Less than 25 kg/m2; 2 = 25 to 30 kg/m2; 3 = Over 30 kg/m2 The analyses that are described here, can also be found in the DataSHIELD Tutorial where these resources here uploaded to the Opal server as three tables, an inferior approach since data have to be moved from their original repositories. 12.1 Analysis from a single study Let us start by illustrating how to analyze one data set (CNSIM2). library(DSOpal) library(dsBaseClient) # prepare login data and resource to assign builder &lt;- DSI::newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.CNSIM1&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() # login and assign resource conns &lt;- DSI::datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) # coerce ResourceClient objects to a data.frame called &#39;D&#39; datashield.assign.expr(conns, symbol = &quot;D&quot;, expr = quote(as.resource.data.frame(res, strict=TRUE))) Then we can inspect the type of data we have ds.class(&quot;D&quot;) $study1 [1] &quot;data.frame&quot; ds.colnames(&quot;D&quot;) $study1 [1] &quot;id&quot; &quot;LAB_TSC&quot; &quot;LAB_TRIG&quot; &quot;LAB_HDL&quot; &quot;LAB_GLUC_ADJUSTED&quot; [6] &quot;PM_BMI_CONTINUOUS&quot; &quot;DIS_CVA&quot; &quot;MEDI_LPD&quot; &quot;DIS_DIAB&quot; &quot;DIS_AMI&quot; [11] &quot;GENDER&quot; &quot;PM_BMI_CATEGORICAL&quot; Perform some data descriptive analyses ds.table(&quot;D$DIS_DIAB&quot;) Data in all studies were valid Study 1 : No errors reported from this study $output.list $output.list$TABLE_rvar.by.study_row.props study D$DIS_DIAB 1 0 1 1 1 $output.list$TABLE_rvar.by.study_col.props study D$DIS_DIAB 1 0 0.98613037 1 0.01386963 $output.list$TABLE_rvar.by.study_counts study D$DIS_DIAB 1 0 2133 1 30 $output.list$TABLES.COMBINED_all.sources_proportions D$DIS_DIAB 0 1 0.9860 0.0139 $output.list$TABLES.COMBINED_all.sources_counts D$DIS_DIAB 0 1 2133 30 $validity.message [1] &quot;Data in all studies were valid&quot; ds.table(&quot;D$DIS_DIAB&quot;, &quot;D$GENDER&quot;) Data in all studies were valid Study 1 : No errors reported from this study $output.list $output.list$TABLE.STUDY.1_row.props D$GENDER D$DIS_DIAB 0 1 0 0.502 0.498 1 0.700 0.300 $output.list$TABLE.STUDY.1_col.props D$GENDER D$DIS_DIAB 0 1 0 0.9810 0.9920 1 0.0192 0.0084 $output.list$TABLES.COMBINED_all.sources_row.props D$GENDER D$DIS_DIAB 0 1 0 0.502 0.498 1 0.700 0.300 $output.list$TABLES.COMBINED_all.sources_col.props D$GENDER D$DIS_DIAB 0 1 0 0.9810 0.9920 1 0.0192 0.0084 $output.list$TABLE_STUDY.1_counts D$GENDER D$DIS_DIAB 0 1 0 1071 1062 1 21 9 $output.list$TABLES.COMBINED_all.sources_counts D$GENDER D$DIS_DIAB 0 1 0 1071 1062 1 21 9 $validity.message [1] &quot;Data in all studies were valid&quot; Or even some statistical modelling. In this case we want to assess whether sex (GENDER) or triglycerides (LAB_TRIG) are risk factors for diabetes (DIS_DIAB) mod &lt;- ds.glm(DIS_DIAB ~ LAB_TRIG + GENDER, data = &quot;D&quot; , family=&quot;binomial&quot;) mod$coeff Estimate Std. Error z-value p-value low0.95CI.LP high0.95CI.LP P_OR (Intercept) -5.1696619 0.4549328 -11.363572 6.349427e-30 -6.0613138 -4.2780099 0.005654338 LAB_TRIG 0.3813891 0.1037611 3.675647 2.372471e-04 0.1780211 0.5847570 1.464317247 GENDER -0.2260851 0.4375864 -0.516664 6.053908e-01 -1.0837387 0.6315685 0.797650197 low0.95CI.P_OR high0.95CI.P_OR (Intercept) 0.002325913 0.01368049 LAB_TRIG 1.194850574 1.79455494 GENDER 0.338328242 1.88055787 As usual the connection must be closed datashield.logout(conns) 12.2 Analysis from a multiple studies Now, let us illustrate a similar analysis with multiple studies. In this case we see results aggregated across all three studies. library(DSOpal) library(dsBaseClient) # prepare login data and resources to assign builder &lt;- DSI::newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.CNSIM1&quot;, driver = &quot;OpalDriver&quot;) builder$append(server = &quot;study2&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.CNSIM2&quot;, driver = &quot;OpalDriver&quot;) builder$append(server = &quot;study3&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.CNSIM3&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() # login and assign resources conns &lt;- datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) # assigned objects are of class ResourceClient (and others) ds.class(&quot;res&quot;) $study1 [1] &quot;SQLResourceClient&quot; &quot;ResourceClient&quot; &quot;R6&quot; $study2 [1] &quot;TidyFileResourceClient&quot; &quot;FileResourceClient&quot; &quot;ResourceClient&quot; &quot;R6&quot; $study3 [1] &quot;TidyFileResourceClient&quot; &quot;FileResourceClient&quot; &quot;ResourceClient&quot; &quot;R6&quot; # coerce ResourceClient objects to data.frames # (DataSHIELD config allows as.resource.data.frame() assignment function for the purpose of the demo) datashield.assign.expr(conns, symbol = &quot;D&quot;, expr = quote(as.resource.data.frame(res, strict = TRUE))) ds.class(&quot;D&quot;) $study1 [1] &quot;data.frame&quot; $study2 [1] &quot;data.frame&quot; $study3 [1] &quot;data.frame&quot; # do usual dsBase analysis ds.summary(&#39;D$LAB_HDL&#39;) $study1 $study1$class [1] &quot;numeric&quot; $study1$length [1] 2163 $study1$`quantiles &amp; mean` 5% 10% 25% 50% 75% 90% 95% Mean 0.875240 1.047400 1.300000 1.581000 1.844500 2.090000 2.210900 1.569416 $study2 $study2$class [1] &quot;numeric&quot; $study2$length [1] 3088 $study2$`quantiles &amp; mean` 5% 10% 25% 50% 75% 90% 95% Mean 0.850280 1.032200 1.294000 1.563000 1.840000 2.077000 2.225000 1.556648 $study3 $study3$class [1] &quot;numeric&quot; $study3$length [1] 4128 $study3$`quantiles &amp; mean` 5% 10% 25% 50% 75% 90% 95% Mean 0.876760 1.039200 1.304000 1.589000 1.856000 2.098800 2.244200 1.574687 # vector types are not necessarily the same depending on the data reader that was used ds.class(&#39;D$GENDER&#39;) $study1 [1] &quot;integer&quot; $study2 [1] &quot;haven_labelled&quot; &quot;vctrs_vctr&quot; &quot;double&quot; $study3 [1] &quot;numeric&quot; ds.asFactor(&#39;D$GENDER&#39;, &#39;GENDER&#39;) $all.unique.levels [1] &quot;0&quot; &quot;1&quot; $return.message [1] &quot;Data object &lt;GENDER&gt; correctly created in all specified data sources&quot; ds.summary(&#39;GENDER&#39;) $study1 $study1$class [1] &quot;factor&quot; $study1$length [1] 2163 $study1$categories [1] &quot;0&quot; &quot;1&quot; $study1$`count of &#39;0&#39;` [1] 1092 $study1$`count of &#39;1&#39;` [1] 1071 $study2 $study2$class [1] &quot;factor&quot; $study2$length [1] 3088 $study2$categories [1] &quot;0&quot; &quot;1&quot; $study2$`count of &#39;0&#39;` [1] 1585 $study2$`count of &#39;1&#39;` [1] 1503 $study3 $study3$class [1] &quot;factor&quot; $study3$length [1] 4128 $study3$categories [1] &quot;0&quot; &quot;1&quot; $study3$`count of &#39;0&#39;` [1] 2091 $study3$`count of &#39;1&#39;` [1] 2037 mod &lt;- ds.glm(&quot;DIS_DIAB ~ LAB_TRIG + GENDER&quot;, data = &quot;D&quot; , family=&quot;binomial&quot;) mod$coeff Estimate Std. Error z-value p-value low0.95CI.LP high0.95CI.LP P_OR (Intercept) -4.7792110 0.21081170 -22.670521 8.755236e-114 -5.1923944 -4.36602770 0.00833261 LAB_TRIG 0.3035931 0.05487436 5.532514 3.156737e-08 0.1960414 0.41114488 1.35471774 GENDER -0.4455989 0.20797931 -2.142516 3.215202e-02 -0.8532309 -0.03796695 0.64044060 low0.95CI.P_OR high0.95CI.P_OR (Intercept) 0.005527953 0.01254229 LAB_TRIG 1.216577226 1.50854390 GENDER 0.426036242 0.96274475 datashield.logout(conns) "],
["Omic.html", "13 Omic data analysis: types of implemented analyses", " 13 Omic data analysis: types of implemented analyses The Figure 13.1 describes the different types of ’omic association analyses that can be performed using DataSHIELD client functions implemented in the dsOmicsClient package. Basically, data (’omic and phenotypes/covariates) can be stored in different sites (http, ssh, AWS S3, local, …) and are managed with Opal through the resourcer package and their extensions implemented in dsOmics. Figure 13.1: Non-disclosive omic data analysis with DataSHIELD and Bioconductor. The figure illustrates how the resourcer package is used to get access to omic data through the Opal servers. Then DataSHIELD is used in the client side to perform non-disclosive data analyses. Then, dsOmicsClient package allows different types of analyses: pooled and meta-analysis. Both methods are based on fitting different Generalized Linear Models (GLMs) for each feature when assesing association between ’omic data and the phenotype/trait/condition of interest. Of course, non-disclosive ’omic data analysis from a single study can also be performed. The pooled approach (Figure 13.2) is recommended when the user wants to analyze ’omic data from different sources and obtain results as if the data were located in a single computer. It should be noted that this can be very time consuming when analyzing multiple features since it calls a base function in DataSHIELD (ds.glm) repeatedly. It also cannot be recommended when data are not properly harmonized (e.g. gene expression normalized using different methods, GWAS data having different platforms, …). Furthermore when it is necesary to remove unwanted variability (for transcriptomic and epigenomica analysis) or control for population stratification (for GWAS analysis), this approach cannot be used since we need to develop methods to compute surrogate variables (to remove unwanted variability) or PCAs (to to address population stratification) in a non-disclosive way. The meta-analysis approach Figure 13.3 overcomes the limitations raised when performing pooled analyses. First, the computation issue is addressed by using scalable and fast methods to perform data analysis at whole-genome level at each location The transcriptomic and epigenomic data analyses make use of the widely used limma package that uses ExpressionSet or RangedSummarizedExperiment Bioc infrastructures to deal with ’omic and phenotypic (e.g covariates). The genomic data are analyzed using GWASTools and GENESIS that are designed to perform quality control (QC) and GWAS using GDS infrastructure. Next, we describe how both approaches are implemented: Pooled approach: Figure 13.2 illustrate how this analysis is performed. This corresponds to generalized linear models (glm) on data from single or multiple sources. It makes use of ds.glm() function which is a DataSHIELD function that uses an approach that is mathematically equivalent to placing all individual-level data froma all sources in one central warehouse and analysing those data using the conventional glm() function in R. The user can select one (or multiple) features (i.e., genes, transcripts, CpGs, SNPs, …) Figure 13.2: Non-disclosive omic data analysis with DataSHIELD and Bioconductor. The figure illustrates how to perform single pooled omic data analysis. The analyses are performed by using a generalized linear model (glm) on data from one or multiple sources. It makes use of ds.glm(), a DataSHIELD function, that uses an approach that is mathematically equivalent to placing all individual-level data from all sources in one central warehouse and analysing those data using the conventional glm() function in R. Meta-analysis: Figure 13.3 illustrate how this analysis is performed. This corresponds to performing a genome-wide analysis at each location using functions that are specifically design for that purpose and that are scalable. Then the results from each location can be meta-analyzed using methods that meta-analyze either effect sizes or p-values. Figure 13.3: Non-disclosive omic data analysis with DataSHIELD and Bioconductor. The figure illustrates how to perform anlyses at genome-wide level from one or multiple sources. It runs standard Bioconductor functions at each server independently to speed up the analyses and in the case of having multiple sources, results can be meta-analyzed uning standar R functions. "],
["differential-gene-expression-dge-analysis.html", "14 Differential gene expression (DGE) analysis", " 14 Differential gene expression (DGE) analysis Let us illustrate how to perform transcriptomic data analysis using data from TCGA project. We have uploaded to the opal server a resource called tcga_liver whose URL is http://duffel.rail.bio/recount/TCGA/rse_gene_liver.Rdata which is available through the recount project. This resource contains the RangeSummarizedExperiment with the RNAseq profiling of liver cancer data from TCGA. Next, we illustrate how a differential expression analysis to compare RNAseq profiling of women vs men (variable gdc_cases.demographic.gender). The DGE analysis is normally performed using limma package. In that case, as we are analyzing RNA-seq data, limma + voom method will be required. Let us start by creating the connection to the opal server: builder &lt;- newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.tcga_liver&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() conns &lt;- datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) Then, let us coerce the resource to a RangedSummarizedExperiment which is the type of object that is available in the recount project. datashield.assign.expr(conns, symbol = &quot;rse&quot;, expr = quote(as.resource.object(res))) ds.class(&quot;rse&quot;) $study1 [1] &quot;RangedSummarizedExperiment&quot; attr(,&quot;package&quot;) [1] &quot;SummarizedExperiment&quot; The number of features and samples can be inspected by ds.dim(&quot;rse&quot;) $`dimensions of rse in study1` [1] 58037 424 $`dimensions of rse in combined studies` [1] 58037 424 And the names of the features using the same function used in the case of analyzing an ExpressionSet name.features &lt;- ds.featureNames(&quot;rse&quot;) lapply(name.features, head) $study1 [1] &quot;ENSG00000000003.14&quot; &quot;ENSG00000000005.5&quot; &quot;ENSG00000000419.12&quot; &quot;ENSG00000000457.13&quot; &quot;ENSG00000000460.16&quot; [6] &quot;ENSG00000000938.12&quot; Also the covariate names can be inspected by name.vars &lt;- ds.featureData(&quot;rse&quot;) lapply(name.vars, head, n=15) $study1 [1] &quot;project&quot; &quot;sample&quot; [3] &quot;experiment&quot; &quot;run&quot; [5] &quot;read_count_as_reported_by_sra&quot; &quot;reads_downloaded&quot; [7] &quot;proportion_of_reads_reported_by_sra_downloaded&quot; &quot;paired_end&quot; [9] &quot;sra_misreported_paired_end&quot; &quot;mapped_read_count&quot; [11] &quot;auc&quot; &quot;sharq_beta_tissue&quot; [13] &quot;sharq_beta_cell_type&quot; &quot;biosample_submission_date&quot; [15] &quot;biosample_publication_date&quot; We can visualize the levels of the variable having gender information ds.table(&quot;rse$gdc_cases.demographic.gender&quot;) Data in all studies were valid Study 1 : No errors reported from this study $output.list $output.list$TABLE_rvar.by.study_row.props study rse$gdc_cases.demographic.gender 1 female 1 male 1 $output.list$TABLE_rvar.by.study_col.props study rse$gdc_cases.demographic.gender 1 female 0.3372642 male 0.6627358 $output.list$TABLE_rvar.by.study_counts study rse$gdc_cases.demographic.gender 1 female 143 male 281 $output.list$TABLES.COMBINED_all.sources_proportions rse$gdc_cases.demographic.gender female male 0.337 0.663 $output.list$TABLES.COMBINED_all.sources_counts rse$gdc_cases.demographic.gender female male 143 281 $validity.message [1] &quot;Data in all studies were valid&quot; The differential expression analysis is then performed by: ans.gender &lt;- ds.limma(model = ~ gdc_cases.demographic.gender, Set = &quot;rse&quot;, type.data = &quot;RNAseq&quot;, sva = FALSE) Notice that we have set type.data='RNAseq' to consider that our data are counts obtained from a RNA-seq experiment. By indicating so, the differential analysis is performed by using voom + limma as previously mention. The top differentially expressed genes can be visualized by: ans.gender $study1 # A tibble: 58,037 x 10 id logFC CI.L CI.R AveExpr t P.Value adj.P.Val B SE &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 ENSG00000233070.1 10.9 10.5 11.3 -5.59 53.6 7.05e-191 4.09e-186 402. 0.0761 2 ENSG00000213318.4 11.4 10.9 11.8 -4.35 49.4 5.93e-178 1.72e-173 376. 0.462 3 ENSG00000067048.16 9.63 9.24 10.0 0.856 47.9 5.05e-173 9.78e-169 366. 0.0608 4 ENSG00000260197.1 10.2 9.76 10.6 -5.88 46.3 5.27e-168 7.65e-164 355. 0.0654 5 ENSG00000012817.15 11.3 10.8 11.8 -0.226 44.7 1.29e-162 1.50e-158 344. 0.0880 6 ENSG00000131002.11 11.4 10.9 11.9 -1.10 44.5 7.80e-162 7.55e-158 343. 0.120 7 ENSG00000198692.9 12.3 11.8 12.9 -0.882 44.4 1.62e-161 1.34e-157 342. 0.129 8 ENSG00000183878.15 8.66 8.27 9.05 -0.781 43.9 9.83e-160 7.13e-156 338. 0.0813 9 ENSG00000129824.15 11.4 10.8 11.9 2.37 43.3 1.44e-157 9.29e-154 334. 0.0896 10 ENSG00000274655.1 -12.4 -12.9 -11.8 -7.71 -43.1 5.20e-157 3.02e-153 333. 0.0931 # ... with 58,027 more rows attr(,&quot;class&quot;) [1] &quot;dsLimma&quot; &quot;list&quot; We have also implemented two other functions ds.DESeq2 and ds.edgeR that perform DGE analysis using DESeq2 and edgeR methods. This is the R code used to that purpose: To be supplied We close the DataSHIELD session by: datashield.logout(conns) "],
["epigenome-wide-association-analysis-ewas.html", "15 Epigenome-wide association analysis (EWAS) 15.1 Single CpG analysis 15.2 Multiple CpG analysis 15.3 Adjusting for Surrogate Variables", " 15 Epigenome-wide association analysis (EWAS) EWAS requires basically the same statistical methods as those used in DGE. It should be notice that the pooled analysis we are going to illustrate here can also be performed with transcriptomic data since each study must have different range values. If so, gene expression harmonization should be performed, for instance, by standardizing the data at each study. For EWAS where methylation is measured using beta values (e.g CpG data are in the range 0-1) this is not a problem. In any case, adopting the meta-analysis approach could be a safe option. We have downloaded data from GEO corresponding to the accesion number GSE66351 which includes DNA methylation profiling (Illumina 450K array) of 190 individuals. Data corresponds to CpGs beta values measured in the superior temporal gyrus and prefrontal cortex brain regions of patients with Alzheimer’s. Data have been downloaded using GEOquery package that gets GEO data as ExpressionSet objects. Researchers who are not familiar with ExpressionSets can read this Section. Notice that data are encoded as beta-values that ensure data harmonization across studies. In order to illustrate how to perform data analyses using federated data, we have split the data into two ExpressionSets having 100 and 90 samples as if they were two different studies. Figure 6.2 shows the two resources defined for both studies (GSE66351_1 and GSE66351_2) In order to perform omic data analyses, we need first to login and assign resources to DataSHIELD. This can be performed using the as.resource.object() function builder &lt;- DSI::newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.GSE66351_1&quot;, driver = &quot;OpalDriver&quot;) builder$append(server = &quot;study2&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.GSE66351_2&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() conns &lt;- DSI::datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) # Assign to the original R class (e.g ExpressionSet) datashield.assign.expr(conns, symbol = &quot;methy&quot;, expr = quote(as.resource.object(res))) Now, we can see that the resources are actually loaded into the R servers as their original class ds.class(&quot;methy&quot;) $study1 [1] &quot;ExpressionSet&quot; attr(,&quot;package&quot;) [1] &quot;Biobase&quot; $study2 [1] &quot;ExpressionSet&quot; attr(,&quot;package&quot;) [1] &quot;Biobase&quot; Then, some Bioconductor-type functions can be use to return non-disclosive information of ExpressionSets from each server to the client, using similar functions as those defined in the dsBaseClient package. For example, feature names can be returned by fn &lt;- ds.featureNames(&quot;methy&quot;) lapply(fn, head) $study1 [1] &quot;cg00000029&quot; &quot;cg00000108&quot; &quot;cg00000109&quot; &quot;cg00000165&quot; &quot;cg00000236&quot; &quot;cg00000289&quot; $study2 [1] &quot;cg00000029&quot; &quot;cg00000108&quot; &quot;cg00000109&quot; &quot;cg00000165&quot; &quot;cg00000236&quot; &quot;cg00000289&quot; Experimental phenotypes variables can be obtained by ds.varLabels(&quot;methy&quot;) $study1 [1] &quot;title&quot; &quot;geo_accession&quot; &quot;status&quot; &quot;submission_date&quot; [5] &quot;last_update_date&quot; &quot;type&quot; &quot;channel_count&quot; &quot;source_name_ch1&quot; [9] &quot;organism_ch1&quot; &quot;characteristics_ch1&quot; &quot;characteristics_ch1.1&quot; &quot;characteristics_ch1.2&quot; [13] &quot;characteristics_ch1.3&quot; &quot;characteristics_ch1.4&quot; &quot;characteristics_ch1.5&quot; &quot;characteristics_ch1.6&quot; [17] &quot;characteristics_ch1.7&quot; &quot;characteristics_ch1.8&quot; &quot;molecule_ch1&quot; &quot;extract_protocol_ch1&quot; [21] &quot;label_ch1&quot; &quot;label_protocol_ch1&quot; &quot;taxid_ch1&quot; &quot;hyb_protocol&quot; [25] &quot;scan_protocol&quot; &quot;description&quot; &quot;data_processing&quot; &quot;platform_id&quot; [29] &quot;contact_name&quot; &quot;contact_email&quot; &quot;contact_phone&quot; &quot;contact_laboratory&quot; [33] &quot;contact_institute&quot; &quot;contact_address&quot; &quot;contact_city&quot; &quot;contact_zip/postal_code&quot; [37] &quot;contact_country&quot; &quot;supplementary_file&quot; &quot;supplementary_file.1&quot; &quot;data_row_count&quot; [41] &quot;age&quot; &quot;braak_stage&quot; &quot;brain_region&quot; &quot;cell type&quot; [45] &quot;diagnosis&quot; &quot;donor_id&quot; &quot;sentrix_id&quot; &quot;sentrix_position&quot; [49] &quot;Sex&quot; $study2 [1] &quot;title&quot; &quot;geo_accession&quot; &quot;status&quot; &quot;submission_date&quot; [5] &quot;last_update_date&quot; &quot;type&quot; &quot;channel_count&quot; &quot;source_name_ch1&quot; [9] &quot;organism_ch1&quot; &quot;characteristics_ch1&quot; &quot;characteristics_ch1.1&quot; &quot;characteristics_ch1.2&quot; [13] &quot;characteristics_ch1.3&quot; &quot;characteristics_ch1.4&quot; &quot;characteristics_ch1.5&quot; &quot;characteristics_ch1.6&quot; [17] &quot;characteristics_ch1.7&quot; &quot;characteristics_ch1.8&quot; &quot;molecule_ch1&quot; &quot;extract_protocol_ch1&quot; [21] &quot;label_ch1&quot; &quot;label_protocol_ch1&quot; &quot;taxid_ch1&quot; &quot;hyb_protocol&quot; [25] &quot;scan_protocol&quot; &quot;description&quot; &quot;data_processing&quot; &quot;platform_id&quot; [29] &quot;contact_name&quot; &quot;contact_email&quot; &quot;contact_phone&quot; &quot;contact_laboratory&quot; [33] &quot;contact_institute&quot; &quot;contact_address&quot; &quot;contact_city&quot; &quot;contact_zip/postal_code&quot; [37] &quot;contact_country&quot; &quot;supplementary_file&quot; &quot;supplementary_file.1&quot; &quot;data_row_count&quot; [41] &quot;age&quot; &quot;braak_stage&quot; &quot;brain_region&quot; &quot;cell type&quot; [45] &quot;diagnosis&quot; &quot;donor_id&quot; &quot;sentrix_id&quot; &quot;sentrix_position&quot; [49] &quot;Sex&quot; attr(,&quot;class&quot;) [1] &quot;dsvarLabels&quot; &quot;list&quot; 15.1 Single CpG analysis Once the methylation data have been loaded into the opal server, we can perform different type of analyses using functions from the dsOmicsClient package. Let us start by illustrating how to analyze a single CpG from two studies by using an approach that is mathematically equivalent to placing all individual-level. ans &lt;- ds.lmFeature(feature = &quot;cg07363416&quot;, model = ~ diagnosis + Sex, Set = &quot;methy&quot;, datasources = conns) ans Estimate Std. Error p-value cg07363416 0.03459886 0.02504291 0.1670998 attr(,&quot;class&quot;) [1] &quot;dsLmFeature&quot; &quot;matrix&quot; &quot;array&quot; 15.2 Multiple CpG analysis The same analysis can be performed for all features (e.g. CpGs) just avoiding the feature argument. This process can be parallelized using mclapply function from the multicore package. ans &lt;- ds.lmFeature(model = ~ diagnosis + Sex, Set = &quot;methy&quot;, datasources = conns, mc.cores = 20) This method corresponds to the pooled analysis approach and can be very time consiming since the function repeatedly calls the DataSHIELD function ds.glm(). We can adopt another strategy that is to run a glm of each feature independently at each study using limma package (which is really fast) and then combine the results (i.e. meta-analysis approach). ans.limma &lt;- ds.limma(model = ~ diagnosis + Sex, Set = &quot;methy&quot;, datasources = conns) Then, we can visualize the top genes at each study (i.e server) by lapply(ans.limma, head) $study1 # A tibble: 6 x 10 id logFC CI.L CI.R AveExpr t P.Value adj.P.Val B SE &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg13138089 -0.147 -0.191 -0.103 0.380 -6.62 0.00000000190 0.000466 10.6 0.0122 2 cg23859635 -0.0569 -0.0741 -0.0397 0.200 -6.58 0.00000000232 0.000466 10.4 0.00520 3 cg13772815 -0.0820 -0.107 -0.0570 0.437 -6.50 0.00000000327 0.000466 10.0 0.0135 4 cg12706938 -0.0519 -0.0678 -0.0359 0.145 -6.45 0.00000000425 0.000466 9.76 0.00872 5 cg24724506 -0.0452 -0.0593 -0.0312 0.139 -6.39 0.00000000547 0.000466 9.51 0.00775 6 cg02812891 -0.125 -0.165 -0.0860 0.247 -6.33 0.00000000731 0.000466 9.23 0.0163 $study2 # A tibble: 6 x 10 id logFC CI.L CI.R AveExpr t P.Value adj.P.Val B SE &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg04046629 -0.101 -0.135 -0.0669 0.345 -5.91 0.0000000621 0.0172 7.18 0.0128 2 cg07664323 -0.0431 -0.0577 -0.0284 0.776 -5.85 0.0000000822 0.0172 6.90 0.00390 3 cg27098804 -0.0688 -0.0924 -0.0452 0.277 -5.79 0.000000107 0.0172 6.64 0.0147 4 cg08933615 -0.0461 -0.0627 -0.0296 0.166 -5.55 0.000000298 0.0360 5.64 0.00791 5 cg18349298 -0.0491 -0.0671 -0.0311 0.157 -5.42 0.000000507 0.0489 5.12 0.00848 6 cg02182795 -0.0199 -0.0272 -0.0125 0.0947 -5.36 0.000000670 0.0538 4.84 0.0155 The annotation can be added by using the argument annotCols. It should be a vector with the columns of the annotation available in the ExpressionSet or RangedSummarizedExperiment that want to be showed. The columns of the annotation can be obtained by ds.fvarLabels(&quot;methy&quot;) $study1 [1] &quot;ID&quot; &quot;Name&quot; &quot;AddressA_ID&quot; [4] &quot;AlleleA_ProbeSeq&quot; &quot;AddressB_ID&quot; &quot;AlleleB_ProbeSeq&quot; [7] &quot;Infinium_Design_Type&quot; &quot;Next_Base&quot; &quot;Color_Channel&quot; [10] &quot;Forward_Sequence&quot; &quot;Genome_Build&quot; &quot;CHR&quot; [13] &quot;MAPINFO&quot; &quot;SourceSeq&quot; &quot;Chromosome_36&quot; [16] &quot;Coordinate_36&quot; &quot;Strand&quot; &quot;Probe_SNPs&quot; [19] &quot;Probe_SNPs_10&quot; &quot;Random_Loci&quot; &quot;Methyl27_Loci&quot; [22] &quot;UCSC_RefGene_Name&quot; &quot;UCSC_RefGene_Accession&quot; &quot;UCSC_RefGene_Group&quot; [25] &quot;UCSC_CpG_Islands_Name&quot; &quot;Relation_to_UCSC_CpG_Island&quot; &quot;Phantom&quot; [28] &quot;DMR&quot; &quot;Enhancer&quot; &quot;HMM_Island&quot; [31] &quot;Regulatory_Feature_Name&quot; &quot;Regulatory_Feature_Group&quot; &quot;DHS&quot; [34] &quot;RANGE_START&quot; &quot;RANGE_END&quot; &quot;RANGE_GB&quot; [37] &quot;SPOT_ID&quot; $study2 [1] &quot;ID&quot; &quot;Name&quot; &quot;AddressA_ID&quot; [4] &quot;AlleleA_ProbeSeq&quot; &quot;AddressB_ID&quot; &quot;AlleleB_ProbeSeq&quot; [7] &quot;Infinium_Design_Type&quot; &quot;Next_Base&quot; &quot;Color_Channel&quot; [10] &quot;Forward_Sequence&quot; &quot;Genome_Build&quot; &quot;CHR&quot; [13] &quot;MAPINFO&quot; &quot;SourceSeq&quot; &quot;Chromosome_36&quot; [16] &quot;Coordinate_36&quot; &quot;Strand&quot; &quot;Probe_SNPs&quot; [19] &quot;Probe_SNPs_10&quot; &quot;Random_Loci&quot; &quot;Methyl27_Loci&quot; [22] &quot;UCSC_RefGene_Name&quot; &quot;UCSC_RefGene_Accession&quot; &quot;UCSC_RefGene_Group&quot; [25] &quot;UCSC_CpG_Islands_Name&quot; &quot;Relation_to_UCSC_CpG_Island&quot; &quot;Phantom&quot; [28] &quot;DMR&quot; &quot;Enhancer&quot; &quot;HMM_Island&quot; [31] &quot;Regulatory_Feature_Name&quot; &quot;Regulatory_Feature_Group&quot; &quot;DHS&quot; [34] &quot;RANGE_START&quot; &quot;RANGE_END&quot; &quot;RANGE_GB&quot; [37] &quot;SPOT_ID&quot; attr(,&quot;class&quot;) [1] &quot;dsfvarLabels&quot; &quot;list&quot; Then we can run the analysis and obtain the output with the chromosome and gene symbol by: ans.limma.annot &lt;- ds.limma(model = ~ diagnosis + Sex, Set = &quot;methy&quot;, annotCols = c(&quot;CHR&quot;, &quot;UCSC_RefGene_Name&quot;), datasources = conns) lapply(ans.limma.annot, head) $study1 # A tibble: 6 x 12 id CHR UCSC_RefGene_Name logFC CI.L CI.R AveExpr t P.Value adj.P.Val B SE &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg131380~ 2 &quot;ECEL1P2&quot; -0.147 -0.191 -0.103 0.380 -6.62 1.90e-9 0.000466 10.6 0.0122 2 cg238596~ 2 &quot;MTA3&quot; -0.0569 -0.0741 -0.0397 0.200 -6.58 2.32e-9 0.000466 10.4 0.00520 3 cg137728~ 17 &quot;&quot; -0.0820 -0.107 -0.0570 0.437 -6.50 3.27e-9 0.000466 10.0 0.0135 4 cg127069~ 19 &quot;MEX3D&quot; -0.0519 -0.0678 -0.0359 0.145 -6.45 4.25e-9 0.000466 9.76 0.00872 5 cg247245~ 19 &quot;ISOC2;ISOC2;ISOC2&quot; -0.0452 -0.0593 -0.0312 0.139 -6.39 5.47e-9 0.000466 9.51 0.00775 6 cg028128~ 2 &quot;ECEL1P2&quot; -0.125 -0.165 -0.0860 0.247 -6.33 7.31e-9 0.000466 9.23 0.0163 $study2 # A tibble: 6 x 12 id CHR UCSC_RefGene_Name logFC CI.L CI.R AveExpr t P.Value adj.P.Val B SE &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg04046629 11 &quot;CD6&quot; -0.101 -0.135 -0.0669 0.345 -5.91 0.0000000621 0.0172 7.18 0.0128 2 cg07664323 6 &quot;MUC21&quot; -0.0431 -0.0577 -0.0284 0.776 -5.85 0.0000000822 0.0172 6.90 0.00390 3 cg27098804 11 &quot;CD6&quot; -0.0688 -0.0924 -0.0452 0.277 -5.79 0.000000107 0.0172 6.64 0.0147 4 cg08933615 1 &quot;&quot; -0.0461 -0.0627 -0.0296 0.166 -5.55 0.000000298 0.0360 5.64 0.00791 5 cg18349298 3 &quot;RARRES1;RARRES1&quot; -0.0491 -0.0671 -0.0311 0.157 -5.42 0.000000507 0.0489 5.12 0.00848 6 cg02182795 8 &quot;&quot; -0.0199 -0.0272 -0.0125 0.0947 -5.36 0.000000670 0.0538 4.84 0.0155 Then, the last step is to meta-analyze the results. Different methods can be used to this end. We have implemented a method that meta-analyze the p-pvalues of each study as follows: ans.meta &lt;- metaPvalues(ans.limma) ans.meta # A tibble: 481,868 x 4 id study1 study2 p.meta &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg13138089 0.00000000190 0.00000763 4.78e-13 2 cg25317941 0.0000000179 0.00000196 1.12e-12 3 cg02812891 0.00000000731 0.00000707 1.63e-12 4 cg12706938 0.00000000425 0.0000161 2.14e-12 5 cg16026647 0.000000101 0.000000797 2.51e-12 6 cg12695465 0.00000000985 0.0000144 4.33e-12 7 cg21171625 0.000000146 0.00000225 9.78e-12 8 cg13772815 0.00000000327 0.000122 1.18e-11 9 cg00228891 0.000000166 0.00000283 1.38e-11 10 cg21488617 0.0000000186 0.0000299 1.62e-11 # ... with 481,858 more rows We can verify that the results are pretty similar to those obtained using pooled analyses. Here we compute the association for two of the top-CpGs: res1 &lt;- ds.lmFeature(feature = &quot;cg13138089&quot;, model = ~ diagnosis + Sex, Set = &quot;methy&quot;, datasources = conns) res1 Estimate Std. Error p-value cg13138089 -0.1373348 0.01712405 1.057482e-15 attr(,&quot;class&quot;) [1] &quot;dsLmFeature&quot; &quot;matrix&quot; &quot;array&quot; res2 &lt;- ds.lmFeature(feature = &quot;cg13772815&quot;, model = ~ diagnosis + Sex, Set = &quot;methy&quot;, datasources = conns) res2 Estimate Std. Error p-value cg13772815 -0.06786137 0.009128915 1.056225e-13 attr(,&quot;class&quot;) [1] &quot;dsLmFeature&quot; &quot;matrix&quot; &quot;array&quot; We can create a QQ-plot by using the function qqplot available in our package. qqplot(ans.meta$p.meta) Here In some cases inflation can be observed, so that, correction for cell-type or surrogate variables must be performed. We describe how we can do that in the next two sections. 15.3 Adjusting for Surrogate Variables The vast majority of omic studies require to control for unwanted variability. The surrogate variable analysis (SVA) can address this issue by estimating some hidden covariates that capture differences across individuals due to some artifacts such as batch effects or sample quality sam among others. The method is implemented in SVA package. Performing this type of analysis using the ds.lmFeature function is not allowed since estimating SVA would require to implement a non-disclosive method that computes SVA from the different servers. This will be a future topic of the dsOmicsClient. NOTE that, estimating SVA separately at each server would not be a good idea since the aim of SVA is to capture differences mainly due to experimental issues among ALL individuals. What we can do instead is to use the ds.limma function to perform the analyses adjusted for SVA at each study. ans.sva &lt;- ds.limma(model = ~ diagnosis + Sex, Set = &quot;methy&quot;, sva = TRUE, annotCols = c(&quot;CHR&quot;, &quot;UCSC_RefGene_Name&quot;)) ans.sva $study1 # A tibble: 481,868 x 12 id CHR UCSC_RefGene_Name logFC CI.L CI.R AveExpr t P.Value adj.P.Val B SE &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg101814~ 19 &quot;GNG7&quot; -0.0547 -0.0721 -0.0373 0.338 -6.25 1.31e-8 0.00591 8.51 0.0103 2 cg137728~ 17 &quot;&quot; -0.0569 -0.0757 -0.0381 0.437 -6.00 3.91e-8 0.00591 7.43 0.00425 3 cg118020~ 19 &quot;PODNL1;PODNL1;POD~ 0.0334 0.0223 0.0445 0.568 5.97 4.53e-8 0.00591 7.29 0.00897 4 cg272310~ 17 &quot;SLC47A2;SLC47A2&quot; 0.0274 0.0182 0.0365 0.548 5.95 4.91e-8 0.00591 7.21 0.00696 5 cg210789~ 6 &quot;&quot; -0.0453 -0.0609 -0.0297 0.799 -5.78 1.05e-7 0.00666 6.46 0.00733 6 cg238596~ 2 &quot;MTA3&quot; -0.0327 -0.0440 -0.0215 0.200 -5.77 1.09e-7 0.00666 6.42 0.0149 7 cg102974~ 16 &quot;SALL1;SALL1&quot; 0.0366 0.0240 0.0492 0.137 5.77 1.10e-7 0.00666 6.42 0.00688 8 cg249245~ 1 &quot;&quot; 0.0317 0.0208 0.0427 0.821 5.76 1.11e-7 0.00666 6.41 0.00969 9 cg136631~ 9 &quot;&quot; -0.0326 -0.0440 -0.0213 0.367 -5.72 1.32e-7 0.00705 6.24 0.00562 10 cg131380~ 2 &quot;ECEL1P2&quot; -0.106 -0.144 -0.0686 0.380 -5.61 2.15e-7 0.00835 5.76 0.00137 # ... with 481,858 more rows $study2 # A tibble: 481,868 x 12 id CHR UCSC_RefGene_Na~ logFC CI.L CI.R AveExpr t P.Value adj.P.Val B SE &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg16766632 12 &quot;LRP1&quot; -0.0418 -0.0548 -0.0289 0.388 -6.42 7.82e-9 0.00374 9.05 0.00917 2 cg25036710 1 &quot;&quot; -0.0664 -0.0874 -0.0453 0.569 -6.27 1.55e-8 0.00374 8.37 0.00373 3 cg12938128 11 &quot;NRXN2;NRXN2&quot; -0.0273 -0.0367 -0.0179 0.728 -5.81 1.13e-7 0.0149 6.42 0.00901 4 cg07349815 3 &quot;&quot; -0.0393 -0.0529 -0.0258 0.160 -5.78 1.24e-7 0.0149 6.32 0.00737 5 cg07664323 6 &quot;MUC21&quot; -0.0427 -0.0579 -0.0276 0.776 -5.61 2.63e-7 0.0163 5.59 0.00682 6 cg00228891 1 &quot;CR1L&quot; -0.0568 -0.0771 -0.0365 0.350 -5.56 3.18e-7 0.0163 5.40 0.0139 7 cg11743675 12 &quot;CNTN1;CNTN1&quot; -0.0443 -0.0601 -0.0284 0.152 -5.56 3.18e-7 0.0163 5.40 0.00786 8 cg03470754 7 &quot;PGAM2;PGAM2&quot; -0.0442 -0.0600 -0.0283 0.211 -5.56 3.26e-7 0.0163 5.38 0.00790 9 cg07015749 1 &quot;KCNAB2;KCNAB2&quot; 0.0611 0.0392 0.0831 0.659 5.54 3.47e-7 0.0163 5.31 0.00469 10 cg25647784 17 &quot;WNK4&quot; -0.0392 -0.0533 -0.0251 0.512 -5.52 3.70e-7 0.0163 5.25 0.00157 # ... with 481,858 more rows attr(,&quot;class&quot;) [1] &quot;dsLimma&quot; &quot;list&quot; Then, data can be combined meta-anlyzed as follows: ans.meta.sv &lt;- metaPvalues(ans.sva) ans.meta.sv # A tibble: 481,868 x 4 id study1 study2 p.meta &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg00228891 0.00000397 0.000000318 3.58e-11 2 cg01301319 0.00000609 0.00000123 2.00e-10 3 cg22962123 0.00000106 0.00000767 2.17e-10 4 cg24302412 0.0000139 0.000000762 2.77e-10 5 cg02812891 0.000000408 0.0000327 3.47e-10 6 cg23859635 0.000000109 0.000190 5.29e-10 7 cg13138089 0.000000215 0.000105 5.74e-10 8 cg24938077 0.0000125 0.00000254 8.02e-10 9 cg13772815 0.0000000391 0.00132 1.27e- 9 10 cg21212881 0.000000340 0.000248 2.04e- 9 # ... with 481,858 more rows The DataSHIELD session must by closed by: datashield.logout(conns) "],
["genome-wide-association-analyses-gwas.html", "16 Genome-wide association analyses (GWAS) 16.1 GWAS with Bioconductor 16.2 GWAS with PLINK", " 16 Genome-wide association analyses (GWAS) Genomic data can be stored in different formats. VCF and PLINK files are commonly used in genetic epidemiology studies. We have a GWAS example available at BRGE data repository that aims to find SNPs associated with asthma. We have data stored in VCF (brge.vcf) with several covariates and phenotypes available in the file brge.txt (gender, age, obesity, smoking, country and asthma status). The same data is also available in PLINK format (brge.bed, brge.bim, brge.fam) with covariates in the file brge.phe. Here we illustrate how to perform GWAS using R and Bioconductor packages or PLINK shell command line. 16.1 GWAS with Bioconductor We have created a resource having the VCF file of our study on asthma as previously described. The name of the resource is brge_vcf the phenotypes are available in another resource called brge that is a .txt file. The GWAS analysis is then perform as follows We first start by preparing login data builder &lt;- newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.brge_vcf&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() conns &lt;- datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) In this case we have to assign to different resources. One for the VCF (obesity_vcf) and another one for the phenotypic data (obesity). To this end, the datashield.assign.resource function is required before assigning any object to the specific resource. Notice that the VCF resource can be load into R as a GDS thanks to our extension of existing resources in the reourcer datashield.assign.resource(conns, symbol = &quot;vcf.res&quot;, resource = list(study1 = &quot;RSRC.brge_vcf&quot;)) datashield.assign.expr(conns, symbol = &quot;gds&quot;, expr = quote(as.resource.object(vcf.res))) datashield.assign.resource(conns, symbol = &quot;covars.res&quot;, resource = list(study1 = &quot;RSRC.brge&quot;)) datashield.assign.expr(conns, symbol = &quot;covars&quot;, expr = quote(as.resource.data.frame(covars.res))) These are the objects available in the Opal server ds.ls() $study1 $study1$environment.searched [1] &quot;R_GlobalEnv&quot; $study1$objects.found [1] &quot;covars&quot; &quot;covars.res&quot; &quot;gds&quot; &quot;res&quot; &quot;vcf.res&quot; We can use dsBaseClient functions to inspect the variables that are in the covars data.frame. The variables are ds.colnames(&quot;covars&quot;) $study1 [1] &quot;scanID&quot; &quot;gender&quot; &quot;obese&quot; &quot;age&quot; &quot;smoke&quot; &quot;country&quot; &quot;asthma&quot; The asthma variable has this number of individuals at each level (0: controls, 1: cases) ds.table(&quot;covars$asthma&quot;) Data in all studies were valid Study 1 : No errors reported from this study $output.list $output.list$TABLE_rvar.by.study_row.props study covars$asthma 1 0 1 1 1 $output.list$TABLE_rvar.by.study_col.props study covars$asthma 1 0 0.6864187 1 0.3135813 $output.list$TABLE_rvar.by.study_counts study covars$asthma 1 0 1587 1 725 $output.list$TABLES.COMBINED_all.sources_proportions covars$asthma 0 1 0.686 0.314 $output.list$TABLES.COMBINED_all.sources_counts covars$asthma 0 1 1587 725 $validity.message [1] &quot;Data in all studies were valid&quot; There may be interest in only studying certain genes, for that matter, the loaded VCF resource can be subsetting as follows genes &lt;- c(&quot;A1BG&quot;,&quot;A2MP1&quot;) ds.getSNPSbyGen(&quot;gds&quot;, genes) The previous code will over-write the VCF with the SNPs corresponding to the selected genes, if the intention is to perform studies with both the complete VCF and a subsetted one, the argument name can be used to create a new object on the server with the subsetted VCF, preserving the complete one. genes &lt;- c(&quot;A1BG&quot;,&quot;A2MP1&quot;) ds.getSNPSbyGen(&quot;gds&quot;, genes = genes, name = &quot;subset.vcf&quot;) Then, an object of class GenotypeData must be created at the server side to perform genetic data analyses. This is a container defined in the GWASTools package for storing genotype and phenotypic data from genetic association studies. By doing that we will also verify whether individuals in the GDS (e.g VCF) and covariates files have the same individuals and are in the same order. This can be performed by ds.GenotypeData(x=&#39;gds&#39;, covars = &#39;covars&#39;, columnId = 1, newobj.name = &#39;gds.Data&#39;) Before performing the association analyses, quality control (QC) can be performed to the loaded data. Three methodologies are available; 1) Principal Component Analysis (PCA) of the genomic data, 2) Hardy-Weinberg Equilibrium (HWE) testing and 3) Allelic frequency estimates. The QC methods 2 and 3 have as inputs a GenotypeData object, created with a covariates file that has a gender column; while method 1 has as input a VCF. To perform the PCA, a pruning functionality is built inside so that redundant SNPs are discarted (there is an extra argument ld.threshold which controls the pruning, more information about it at the SNPRelate documentation), speeding up the execution time ds.PCASNPS(&quot;gds&quot;, prune = TRUE) To perform QC methodologies 2 and 3, the name of the gender column as well as the keys to describe male or female have to be provided. Remember that we can visualize the names of the variables from our data by executing ds.colnames(\"covars\"). In our case, this variable is called “gender”, and the levels of this variable are 1 for male and 2 for female as we can see here (NOTE: we cannot use ds.levels since gender variable is not a factor): ds.table1D(&quot;covars$gender&quot;)$counts covars$gender 1 1215 2 1097 Total 2312 The HWE test can be performed to selected chromosomes using the argument chromosomes, only the autosomes can be selected when performing a HWE test, the encoding of the autosomes can be fetched with ds.getChromosomeNames(&quot;gds.Data&quot;)$autosomes NULL Therefore, HWE can be performed by: ds.exactHWE(&quot;gds.Data&quot;, sexcol = &quot;gender&quot;, male = &quot;1&quot;, female = &quot;2&quot;, chromosome = &quot;22&quot;) $study1 # A tibble: 1,581 x 9 snpID chr nAA nAB nBB MAF minor.allele f pval &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 95140 22 531 1082 640 0.475810031069685 A 0.0372494534103404 0.0762378420092437 2 95141 22 580 1092 590 0.497789566755084 A 0.0344638881244839 0.101115839682148 3 95142 22 0 333 1951 0.0728984238178634 A -0.0786304604486423 3.29321452000707e-06 4 95143 22 0 225 2080 0.0488069414316703 A -0.0513112884834663 0.00571353422173819 5 95144 22 176 831 1249 0.262189716312057 A 0.0479240933754878 0.025554465064758 6 95145 22 249 1003 1055 0.32531426094495 A 0.00958157673233362 0.635607524466151 7 95146 22 55 516 1738 0.135556517973149 A 0.0464603328061876 0.0325126363699498 8 95147 22 203 929 1164 0.290722996515679 A 0.0188880417746163 0.362631409282596 9 95148 22 291 1035 985 0.349848550411077 A 0.0154998317584483 0.464409674690009 10 95149 22 11 276 2013 0.0647826086956522 A 0.00966929694008412 0.604847402951204 # ... with 1,571 more rows attr(,&quot;class&quot;) [1] &quot;dsexactHWE&quot; &quot;list&quot; Similarly, allele frequencies estimates can be estimated by: ds.alleleFrequency(&quot;gds.Data&quot;, sexcol = &quot;gender&quot;, male = &quot;1&quot;, female = &quot;2&quot;) $study1 # A tibble: 99,289 x 7 M F all n.M n.F n MAF &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.015 0.0147 0.0149 1200 1086 2286 0.0149 2 0.000837 0.00276 0.00175 1195 1087 2282 0.00175 3 0.00330 0.00548 0.00433 1212 1095 2307 0.00433 4 0.0124 0.00868 0.0106 1213 1095 2308 0.0106 5 0.00165 0 0.000866 1214 1095 2309 0.000866 6 0.0712 0.0754 0.0732 1208 1094 2302 0.0732 7 0.00660 0.00913 0.00780 1213 1095 2308 0.00780 8 0.00248 0.00274 0.00261 1209 1093 2302 0.00261 9 0.182 0.181 0.181 1188 1071 2259 0.181 10 0.00165 0 0.000868 1210 1094 2304 0.000868 # ... with 99,279 more rows attr(,&quot;class&quot;) [1] &quot;dsalleleFrequency&quot; &quot;list&quot; In the future, more functions will be created to perform quality control (QC) for both, SNPs and inviduals. Association analysis for a given SNP is performed by simply ds.glmSNP(snps.fit = &quot;rs11247693&quot;, model = asthma ~ gender + age, genoData=&#39;gds.Data&#39;) Estimate Std. Error p-value rs11247693 -0.1543215 0.2309585 0.5040196 attr(,&quot;class&quot;) [1] &quot;dsGlmSNP&quot; &quot;matrix&quot; &quot;array&quot; The analysis of all available SNPs is performed when the argument snps.fit is missing. The function performs the analysis of the selected SNPs in a single repository or in multiple repositories as performing pooled analyses (it uses ds.glm DataSHIELD function). As in the case of transcriptomic data, analyzing all the SNPs in the genome (e.g GWAS) will be high time-consuming. We can adopt a similar approach as the one adopted using the limma at each server. That is, we run GWAS at each repository using specific and scalable packages available in R/Bioc. In that case we use the GWASTools and GENESIS packages. The complete pipeline is implemented in this function ans.bioC &lt;- ds.GWAS(&#39;gds.Data&#39;, model=asthma~age+country) ans.bioC $study1 # A tibble: 99,288 x 14 variant.id rs chr pos n.obs freq MAC Score Score.SE Score.Stat Score.pval Est Est.SE PVE &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 97800 rs12008773 X 64955287 2227 0.0126 56 19.5 3.84 5.07 0.000000408 1.32 0.260 0.0111 2 91449 rs2267914 20 1858988 2311 0.104 481 47.5 9.62 4.93 0.000000809 0.513 0.104 0.0105 3 19742 rs7153 3 122865987 2308 0.256 1181 62.3 13.7 4.56 0.00000508 0.334 0.0732 0.00900 4 93268 rs6097326 20 51324797 2311 0.125 579 46.4 10.3 4.51 0.00000642 0.439 0.0973 0.00881 5 19744 rs3732410 3 122898410 2308 0.254 1173 60.3 13.6 4.43 0.00000940 0.325 0.0734 0.00849 6 66984 rs11055608 12 13804693 2306 0.446 2057 -67.6 15.7 -4.31 0.0000161 -0.275 0.0638 0.00805 7 74760 rs7995146 13 111804035 2304 0.0527 243 29.7 6.96 4.27 0.0000195 0.614 0.144 0.00789 8 59678 rs7098143 10 83329037 2295 0.210 963 -54.5 12.8 -4.25 0.0000214 -0.331 0.0780 0.00781 9 13835 rs13002717 2 186197782 2311 0.00649 30 10.6 2.52 4.22 0.0000244 1.67 0.397 0.00770 10 27337 rs1602679 4 167576714 2299 0.101 463 39.7 9.46 4.20 0.0000264 0.444 0.106 0.00764 # ... with 99,278 more rows attr(,&quot;class&quot;) [1] &quot;dsGWAS&quot; &quot;list&quot; This close the DataSHIELD session datashield.logout(conns) 16.2 GWAS with PLINK Here we illustrate how to perform the same GWAS analyses on the asthma using PLINK secure shell commands. This can be performed thanks to the posibility of having ssh resources as described here. It is worth to notice that this workflow and the new R functions implemented in dsOmicsClient could be used as a guideline to carry out similar analyses using existing analysis tools in genomics such as IMPUTE, SAMtools or BEDtools among many others. We start by assigning login resources library(DSOpal) library(dsBaseClient) library(dsOmicsClient) builder &lt;- newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.brge_plink&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() Then we assign the resource to a symbol (i.e. R object) called client which is a ssh resource conns &lt;- datashield.login(logins = logindata, assign = TRUE, symbol = &quot;client&quot;) ds.class(&quot;client&quot;) $study1 [1] &quot;SshResourceClient&quot; &quot;CommandResourceClient&quot; &quot;ResourceClient&quot; &quot;R6&quot; Now, we are ready to run any PLINK command from the client site. Notice that in this case we want to assess association between the genotype data in bed format and use as phenotype the variable ‘asthma’ that is in the file ‘brge.phe’ in the 6th column. The sentence in a PLINK command would be (NOTE: we avoid –out to indicate the output file since the file will be available in R as a tibble). plink --bfile brge --logistic --pheno brge.phe --mpheno 6 --covar brge.phe --covar-name gender,age The arguments must be encapsulated in a single character without the command ‘plink’ plink.arguments &lt;- &quot;--bfile brge --logistic --pheno brge.phe --mpheno 6 --covar brge.phe --covar-name gender,age&quot; the analyses are then performed by ans.plink &lt;- ds.PLINK(&quot;client&quot;, plink.arguments) The object ans.plink contains the PLINK results at each server as well as the outuput provided by PLINK lapply(ans.plink, names) $study1 [1] &quot;results&quot; &quot;plink.out&quot; head(ans.plink$study1$results) # A tibble: 6 x 9 CHR SNP BP A1 TEST NMISS OR STAT P &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 MitoC3993T 3993 T ADD 2286 0.752 -1.33 0.182 2 0 MitoC3993T 3993 T gender 2286 0.742 -3.27 0.00107 3 0 MitoC3993T 3993 T age 2286 1.00 0.565 0.572 4 0 MitoG4821A 4821 A ADD 2282 2.68 1.71 0.0879 5 0 MitoG4821A 4821 A gender 2282 0.740 -3.31 0.000940 6 0 MitoG4821A 4821 A age 2282 1.00 0.465 0.642 ans.plink$study$plink.out $status [1] 0 $output [1] &quot;&quot; [2] &quot;@----------------------------------------------------------@&quot; [3] &quot;| PLINK! | v1.07 | 10/Aug/2009 |&quot; [4] &quot;|----------------------------------------------------------|&quot; [5] &quot;| (C) 2009 Shaun Purcell, GNU General Public License, v2 |&quot; [6] &quot;|----------------------------------------------------------|&quot; [7] &quot;| For documentation, citation &amp; bug-report instructions: |&quot; [8] &quot;| http://pngu.mgh.harvard.edu/purcell/plink/ |&quot; [9] &quot;@----------------------------------------------------------@&quot; [10] &quot;&quot; [11] &quot;Skipping web check... [ --noweb ] &quot; [12] &quot;Writing this text to log file [ /tmp/ssh-2566/out.log ]&quot; [13] &quot;Analysis started: Tue Jan 12 14:50:23 2021&quot; [14] &quot;&quot; [15] &quot;Options in effect:&quot; [16] &quot;\\t--bfile brge&quot; [17] &quot;\\t--logistic&quot; [18] &quot;\\t--covar brge.phe&quot; [19] &quot;\\t--covar-name gender,age&quot; [20] &quot;\\t--noweb&quot; [21] &quot;\\t--out /tmp/ssh-2566/out&quot; [22] &quot;&quot; [23] &quot;Reading map (extended format) from [ brge.bim ] &quot; [24] &quot;100000 markers to be included from [ brge.bim ]&quot; [25] &quot;Reading pedigree information from [ brge.fam ] &quot; [26] &quot;2312 individuals read from [ brge.fam ] &quot; [27] &quot;2312 individuals with nonmissing phenotypes&quot; [28] &quot;Assuming a disease phenotype (1=unaff, 2=aff, 0=miss)&quot; [29] &quot;Missing phenotype value is also -9&quot; [30] &quot;725 cases, 1587 controls and 0 missing&quot; [31] &quot;1097 males, 1215 females, and 0 of unspecified sex&quot; [32] &quot;Reading genotype bitfile from [ brge.bed ] &quot; [33] &quot;Detected that binary PED file is v1.00 SNP-major mode&quot; [34] &quot;Reading 6 covariates from [ brge.phe ] with nonmissing values for 2199 individuals&quot; [35] &quot;Selected subset of 2 from 6 covariates&quot; [36] &quot;For these, nonmissing covariate values for 2312 individuals&quot; [37] &quot;Before frequency and genotyping pruning, there are 100000 SNPs&quot; [38] &quot;2312 founders and 0 non-founders found&quot; [39] &quot;6009 heterozygous haploid genotypes; set to missing&quot; [40] &quot;Writing list of heterozygous haploid genotypes to [ /tmp/ssh-2566/out.hh ]&quot; [41] &quot;7 SNPs with no founder genotypes observed&quot; [42] &quot;Warning, MAF set to 0 for these SNPs (see --nonfounders)&quot; [43] &quot;Writing list of these SNPs to [ /tmp/ssh-2566/out.nof ]&quot; [44] &quot;Total genotyping rate in remaining individuals is 0.994408&quot; [45] &quot;0 SNPs failed missingness test ( GENO &gt; 1 )&quot; [46] &quot;0 SNPs failed frequency test ( MAF &lt; 0 )&quot; [47] &quot;After frequency and genotyping pruning, there are 100000 SNPs&quot; [48] &quot;After filtering, 725 cases, 1587 controls and 0 missing&quot; [49] &quot;After filtering, 1097 males, 1215 females, and 0 of unspecified sex&quot; [50] &quot;Converting data to Individual-major format&quot; [51] &quot;Writing logistic model association results to [ /tmp/ssh-2566/out.assoc.logistic ] &quot; [52] &quot;&quot; [53] &quot;Analysis finished: Tue Jan 12 14:52:20 2021&quot; [54] &quot;&quot; $error character(0) $command [1] &quot;cd /home/master/brge &amp;&amp; plink1 --bfile brge --logistic --covar brge.phe --covar-name gender,age --noweb --out /tmp/ssh-2566/out&quot; attr(,&quot;class&quot;) [1] &quot;resource.exec&quot; We can compare the p-values obtained using PLINK with Bioconductor-based packages for the top-10 SNPs as follows: library(tidyverse) # get SNP p.values (additive model - ADD) res.plink &lt;- ans.plink$study1$results %&gt;% filter(TEST==&quot;ADD&quot;) %&gt;% arrange(P) # compare top-10 with Biocoductor&#39;s results snps &lt;- res.plink$SNP[1:10] plink &lt;- res.plink %&gt;% filter(SNP%in%snps) %&gt;% dplyr::select(SNP, P) bioC &lt;- ans.bioC$study1 %&gt;% filter(rs%in%snps) %&gt;% dplyr::select(rs, Score.pval) left_join(plink, bioC, by=c(&quot;SNP&quot; = &quot;rs&quot;)) # A tibble: 10 x 3 SNP P Score.pval &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 rs2267914 0.00000151 0.000000809 2 rs6097326 0.00000424 0.00000642 3 rs7153 0.00000440 0.00000508 4 rs3732410 0.00000817 0.00000940 5 rs7995146 0.0000170 0.0000195 6 rs6495788 0.0000213 0.0000278 7 rs1602679 0.0000268 0.0000264 8 rs11055608 0.0000270 0.0000161 9 rs7098143 0.0000313 0.0000214 10 rs7676164 0.0000543 0.0000537 As expected, the p-values are in the same order of magnitud having little variations due to the implemented methods of each software. We can do the same comparions of minor allele frequency (MAF) estimation performed with Bioconductor and PLINK. To this end, we need first to estimate MAF using PLINK plink.arguments &lt;- &quot;--bfile brge --freq&quot; ans.plink2 &lt;- ds.PLINK(&quot;client&quot;, plink.arguments) maf.plink &lt;- ans.plink2$study1$results plink &lt;- maf.plink %&gt;% filter(SNP%in%snps) %&gt;% dplyr::select(SNP, MAF) bioC &lt;- ans.bioC$study1 %&gt;% filter(rs%in%snps) %&gt;% dplyr::select(rs, freq) left_join(plink, bioC, by=c(&quot;SNP&quot; = &quot;rs&quot;)) # A tibble: 10 x 3 SNP MAF freq &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 rs7153 0.256 0.256 2 rs3732410 0.254 0.254 3 rs7676164 0.304 0.304 4 rs1602679 0.101 0.101 5 rs7098143 0.210 0.210 6 rs11055608 0.446 0.446 7 rs7995146 0.0527 0.0527 8 rs6495788 0.267 0.267 9 rs2267914 0.104 0.104 10 rs6097326 0.125 0.125 This close the DataSHIELD session datashield.logout(conns) "],
["GIS.html", "17 Geospatial data analysis 17.1 Introducing the analysis 17.2 Setting up the analysis 17.3 Data manipulation 17.4 Generating the final result", " 17 Geospatial data analysis In this section we will provide some realistic data analyses of geographic data including Geospatial Positionning System (GPS) and geolocation data, which is then combined with phenotypic data. The objectives of the analyses are intended to mimic the real-life usage of data as described in Burgoine et al. 17.1 Introducing the analysis In this example we consider GPS traces captured by individuals in some eastern districts of London, publicly available from OpenStreetMap. For this example analysis, we will imagine these 819 GPS traces as commutes between individuals’ home and work. If these data were actual commutes, they would be highly sensitive as they identify an individual’s home and work location. Therefore it is less likely that commuting data would be readily accessible for traditional pooled analysis and a federated approach with DataSHIELD could be more practical. We also have real data on the location of 6100 fast food or takeaway outlets in this same area, sourced from the Food Standards Agency. This is shown in the figure below. We manufactured a corresponding data set on individuals’ BMI, age, sex, total household income, highest educational qualification (as proxies for individual level socioeconomic status) and smoking status. The purpose of the analysis is to test the association between exposure to takeaway food on a commute with BMI, using the other variables to adjust for confounding factors. Given that reducing obesity is a public health priority, this type of research could help inform local authority policies towards food outlet location, type and density. We illustrate how the tools available in the dsGeo package allow this question to be addressed. Figure 17.1: GPS traces in eastern London which could be imagined to be commutes between homes and workplaces Figure 17.2: Location of food outlets in eastern London 17.2 Setting up the analysis Note that in this example, the data are held in a single resource. There is scope for the data to be held in different resources. In order to perform geospatial data analyses, we need first to login and assign resources to DataSHIELD. This can be performed using the as.resource.object() function. The data have been uploaded to the server and configured as resources. builder &lt;- DSI::newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.gps_data&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() conns &lt;- DSI::datashield.login(logins = logindata, assign = TRUE, symbol = &quot;gps.res&quot;) # Assign additional datasets as resources for the location of the food outlets / takeaways and # the participant phenotype data datashield.assign.resource(conns, symbol=&quot;takeaway.res&quot;, resource=&quot;RSRC.takeaway_gps&quot;) datashield.assign.resource(conns, symbol=&quot;participant.res&quot;, resource=&quot;RSRC.gps_participant&quot;) # Assign to the original R classes (e.g Spatial) datashield.assign.expr(conns, symbol = &quot;takeaway&quot;, expr = quote(as.resource.object(takeaway.res))) datashield.assign.expr(conns, symbol = &quot;journeys&quot;, expr = quote(as.resource.object(gps.res))) datashield.assign.expr(conns, symbol = &quot;participant&quot;, expr = quote(as.resource.data.frame(participant.res))) Now, we can see that the resources are loaded into the R servers as their original class ds.class(&quot;takeaway&quot;) $study1 [1] &quot;SpatialPointsDataFrame&quot; attr(,&quot;package&quot;) [1] &quot;sp&quot; ds.class(&quot;journeys&quot;) $study1 [1] &quot;SpatialLinesDataFrame&quot; attr(,&quot;package&quot;) [1] &quot;sp&quot; ds.class(&quot;participant&quot;) $study1 [1] &quot;data.frame&quot; In the same way that BioConductor provides convenient data structures for ’omics data, the sp package allows each row of data to have a corresponding set of geometries. So if a row of data has information about an individual (age, BMI, etc.) it can also have a set of many points defining geometries such as points, lines and polygons. # Standard ds.summary can be used on the &#39;data&#39; part of the spatial dataframe ds.summary(&#39;journeys$age&#39;) $study1 $study1$class [1] &quot;numeric&quot; $study1$length [1] 819 $study1$`quantiles &amp; mean` 5% 10% 25% 50% 75% 90% 95% Mean 41.23623 42.76502 46.61040 52.53669 58.73395 62.25656 63.72577 52.58792 We have provided an additional function to allow a summary of the geometries to be provided. ds.geoSummary(&#39;journeys&#39;) $study1 $class [1] &quot;SpatialLinesDataFrame&quot; attr(,&quot;package&quot;) [1] &quot;sp&quot; $bbox min max x 538171.2 556078.4 y 179796.4 208063.6 $is.projected [1] TRUE $proj4string [1] &quot;+init=epsg:27700 +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +datum=OSGB36 +units=m +no_defs +ellps=airy +towgs84=446.448,-125.157,542.060,0.1502,0.2470,0.8421,-20.4894&quot; $data id age sex inc fsmoke fedu Min. : 1.0 Min. :40.02 Min. :0.0000 Min. : 2479 current:240 Secondary:156 1st Qu.:218.5 1st Qu.:46.61 1st Qu.:0.0000 1st Qu.:40545 former :169 Higher :393 Median :426.0 Median :52.54 Median :0.0000 Median :50949 never :410 Advanced :270 Mean :427.6 Mean :52.59 Mean :0.4872 Mean :50395 3rd Qu.:640.5 3rd Qu.:58.73 3rd Qu.:1.0000 3rd Qu.:59912 Max. :849.0 Max. :64.99 Max. :1.0000 Max. :99301 BMI Min. : 17.01 1st Qu.: 22.42 Median : 24.34 Mean : 25.13 3rd Qu.: 26.60 Max. :116.72 attr(,&quot;class&quot;) [1] &quot;summary.Spatial&quot; 17.3 Data manipulation In our analysis we need to define how we determine whether an individual has been ‘exposed’ to a food outlet. First we will define a 10m buffer around the location of the food outlet. If an individual’s GPS trace falls within that buffered region, we will say that they are ‘exposed’ to that outlet. The food outlets are defined as points, and after the buffer is applied they become polygons. # Add a buffer to each takeaway - now they are polygons ds.gBuffer(input = &#39;takeaway&#39;, ip_width=10, newobj.name = &#39;take.buffer&#39;, by_id=TRUE) ds.geoSummary(&#39;take.buffer&#39;) $study1 $class [1] &quot;SpatialPolygonsDataFrame&quot; attr(,&quot;package&quot;) [1] &quot;sp&quot; $bbox min max x 538336.1 556245.1 y 179810.1 207683.1 $is.projected [1] TRUE $proj4string [1] &quot;+init=epsg:27700 +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +datum=OSGB36 +units=m +no_defs +ellps=airy +towgs84=446.448,-125.157,542.060,0.1502,0.2470,0.8421,-20.4894&quot; $data BusinessName BusinessTypeID Tesco : 43 Min. : 1 Subway : 32 1st Qu.: 1 Costa Coffee : 24 Median :4613 Sainsbury&#39;s : 22 Mean :4210 Londis : 19 3rd Qu.:7843 Domino&#39;s Pizza: 17 Max. :7846 (Other) :5943 attr(,&quot;class&quot;) [1] &quot;summary.Spatial&quot; To simplify the next step, we remove the data part of the SpatialPolygonsDataFrame to leave the polygons #extract the polygons ds.geometry(input_x = &#39;take.buffer&#39;, newobj.name = &#39;take.buffer.strip&#39;) ds.geoSummary(&#39;take.buffer.strip&#39;) $study1 $class [1] &quot;SpatialPolygons&quot; attr(,&quot;package&quot;) [1] &quot;sp&quot; $bbox min max x 538336.1 556245.1 y 179810.1 207683.1 $is.projected [1] TRUE $proj4string [1] &quot;+init=epsg:27700 +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +datum=OSGB36 +units=m +no_defs +ellps=airy +towgs84=446.448,-125.157,542.060,0.1502,0.2470,0.8421,-20.4894&quot; attr(,&quot;class&quot;) [1] &quot;summary.Spatial&quot; Now, we take the intersection of each individual’s commute with each food outlet as shown in the figure below. Figure 17.3: Food outlets with a buffer are shown by a cross surrounded by a circle. The line segments represent the GPS trace. In this case there is an intersection with 2 of the 3 food outlets. This results in a list of individuals, with each list element being a numeric vector containing the ids of the food outlet buffers that were intersected by the GPS trace. These are the food outlets that the individual was ‘exposed’ to. We then convert the numeric vectors to counts by applying the length function to the list. # Do the intersection of journeys with buffered takeaways ds.over(input_x = &#39;journeys&#39;, input_y = &#39;take.buffer.strip&#39;, newobj.name = &#39;my.over&#39;, retList = TRUE) ds.lapply(input = &#39;my.over&#39;,newobj.name = &#39;count.list&#39;, fun = &#39;length&#39;) The result of the lapply needs to be unlisted, and we check the result looks reasonable ds.unList(x.name = &#39;count.list&#39;, newobj=&#39;counts&#39;) $is.object.created [1] &quot;A data object &lt;counts&gt; has been created in all specified data sources&quot; $validity.check [1] &quot;&lt;counts&gt; appears valid in all sources&quot; ds.summary(&#39;counts&#39;) $study1 $study1$class [1] &quot;integer&quot; $study1$length [1] 819 $study1$`quantiles &amp; mean` 5% 10% 25% 50% 75% 90% 95% Mean 0.000000 0.000000 0.000000 0.000000 1.000000 5.000000 9.000000 1.882784 17.4 Generating the final result Finally we complete the association analysis and see that BMI is positively associated with the count of food outlets that an individual is exposed to on commutes (see the ‘counts’ coefficient). This type of analysis could be used to convey a public health message about density and location of takeaway food outlets. ds.dataFrame(x = c(&#39;participant&#39;, &#39;counts&#39;), newobj = &#39;geo.df&#39;) $is.object.created [1] &quot;A data object &lt;geo.df&gt; has been created in all specified data sources&quot; $validity.check [1] &quot;&lt;geo.df&gt; appears valid in all sources&quot; result &lt;- ds.glm(formula = &#39;BMI ~ age + sex + inc + fsmoke + fedu + counts&#39;, data = &#39;geo.df&#39;, family = &#39;gaussian&#39;) result$coefficients Estimate Std. Error z-value p-value low0.95CI high0.95CI (Intercept) 2.379500e+01 6.334955e-01 37.5614341 0.000000e+00 2.255337e+01 2.503663e+01 age 2.698037e-02 1.023046e-02 2.6372581 8.357921e-03 6.929033e-03 4.703171e-02 sex -2.960308e+00 1.447462e-01 -20.4517233 5.799002e-93 -3.244006e+00 -2.676611e+00 inc -7.647467e-06 5.202464e-06 -1.4699701 1.415699e-01 -1.784411e-05 2.549176e-06 fsmokeformer 2.849097e-01 2.070037e-01 1.3763509 1.687130e-01 -1.208101e-01 6.906295e-01 fsmokenever 3.906555e-01 1.675560e-01 2.3314925 1.972740e-02 6.225183e-02 7.190592e-01 feduHigher 8.727664e-02 1.951124e-01 0.4473147 6.546479e-01 -2.951367e-01 4.696899e-01 feduAdvanced -1.417767e-01 2.076311e-01 -0.6828300 4.947143e-01 -5.487262e-01 2.651727e-01 counts 7.957629e-01 1.137431e-02 69.9614332 0.000000e+00 7.734697e-01 8.180562e-01 This closes the access to the resources datashield.logout(conns) "],
["exposome.html", "18 Exposome data analysis 18.1 Loading the Exposome Set 18.2 Exploring the loaded dataset 18.3 Exposures Imputation 18.4 Performing an ExWAS 18.5 Exposures PCA 18.6 Exposures Correlation", " 18 Exposome data analysis 18.1 Loading the Exposome Set An exposome dataset is made up from three different tables. In order to be studied, this three tables need to be coerced into a R object of class ExposomeSet, for that reason an exposome dataset can be available on a server as three tables or as a resource. 18.1.1 From tables to ExposomeSet To coerce the three tables that make the exposome dataset into an ExposomeSet R object, the tables have to be loaded inside the same study server. On this example, the tables are on the demo Opal server as resources, which means they have to be loaded on the study server and be coerced as data frames; for more information read the resourcer package documentation. library(dsBaseClient) library(dsExposomeClient) library(DSOpal) builder &lt;- newDSLoginBuilder() builder$append(server = &quot;server1&quot;, url = &quot;https://opal-demo.obiba.org/&quot;, user = &quot;administrator&quot;, password = &quot;password&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() conns &lt;- datashield.login(logins = logindata) datashield.assign.resource(conns, symbol = &#39;description&#39;, resource = list(server1 = &#39;EXPOSOME.description&#39;)) ds.class(&#39;description&#39;) $server1 [1] &quot;TidyFileResourceClient&quot; &quot;FileResourceClient&quot; &quot;ResourceClient&quot; &quot;R6&quot; datashield.assign.expr(conns, symbol = &quot;description&quot;, expr = quote(as.resource.data.frame(description))) ds.class(&#39;description&#39;) $server1 [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; datashield.assign.resource(conns, symbol = &#39;exposures&#39;, resource = list(server1 = &#39;EXPOSOME.exposures&#39;)) datashield.assign.expr(conns, symbol = &quot;exposures&quot;, expr = quote(as.resource.data.frame(exposures))) datashield.assign.resource(conns, symbol = &#39;phenotypes&#39;, resource = list(server1 = &#39;EXPOSOME.phenotypes&#39;)) datashield.assign.expr(conns, symbol = &quot;phenotypes&quot;, expr = quote(as.resource.data.frame(phenotypes))) ds.ls() $server1 $server1$environment.searched [1] &quot;R_GlobalEnv&quot; $server1$objects.found [1] &quot;description&quot; &quot;exposures&quot; &quot;phenotypes&quot; If the tables are on the Opal server as tables instead of resources, the lines after the datashield.login would look like this # Don&#39;t run this code, the tables are not on the demo Opal server, it&#39;s just for demostration purposes datashield.assign.table(conns, symbol = &#39;description&#39;, table = list(server1 = &#39;EXPOSOME.description_table&#39;)) # Repeat for the other two tables When there are tables on the Opal server they can be directly loaded to the study server as data frames, there is no need to perform the as.data.frame assignation. To coerce the three tables to the ExposomeSet object, the dsExposomeClient library has the ds.loadExposome() function, which takes as input parameters the names of the tables on the study server among some further configuration options, refer to the function documentation for more information. ds.loadExposome(&quot;exposures&quot;, &quot;description&quot;, &quot;phenotypes&quot;, &quot;idnum&quot;, &quot;idnum&quot;, &quot;Exposure&quot;, &quot;Family&quot;, 5, FALSE, &quot;exposome_object&quot;) ds.class(&quot;exposome_object&quot;) $server1 [1] &quot;ExposomeSet&quot; attr(,&quot;package&quot;) [1] &quot;rexposome&quot; 18.1.2 From resource to ExposomeSet When there’s an ExposomeSet resource available, it just needs to be loaded into the study server datashield.assign.resource(conns, symbol = &#39;exposome_resource&#39;, resource = list(server1 = &#39;EXPOSOME.exposomeSet&#39;)) ds.ls() $server1 $server1$environment.searched [1] &quot;R_GlobalEnv&quot; $server1$objects.found [1] &quot;description&quot; &quot;exposome_object&quot; &quot;exposome_resource&quot; &quot;exposures&quot; &quot;phenotypes&quot; ds.class(&quot;exposome_resource&quot;) $server1 [1] &quot;RDataFileResourceClient&quot; &quot;FileResourceClient&quot; &quot;ResourceClient&quot; &quot;R6&quot; Once on the study server, the resource has to be coerced into an R object datashield.assign.expr(conns, symbol = &quot;exposome_resource&quot;, expr = quote(as.resource.object(exposome_resource))) ds.class(&quot;exposome_resource&quot;) $server1 [1] &quot;ExposomeSet&quot; attr(,&quot;package&quot;) [1] &quot;rexposome&quot; 18.2 Exploring the loaded dataset Once the Exposome datasets are loaded on the study servers, some simple functions can be used to have some understanding of what the dataset contains. Those correspond to the name of the exposures, the names of the families of the exposures and the name of the phenotypes. 18.2.1 Family names To get the names of the families of exposures present on the exposome set, there’s the ds.familyNames function, it can take the parameter by.exposure = TRUE to list the family names and the exposures that contain each family. ds.familyNames(&quot;exposome_object&quot;) $server1 [1] &quot;Air Pollutants&quot; &quot;Metals&quot; &quot;PBDEs&quot; &quot;Organochlorines&quot; &quot;Bisphenol A&quot; [6] &quot;Water Pollutants&quot; &quot;Built Environment&quot; &quot;Cotinine&quot; &quot;Home Environment&quot; &quot;Phthalates&quot; [11] &quot;Noise&quot; &quot;PFOAs&quot; &quot;Temperature&quot; head(ds.familyNames(&quot;exposome_object&quot;, TRUE)$server1) AbsPM25 As BDE100 BDE138 BDE153 BDE154 &quot;Air Pollutants&quot; &quot;Metals&quot; &quot;PBDEs&quot; &quot;PBDEs&quot; &quot;PBDEs&quot; &quot;PBDEs&quot; 18.2.2 Exposures and phenotypes names To get the names of the exposures or phenotypes (or both combined) there’s the function ds.exposome_variables, it takes as argument exposures, phenotypes or all to retrieve the desired names. head(ds.exposome_variables(&quot;exposome_object&quot; , &quot;exposures&quot;, conns)$server1) [1] &quot;AbsPM25&quot; &quot;As&quot; &quot;BDE100&quot; &quot;BDE138&quot; &quot;BDE153&quot; &quot;BDE154&quot; ds.exposome_variables(&quot;exposome_object&quot; , &quot;phenotypes&quot;, conns) $server1 [1] &quot;whistling_chest&quot; &quot;flu&quot; &quot;rhinitis&quot; &quot;wheezing&quot; &quot;birthdate&quot; &quot;sex&quot; [7] &quot;age&quot; &quot;cbmi&quot; &quot;blood_pre&quot; 18.2.3 Summary of variables Non-disclosive descriptive statistics can be obtained from the exposome dataset, the function ds.exposome_summary is in charge of that. It can obtain descriptive statistics from numeric and factor variables of the exposome dataset (both from exposures and phenotypes). # Numerical variable ds.exposome_summary(&quot;exposome_object&quot;, &quot;PM25&quot;, conns) $server1 $server1$class [1] &quot;numeric&quot; $server1$length [1] 109 $server1$`quantiles &amp; mean` 5% 10% 25% 50% 75% 90% 95% Mean 1.093217 1.132021 1.146654 1.171904 1.181451 1.202192 1.216758 1.165488 # Factor variable ds.exposome_summary(&quot;exposome_object&quot;, &quot;flu&quot;, conns) $server1 $server1$class [1] &quot;character&quot; $server1$length [1] 109 18.2.4 Missing data The number of missing data on each exposure and on each phenotype can be found by using the function ds.tableMissings. This function returns a vector with the amount of missing data in each exposure or phenotype. The argument set indicates if the number of missing values is counted on exposures or phenotypes. The argument output indicates if it is shown as counts (output=\"n\") or as percentage (output=\"p\"). ds.tableMissings(&quot;exposome_object&quot;, set = &quot;exposures&quot;) $server1 Dens Temp Conn AbsPM25 NO NO2 NOx PM10 0 0 1 2 2 2 2 2 PM10Cu PM10Fe PM10K PM10Ni PM10S PM10SI PM10Zn PM25 2 2 2 2 2 2 2 2 PM25CU PM25FE PM25K PM25Ni PM25S PM25Sl PM25Zn PMcoarse 2 2 2 2 2 2 2 2 Benzene PM25V ETS G_pesticides Gas BTHM CHCl3 H_pesticides 3 3 5 5 5 6 6 6 Noise_d Noise_n THM Cotinine bHCH DDE DDT HCB 6 6 6 7 13 13 13 13 PCB118 PCB138 PCB153 PCB180 BPA As Cs Mo 13 13 13 13 21 24 24 24 Ni Tl Zn Hg Cd Sb Green Cu 24 24 24 27 28 30 31 40 PM10V Se MBzP MEHHP MEHP MEOHP MEP MiBP 41 45 46 46 46 46 46 46 MnBP X5cxMEPP Co PFHxS PFNA PFOA PFOS X7OHMMeOP 46 46 47 48 48 48 48 49 Pb X2cxMMHP BDE100 BDE138 BDE153 BDE154 BDE17 BDE183 59 64 76 76 76 76 76 76 BDE190 BDE209 BDE28 BDE47 BDE66 BDE71 BDE85 BDE99 76 76 76 76 76 76 76 76 ds.tableMissings(&quot;exposome_object&quot;, set = &quot;phenotypes&quot;) $server1 whistling_chest flu rhinitis wheezing sex age 0 0 0 0 0 0 cbmi blood_pre birthdate 0 2 3 Optionally, there’s also the ds.plotMissings function which returns a ggplot object with a barplot of missings for exposures or phenotypes, there’s the option of displaying the percentage of missings as well as total counts, check the function documentation for further information. ds.plotMissings(&quot;exposome_object&quot;, &quot;exposures&quot;, &quot;p&quot;) $server1 Please note that since this function call returns a ggplot object, it can be plotted properly to avoid squished Y axis when there are lots of exposures, read the official documentation for information on how to do that. 18.2.5 Exposures Normality Most of the test done in exposome analysis requires that the exposures must follow a normal distribution. The function ds.normalityTest performs a test on each exposure for normality behaviour. The result is a data.frame with the exposures’ names, a flag TRUE/FALSE for normality and the p-value obtained from the Shapiro-Wilk Normality Test (if the p-value is under the threshold, then the exposure is not normal). nm &lt;- ds.normalityTest(&quot;exposome_object&quot;) table(nm$server1$normality) FALSE TRUE 55 29 So, the exposures that do not follow a normal distribution are: nm$server1$exposure[!nm$server1$normality] [1] &quot;DDT&quot; &quot;PM10SI&quot; &quot;PM25K&quot; &quot;PM25Sl&quot; &quot;PCB118&quot; &quot;Tl&quot; &quot;PM10V&quot; &quot;PM25Zn&quot; &quot;PM25FE&quot; [10] &quot;PM10K&quot; &quot;BDE17&quot; &quot;PM25&quot; &quot;PMcoarse&quot; &quot;PM10&quot; &quot;BPA&quot; &quot;Green&quot; &quot;NO2&quot; &quot;Cs&quot; [19] &quot;PFNA&quot; &quot;PCB153&quot; &quot;PM25CU&quot; &quot;MEOHP&quot; &quot;Cu&quot; &quot;HCB&quot; &quot;MEHHP&quot; &quot;DDE&quot; &quot;BDE190&quot; [28] &quot;bHCH&quot; &quot;PM10Zn&quot; &quot;MnBP&quot; &quot;NO&quot; &quot;NOx&quot; &quot;PM10S&quot; &quot;MEHP&quot; &quot;PCB138&quot; &quot;Zn&quot; [37] &quot;X2cxMMHP&quot; &quot;PCB180&quot; &quot;PFOA&quot; &quot;Cotinine&quot; &quot;PM25S&quot; &quot;Co&quot; &quot;Conn&quot; &quot;PM25Ni&quot; &quot;PFHxS&quot; [46] &quot;PM10Ni&quot; &quot;Cd&quot; &quot;Dens&quot; &quot;Se&quot; &quot;X5cxMEPP&quot; &quot;BDE183&quot; &quot;BDE28&quot; &quot;Sb&quot; &quot;BDE138&quot; [55] &quot;PM25V&quot; The ds.normalityTest function has some extra input arguments to tune the normality test, check the function documentation for further information. The exposures can be visualized using non-disclosive histograms to see their distribution along their Shapiro-Wilk Normality Test p-value. ds.exposure_histogram(&quot;exposome_object&quot;, &quot;BDE209&quot;) If the selected exposure is not following a normal distribution, the function ds.exposure_histogram accepts the argument show.trans to visualize the raw data histogram plus three typical transformations (exp, log and sqrt), the Shapiro-Wilk Normality Test p-value is shown for all the transformations. ds.exposure_histogram(&quot;exposome_object&quot;, &quot;BDE209&quot;, TRUE) 18.2.6 Exposures Behaviour We can get a snapshot of the behaviour of the full exposome using the ds.plotFamily function. This function draws a mosaic of boxplots with all the numeric families of exposures, it can also plot single families and perform grouping inside of them. This function makes use of the ggplot2 library to generate the plots. Future plans include displaying non-disclosive plots for non-numeric families. ds.plotFamily(&quot;exposome_object&quot;, family = &quot;all&quot;) ds.plotFamily(&quot;exposome_object&quot;, family = &quot;Phthalates&quot;, group = &quot;sex&quot;) 18.3 Exposures Imputation The missings of an exposome dataset can be imputed using the ds.imputation function, which calls the rexposome function imputation on the study server. Refer to the documentation of rexposome for details about the imputation procedures. ds.imputation(&quot;exposome_object&quot;, &quot;exposome_object_imputed&quot;) 18.4 Performing an ExWAS When the ExposomeSet object is on the study the server, the function ds.exwas is in charge of performing the ExWAS. As an example let’s present a situation where each exposition has to be associated to the blood_pre phenotype using the phenotype sex as a covariate. This study would be done as follows exwas_results &lt;- ds.exwas(&quot;blood_pre ~ sex&quot;, &quot;exposome_object&quot;, &quot;gaussian&quot;) head(exwas_results$exwas_results) exposure family coefficient minE maxE p.value 1 AbsPM25 Air Pollutants 20.2567443 11.532500 28.980989 5.343657e-06 2 As Metals 0.5049329 -1.637091 2.646956 6.440695e-01 3 BDE100 PBDEs -1.4684745 -5.073223 2.136273 4.246181e-01 4 BDE138 PBDEs 1.2819339 -4.872401 7.436269 6.830857e-01 5 BDE153 PBDEs 2.8699183 -1.252769 6.992606 1.724462e-01 6 BDE154 PBDEs 0.1317781 -5.381815 5.645371 9.626373e-01 The model is inputted as a string, where the left side term is the objective phenotype, and the right term are the covariates, in the case of wanting more covariates proceed as: objective ~ cov1 + cov2 + ... + covN. It’s important noting that if the output family (third argument) does not match the objective family, the ExWAS will fail (Example: The objective family is binomial and the user sets it to gaussian). To visualize the results from the ExWAS, the function ds.plotExwas takes the output of ds.exwas and creates two different visualizations. A Manhattan-like plot with the p-values of the association between each exposure and asthma, coloured by families of exposures: ds.plotExwas(exwas_results, &quot;manhattan&quot;) Also, a plot for the effects of a given model can be obtained with: ds.plotExwas(exwas_results, &quot;effect&quot;) 18.5 Exposures PCA A PCA can be performed on the exposures of the exposome dataset. To do so, there’s the ds.exposome_pca. The exposures should be standardized in order to perform the PCA properly, to do so, there’s the arguments standar and method, which standardize the Exposome Set before performing the PCA following the specified method. The available methods are normal (default method), which scales the exposures using the mean as the center and the standard variation as dispersion; the robust method, which uses the median and median absolute deviation respectively; and, interquartile range, which uses the median as the center and the coeficient between the interquartile range of the exposure and the normal range between the percentile 75 and 25 as variance. It is important noting that this function is sensitive to be disclosive, specially for very rectangular data frames (similar number of variables as individuals). To illustrate this problem, let’s try to perform a PCA on the whole exposures test data. ds.exposome_pca(&quot;exposome_object&quot;, standar = TRUE) Error: There are some DataSHIELD errors, list them with datashield.errors() If that is the case, one option is to reduce the families of exposures of the Exposome Set. The ds.exposome_pca function has the argument fam to select the families to subset the Exposome Set to perform the PCA. ds.exposome_pca(&quot;exposome_object&quot;, fam = c(&quot;Metals&quot;, &quot;Noise&quot;), standar = TRUE) The PCA function saves the results on the study server to prevent any dislosures, the default variable they take is called \"ds.exposome_pca.Results\", which has to be passed to the visualization function. To visualize the results of the PCA there is the function ds.exposome_pca_plot, this function relies on the visualization methods already implemented on rexposome for the PCA analysis, it does it however on a non-disclosive way, by passing the scatter plot points through an anonimization process, hence the arguments k, method and noise. The visualization is controlled with the set argument, which takes \"all\" (mosaic of plots of the PCA), \"exposures\" (plot of the exposures space on the first two principal components, color coded by family), \"samples\" (plot of the individuals space on the first two principal components, this plot can take the phenotype argument to color code the individuals by phenotypes), \"variance\" and \"variance_explained\", the two variance plots are quite self explanatory, the color code on the \"variance\" highlights the first two principal components as they are the ones shown on the other drawings. ds.exposome_pca_plot(&quot;ds.exposome_pca.Results&quot;, set = &quot;all&quot;, method = 1, k = 3, noise = 5) ds.exposome_pca_plot(&quot;ds.exposome_pca.Results&quot;, set = &quot;samples&quot;, phenotype = &quot;sex&quot;, method = 1, k = 3, noise = 5) ds.exposome_pca_plot(&quot;ds.exposome_pca.Results&quot;, set = &quot;exposures&quot;, method = 1, k = 3, noise = 5) ds.exposome_pca_plot(&quot;ds.exposome_pca.Results&quot;, set = &quot;variance&quot;, method = 1, k = 3, noise = 5) ds.exposome_pca_plot(&quot;ds.exposome_pca.Results&quot;, set = &quot;variance_explained&quot;, method = 1, k = 3, noise = 5) Furthermore, the ds.exposome_pca_plot function can plot the correlations betweeen the principal components and the exposures and the association of the phenotypes with the principal components. This two visualizations can be obtained by setting the set argument to \"exposures_correlation\" and \"phenotypes_correlation\" respectively. ds.exposome_pca_plot(&quot;ds.exposome_pca.Results&quot;, set = &quot;exposures_correlation&quot;) ds.exposome_pca_plot(&quot;ds.exposome_pca.Results&quot;, set = &quot;phenotypes_correlation&quot;) 18.6 Exposures Correlation The correlation between exposures, in terms of intra-family and inter-family exposures, is interesting to take into account. The correlation of the exposome can be computed using ds.correlation. The correlation could be disclosive, for that reason there’s the argument fam to select the families to compute the correlation subsetting the exposome dataset by families, typical complete exposome datasets will fail as the correlation matrix could be disclosive. ds.correlation(&quot;exposome_object&quot;)[[1]][[1]]$`Correlation Matrix`[1:5,1:5] [,1] [,2] [,3] [,4] [,5] [1,] NA NA NA NA NA [2,] NA NA NA NA NA [3,] NA NA NA NA NA [4,] NA NA NA NA NA [5,] NA NA NA NA NA ds.correlation(&quot;exposome_object&quot;)[[1]][[1]]$`Error message` [1] &quot;ERROR: The ratio of the number of variables over the number of individual-level records exceeds the allowed threshold, there is a possible risk of disclosure&quot; When subsetting the exposome set by families the correlation succeeds. ds.correlation(&quot;exposome_object&quot;, c(&quot;Metals&quot;, &quot;Noise&quot;))[[1]][[1]]$`Correlation Matrix`[1:5,1:5] [,1] [,2] [,3] [,4] [,5] [1,] 1.00000000 0.08897651 -0.12603679 0.1112793 -0.03127031 [2,] 0.08897651 1.00000000 0.10209881 0.5356812 0.84660044 [3,] -0.12603679 0.10209881 1.00000000 -0.5111215 -0.05851681 [4,] 0.11127929 0.53568121 -0.51112146 1.0000000 0.47229280 [5,] -0.03127031 0.84660044 -0.05851681 0.4722928 1.00000000 The output from the ds.correlation consists of the correlation matrix and a vector that contains the names of exposures and families on the correlation table, this second item is used by the corPlot function to display the results of the correlation study. corr_data &lt;- ds.correlation(&quot;exposome_object&quot;, c(&quot;Metals&quot;, &quot;Noise&quot;)) corPlot(corr_data) The best option to see the inter-family correlations is the circos of correlations while the matrix of correlations is a better way for studying the intra-family correlations. corPlot(corr_data, type = &quot;circos&quot;) datashield.logout(conns) "],
["dslite-datashield-implementation-on-local-datasets.html", "19 DSLite: DataSHIELD Implementation on Local Datasets 19.1 Development Environment Setup 19.2 DataSHIELD Development Flow 19.3 DataSHIELD Sessions 19.4 Debugging 19.5 Limitations", " 19 DSLite: DataSHIELD Implementation on Local Datasets DSLite is a serverless DataSHIELD Interface (DSI) implementation whose purpose is to mimic the behavior of a distant (virtualized or barebone) data repository server (see DSOpal for instance). The datasets that are being analyzed must be fully accessible in the local environment and then the non-disclosive constraint of the analysis is not relevant for DSLite: some DSLite functionalities allow inspection of what is under the hood of the DataSHIELD computation nodes, making it a perfect tool for DataSHIELD analysis package developers. 19.1 Development Environment Setup 19.1.1 DataSHIELD Packages Both client and server side packages must be installed in your local R session. The entry point is still the client side package and DSLite will automatically load the corresponding server side package on DataSHIELD aggregate and assignment functions call, based on the DataSHIELD configuration. The minimum required packages are: install.packages(c(&quot;resourcer&quot;, &quot;DSLite&quot;), dependencies = TRUE) install.packages(&quot;dsBase&quot;, repos = c(&quot;https://cloud.r-project.org&quot;, &quot;https://cran.obiba.org&quot;), dependencies = TRUE) 19.1.2 Test Datasets DSLite comes with a set of datasets that can be easily loaded. You can also provide your own to illustrate a specific data analysis function. 19.1.3 R Package Development Tools We recommend using the following tools to facilitate R package development: devtools, the collection of package development tools, usethis, automate package and project setup tasks that are otherwise performed manually, testthat, for unit testing, roxygen2, for writing documentation in-line with code, Rstudio, the R editor that integrates the tools mentioned above and more. 19.2 DataSHIELD Development Flow The typical development flow, using DSLite, is: Build and install your client and/or server side DataSHIELD packages. Create a new DSLiteServer object instance, refering test datasets. Use or alter the default DataSHIELD configuration. Test your DataSHIELD client/server functions. Debug DataSHIELD server nodes using DSLiteServer methods. 19.2.1 DSLiteServer After your client and/or server side DataSHIELD packages have been built and installed, a new DSLiteServer object instance must be created. Some DSLiteServer methods can be used to verify or modify the DSLiteServer behaviour: DSLiteServer$strict() DSLiteServer$home() See the R documentation of the DSLiteServer class for details. As an example: library(DSLite) # prepare test data in a light DS server data(&quot;CNSIM1&quot;) data(&quot;CNSIM2&quot;) data(&quot;CNSIM3&quot;) dslite.server &lt;- newDSLiteServer(tables=list(CNSIM1=CNSIM1, CNSIM2=CNSIM2, CNSIM3=CNSIM3)) # load corresponding DataSHIELD login data data(&quot;logindata.dslite.cnsim&quot;) The previous example can be simplified using the set-up functions based on the provided test datasets: library(DSLite) # load CNSIM test data logindata.dslite.cnsim &lt;- setupCNSIMTest() 19.2.2 DataSHIELD Configuration The DataSHIELD configuration (aggregate and assign functions, R options) is automatically discovered by inspecting the R packages installed and having some DataSHIELD settings defined, either in their DESCRIPTION file or in a DATASHIELD file. This default configuration extracting function is: DSLite::defaultDSConfiguration() The list of the DataSHIELD R packages to be inspected (or excluded) when building the default configuration can be specified as parameters of defaultDSConfiguration(). The DataSHIELD configuration can be specified at DSLiteServer creation time or afterwards with some DSLiteServer methods that can be used to verify or modify the DSLiteServer configuration: DSLiteServer$config() DSLiteServer$aggregateMethods() DSLiteServer$aggregateMethod() DSLiteServer$assignMethods() DSLiteServer$assignMethod() DSLiteServer$options() DSLiteServer$option() See the R documentation of the DSLiteServer class for details. As an example: # verify configuration dslite.server$config() 19.3 DataSHIELD Sessions The following figure illustrates a setup where a single DSLiteServer holds several data frames and is used by two different DataSHIELD Connection (DSConnection) objects. All these objects live in the same R environment (usually the Global Environment). The “server” is responsible for managing DataSHIELD sessions that are implemented as distinct R environments inside of which R symbols are assigned and R functions are evaluated. Using the R environment paradigm ensures that the different DataSHIELD execution context (client and servers) are contained and exclusive from each other. DSLite architecture After performing the login DataSHIELD phase, the DSLiteServer holds the different DataSHIELD server side sessions, i.e. R environments identified by an ID. These IDs are also stored within the DataSHIELD connection objects that are the result of the datashield.login() call. The following example shows how to access these session IDs: # datashield logins and assignments conns &lt;- datashield.login(logindata.dslite.cnsim, assign=TRUE) # get the session ID of &quot;sim1&quot; node connection object conns$sim1@sid # the same ID is in the DSLiteServer dslite.server$hasSession(conns$sim1@sid) 19.4 Debugging Thanks to the DSLiteServer capability of allowing its configuration to be modified at any time, it is possible to add some debugging functions without polluting the DataSHIELD package you are developing. For instance, this code adds an aggregate function print(): # add a print method to configuration dslite.server$aggregateMethod(&quot;print&quot;, function(x){ print(x) }) # and use it to print the D symbol datashield.aggregate(conns, quote(print(D))) Another option is to get a symbol value from the server into the client environment. This can be very helpful for complex data structures. The following example illustrates usage of a shortcut function that iterates over all the connection objects and get the corresponding symbol value: # get data represented by symbol D for each DataSHIELD connection data &lt;- getDSLiteData(conns, &quot;D&quot;) # get data represented by symbol D from a specific DataSHIELD connection data1 &lt;- getDSLiteData(conns$sim1, &quot;D&quot;) 19.5 Limitations 19.5.1 Function Parameters Parser The main difference with a regular DSI implementation (such as the one of DSOpal) is that the arguments of the DataSHIELD functional calls are not parsed in DSLite. The only R language element that is inspected and handled is the name of the functions, that are replaced by the ones defined in the DataSHIELD configuration. For instance the following expression, which includes a function call in the formula, is valid for the DSLiteServer but not for Opal: someregression(D$height ~ D$diameter + poly(D$length,3,raw=TRUE)) As a consequence, DataSHIELD R package development can take advantage of DSLite flexibility for rapid development but will never replace testing on regular DataSHIELD infrastructure using DSOpal. 19.5.2 Server Side Environments For each of the DataSHIELD nodes, the server side code is evaluated within an environment that has no parent, i.e. detached from the global environment where the client code is executed. Some R functions have a parameter that allows to specify to which environment they apply, for instance assign(), get(), eval(), as.formula(), etc. Their env (or envir) parameter default value is parent.frame() which is the global environment when executed in Opal’s R server, because it is the parent frame of the package’s namespace where the function is defined. In DSLiteServer, the parent frame must be the environment where the server code is evaluated. In order to be consistent between these two execution contexts (Opal R server and DSLiteServer), you must specify the env (or envir) value explicitly to be parent.frame(), which is the parent frame of the block being executed (either the global environment in Opal context, or the environment defined in DSLiteServer). Example of a valid server side piece of code that assigns a value to a symbol in the DataSHIELD server’s environment (being the Opal R server’s global environment or a DSLiteServer’s environment): base::assign(x = &quot;D&quot;, value = someValue, envir = parent.frame()) See also the Advanced R - Environments documentation to learn more about environments. "],
["package-info.html", "20 Creating DataSHIELD packages 20.1 Example of DataSHIELD package development", " 20 Creating DataSHIELD packages The procedure for the development of a client-side or a server-side package in DataSHIELD is similar as the procedure of creating packages in native R and RStudio. For the development of a package, the developer will need the R packages devtools and roxygen2. Having those packages installed and loaded in the R session, the developer should follow five basic steps: Create the package directory: this will create the R and the man folders and the DESCRIPTION file in a specified directory. Add functions: copy the R scripts of the functions developed in the R folder created at step (1). Add documentation: add comments at the beginning of each function to give information to the user on what the function does and how the arguments of the function are used. Those comments are compiled into the correct format for the package documentation. More details can be found in the roxygen2 documentation. Create the documentation: this automatically adds in the .Rd files to the man directory and a NAMESPACE file to the main package directory. (see the function document in devtools) Install the development package. In addition to those steps, the developer can make the package a GitHub repo and allows other developers to commit any further developments and improvements. For more details the reader can have a look on a Git/GitHub guide. After the development of a client-server pair of DataSHIELD packages the DataSHIELD Development Team can provide a testing framework where the developed functions and packages are tested including tests related to disclosure protection and if all tests are passed, then the package can be become publicly available for use by the DataSHIELD users. Details for the DataSHIELD testing framework can be found in the DataSHIELD Wiki. 20.1 Example of DataSHIELD package development The following section contains an example on how to setup a whole environment to develop DataSHIELD packages using DSLite and RStudio, as well as a guided illustration of how to create a new functionality. It is targeted at new developers of DataSHIELD packages that need some guidance on the first steps. Please note that anything described on this section is just a template and can be modified and adapted to anyone liking, take it as a starting point. 20.1.1 Folders and files To speed up the development process, the most practical approach to setting up the folders and files is illustrated on the Figure 20.1. The idea is to have two folders that contain each one a package, that means one to be used in the client and one on the server (virtualized server on this development approach); outside those two folders there is the testing script that will be used to call the new functions. Figure 20.1: Folders and files proposal Inside each folder, a typical R package structure is to be found. This is further detailed here and here. If the reader does not have prior experience on R package development, there is the option of setting up a package folder through: RStudio -&gt; File -&gt; New Project... -&gt; New Directory -&gt; R Package The typical style convention for DataSHIELD is to name the client package as dsXClient and the associated server package as dsX. The client package will contain the functions that the user will use, those functions will be basically triggers to the server functions, more information about that on the following lines. 20.1.2 Development of a function There is many information about DataSHIELD and how it works. A very brief introduction will be provided on this section to help. When developing a function, we have to develop both the server function and the client function. The aim of the client function (your RStudio instance) is to send an instruction to the server that will or will not have a return to you; and the aim of the server function is to perform an action(s) on an object that it contains, and make sure that if there is an output to the client, it is non disclosive. We have already described a classification between functions that do and do not have outputs to the client, in DataSHIELD they are called AGGREGATE functions (output to the client) and ASSIGN functions (NO output to the client). It is important to always have in mind that the objects that are being manipulated only exist on the servers, and not on the client, with that in mind let’s take a look at a basic client function. ds.basicFunction &lt;- function(object, option) { } This is a basic function that will modify an object on the server that has one option parameter. Before writing any more code, let’s take a minute to understand that the object variable and option variable will be strings, since they do not exist on the client side. The object variable is the name of the object that we want to modify on the server, and the option variable is a string that in this example will refer to a bool (TRUE/FALSE). Let’s now build the function call that will be done on the client. ds.basicFunction &lt;- function(object, option) { cally &lt;- paste0(&quot;basicFunctionDS(&quot;, object, &quot;, &quot;, option, &quot;)&quot;) return(cally) } ds.basicFunction(&quot;dataset1&quot;, &quot;FALSE&quot;) [1] &quot;basicFunctionDS(dataset1, FALSE)&quot; As it can be seen, we have now created a string that contains a function call that we want to execute on the server, the only thing left to do is to send this instruction to the server. Before doing that let’s make clear the style used here. For the client functions we use ds.X and the associated server function is named XDS. Now we can use as example the following function that we have created on our server package. basicFunctionDS &lt;- function(object, option){ if(option){ object &lt;- sum(object) } else{ object &lt;- mean(object) } return(object) } What needs to be noted about the example server function is 1) Here we are using the input arguments of the functions as R objects, since this will be executed on a server that is expected to have them. On this particular example object must be a numerical vector and option a bool (this is a very simple example with no class checking of the arguments); 2) We have a return, so this can be an AGGREGATE or ASSIGN function, more information on that in the following lines, no need to worry at this stage. Having explained the server function, let’s now retake the client function. We now have our server function defined and what is left to be done is to actually call it. To do so we make use of the DSI package, at this point is when we have to decide if the function call will be AGGREGATE or ASSIGN. # AGGREGATE DSI::datashield.aggregate(conns, as.symbol(cally)) # ASSIGN DSI::datashield.assign.expr(conns, name, as.symbol(cally)) The return of the server function will be passed to the client on a AGGREGATE function and it will be assigned to a new object (on the server) on a ASSIGN function. If we want to create a function that will have an output to the client, we use DSI::datashield.aggregate, this function needs a connection object and the previously formatted string; while if we just want to create a new object on the server we use DSI::datashield.assign.expr, this function also needs the connection object as well as a string (name on the example) that indicates the name the new object will have on the server. Let’s say for this example we are creating a AGGREGATE function, our final function will look like this: ds.basicFunction &lt;- function(object, option, datasources = NULL) { if (is.null(datasources)) { datasources &lt;- DSI::datashield.connections_find() } cally &lt;- paste0(&quot;basicFunctionDS(&quot;, object, &quot;, &quot;, option, &quot;)&quot;) result &lt;- DSI::datashield.aggregate(datasources, as.symbol(cally)) return(result) } On this final example, there is already the handling of the connection object, which is done using the DSI package as well. Now that we have both our client and server functions ready, we are almost ready to test them using DSLite, before doing that there are still a couple of steps to be done. Document using roxygen both functions, to do so go to: Code -&gt; Insert Roxygen Skeleton And complete the header that has been added to the funcion (this step has to be done for both client and server functions). For both the client and server packages, the NAMESPACE has to be updated to declare this new functions, to do so, find both folders on the RStudio Files panel and do the following: Go inside dsX folder on the files panel in RStudio. More -&gt; Set As Working Directory On the RStudio Console run the following command: devtools::document() Repeat 1, 2 and 3 on the dsXClient folder There is just one step left to be done, on DataSHIELD server packages there has to be a file that states all the funcions of the package and whether they are AGGREGATE or ASSIGN functions. This file has to be named DATASHIELD (no extension) and has to be placed on dsX/inst/. The structure of this file is the following: AggregateMethods: basicFunctionDS, basicFunction2DS AssignMethods: assignFunctionDS, assignFunction2DS On our example it would just be: AggregateMethods: basicFunctionDS AssignMethods: 20.1.3 Testing of the function We are now ready to test our new function using DSLite. This will be done from the testing script that we described when talking about the folders and files. On this script, we have to achieve the following: Install the packages we are developing Create (or load) test data Create a virtualized server using DSLite Put the base DataSHIELD package and our new package inside the server Put the test data inside the server Login into the virtualized server Execute the new function and see how it behaves To simplify the explanation, an example script will now be shown with some comments on it to guide the reader. # Make sure the working directory is the test script setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) # Install the current version of both the client and server packages found locally devtools::install(&quot;./dsX&quot;) devtools::install(&quot;./dsXClient&quot;) # Load all the required packages library(dsBase) library(dsBaseClient) library(resourcer) library(DSLite) library(dsX) library(dsXClient) # Create test numerical vector test_data &lt;- rpois(n = 50, lambda = 10) # Create virtualized server with DSLite, assign everything needed on it dslite.server &lt;- newDSLiteServer(tables=list(exposures = test_data), config = DSLite::defaultDSConfiguration(include=c(&quot;dsBase&quot;, &quot;dsX&quot;))) builder &lt;- DSI::newDSLoginBuilder() builder$append(server = &quot;server1&quot;, url = &quot;dslite.server&quot;, table = &quot;exposures&quot;, driver = &quot;DSLiteDriver&quot;) logindata.dslite &lt;- builder$build() # Login to the virtualized server conns &lt;- datashield.login(logindata.dslite, assign=T, symbol = &quot;exposures&quot;) # Test the function ds.basicFunction(&quot;exposures&quot;, FALSE) With this complete example understood, one must feel now ready to start developing new features for DataSHIELD. Once a package has been developed using DSLite, it should be tested on a real instance of Opal before releasing it. "],
["tips-and-tricks.html", "21 Tips and tricks 21.1 How to create a new project into OPAL 21.2 How to upload a new resource into OPAL 21.3 How to install DataSHIELD packages into OPAL server 21.4 How to install R packages into OPAL server", " 21 Tips and tricks The user can see in this link how to create and install an Opal server. Next we illustrate how to deal with some of the basics for setting up a server to be used within DataSHIELD environment. In order to do that, we are using our Opal demo server available here: https://opal-demo.obiba.org/ This is the how an Opal looks like once the user enters the credentials: username: administrator password: password Opal demo main page 21.1 How to create a new project into OPAL 21.1.1 Manually The tab Projects (top-left) goes to the projects available in the Opal demo Opal demo projects A new project can be created by clicking on +Add Project tab. Then this information must be filled in Adding a new project to Opal 21.1.2 Using R A project can be created using the following R code: library(opalr) o &lt;- opal.login(username=&quot;administrator&quot;, password=&quot;password&quot;, url=&quot;https://opal-demo.obiba.org&quot;) # A general purpose project, with a data storage opal.project_create(o, &quot;projectName&quot;, database=&quot;dbName&quot;) # A project that can hold only resources and views (no persisted data) opal.project_create(o, &quot;projectName&quot;) opal.logout(o) See also the other opal.project_* functions. 21.2 How to upload a new resource into OPAL 21.2.1 Manually Once a new project has been created, a new resource can be uploaded by clicking on project’s name. In this case, let us assume that we are working on RSRC project that has been created to illustrate the main examples in this bookdown. After clicking on that project this window will appear Tables, variables and resources from an Opal project Here we can observe that this project contains 16 resources a no tables or variables. We can add a new resource by clicking on the “link tab” (see red circle) Going to the resources of a given project Then a new resource can be added by clicking on the +Add Resource tab Adding a resource of a given project Then this window will appear and information for your resource must be filled in Adding a resource of a given project The different types of resources have been described Chapter 7 21.2.2 Using R A resource can be added to a project by a simple function call, assuming that you know how to express the URL to the resource: library(opalr) o &lt;- opal.login(&quot;administrator&quot;,&quot;password&quot;, url=&quot;https://opal-demo.obiba.org&quot;) opal.resource_create(o, &quot;RSRC&quot;, &quot;CNSIM3&quot;, url = &quot;opal+https://opal-demo.obiba.org/ws/files/projects/RSRC/CNSIM3.zip&quot;, format = &quot;csv&quot;, secret = &quot;EeTtQGIob6haio5bx6FUfVvIGkeZJfGq&quot;) # to test the resource assignment opal.assign.resource(o, &quot;client&quot;, &quot;RSRC.CNSIM3&quot;) opal.execute(o, &quot;class(client)&quot;) opal.logout(o) See also the other opal.resource_*functions. 21.3 How to install DataSHIELD packages into OPAL server 21.3.1 Manually Administration tab at Opal The tab Administration (red circle in the previous figure) allows the user access to the administration page Administration tab at Opal The DataSHIELD tab goes to the DataSHIELD administration details Managing DataSHIELD packages in Opal The tab +Add Package allow the user to install a DataSHIELD package either from the DataSHIELD repository or any other GitHub site Install DataSHIELD packages in Opal 21.3.2 Using R The user can install DataSHIELD R packages from CRAN or GitHub using the following R code: library(opalr) o &lt;- opal.login(username=&quot;administrator&quot;, password=&quot;password&quot;, url=&quot;https://opal-demo.obiba.org&quot;) # CRAN dsadmin.install_package(o, &quot;dsBase&quot;) # GitHub dsadmin.install_github_package(o, &quot;packageName&quot;, username=&quot;orgOrUserName&quot;) opal.logout(o) When developing a new DataSHIELD package, it can be convenient to directly upload the built package archive for being installed on the R server side. This can be done as follow: library(opalr) o &lt;- opal.login(username=&quot;administrator&quot;, password=&quot;password&quot;, url=&quot;https://opal-demo.obiba.org&quot;) # build and install the package archive packageArchivePath &lt;- devtools::build(pkg=&quot;/path/to/the/packageName&quot;) dsadmin.install_local_package(o, path=packageArchivePath) opal.logout(o) 21.4 How to install R packages into OPAL server 21.4.1 Manually All the dependencies in a DataSHIELD package are automatically installed when installing it on the Opal Server. If necessary the user can also use the +Install button from the Administration/R tab Install DataSHIELD packages in Opal 21.4.2 Using R The user can install R packages from CRAN, Bioconductor or GitHub using the following R code: library(opalr) o &lt;- opal.login(username=&quot;administrator&quot;, password=&quot;password&quot;, url=&quot;https://opal-demo.obiba.org&quot;) # CRAN oadmin.install_package(o, &quot;cranPackageName&quot;) # Bioconductor oadmin.install_bioconductor_package(o, &quot;biocPackageName&quot;) # GitHub oadmin.install_github_package(o, &quot;packageName&quot;, username=&quot;orgOrUserName&quot;, ref = &quot;branchOrTagName&quot;) opal.logout(o) "],
["advancedtechnical-questions.html", "22 Advanced/technical questions", " 22 Advanced/technical questions We received a number of question when submitting our paper to Plos Comp Biology by the reviewers. Most of them were addressed into the manuscript. However, there were some raised by one of the reviewers that focused on technical aspects or technical details of the resources. These are reproduced here just in case some of the readers are also interested in such advanced aspects. Comment 1: Resources credentials are fixed and managed by Opal which implies you do not have a refresh token or something like that, that will expire over time. The policy decision and enforcement is now located in the DataSHIELD engine. This means that the resource owner can not decide anymore if someone has access to the resource. Audit logging at the resource side is hard to do this way. Answer: it is true that a more elaborated authentication/authorization policy cannot be handled on the DataSHIELD server side. Any token must be valid for a programmatic usage, and if it happens to expire or be invalidated, it needs to be renewed and updated in the Opal’s definition of the resource. Regarding the auditing, all DataSHIELD user commands are logged by the Opal server. The data owner has both control on the resource data access credentials and the permissions to use this resource. Comment 2: If the computational resource needs to do a lot, you need to program that either in the ResourceExtension or as given commands. When you program an extension you are dependent on the person’s choices regarding the interface he/she exposes and when you give it as parameters in the resource you need to parse it in the resourcer package. For example, how do you prevent malicious SQL injection? We would expect that you would not open up such resources to the external source but only a pre-packaged analysis. Answer: The DataSHIELD R analysis procedure that makes use of a resource is responsible for interacting in a secure way with the underlying resource system (database or ssh server for instance), like any application that makes use of a database. The best practice is to expose an API that is “business” oriented by defining a limited number of parameters that are easy to validate for each specific operations. This is also facilitated by the DataSHIELD infrastructure built-in feature which consists of allowing a limited syntax for expressing the server side function call parameters. Comment 3: You are still bound to the limitations of R in terms of memory and CPU usage when you want to correlate data in R against the data that is available in the resource. You need to either push the data you want to correlate against into the resource or extract it from the resource in the R-environment. Answer: The resource API offers the possibility to work with the dplyr package which delegates as much as possible the work to the underlying database. More generally, R is only required as an entry point and real data analysis can happen in any system that is accessible by R (for instance ML analysis can be launched from R in a Apache Spark cluster). The DataSHIELD analysis procedures must be programmed in that way, to support large to big datasets analysis. Another improvement that is currently being developed (EUCAN-Connect) is the capability of having multiple R servers for a single DataSHIELD data node, in order to address the case of multiple users accessing concurrently the memory and CPU of a server. Comment 4: What is going to happen when you want to finish the analysis on another moment. Or the analysis is taking days how do you retrieve the result. Answer: A plain R server is probably not the most suitable system for handling long running blocking tasks. Usually this kind of situation is addressed by submitting a task execution request to a worker, which will run the task in the background and will make available the progress of the task and its result for latter retrieval. This type of architecture is available in many languages and systems and could be implemented in a DataSHIELD analysis procedure. Comment 5: When the analysis is taking a lot of resources in terms of memory and CPU how do you limit this per DataSHIELD-user? Who is responsible for this, DataSHIELD or the resource owner? Answer: It is possible in R to limit the memory and CPU usage (and much more) using the package RAppArmor. One possible improvement would be the ability to define an AppArmor profile per user or group of users (this profile would be applied at the start of a DataSHIELD session in each data node). The data owner would have the control of the definition of the profiles and which one apply to each user. Comment 6: The resource owner needs to implement a way to be easily accessible for the resourcer-package. Especially when you want to run more complex jobs it usually takes a complex interface to work with. Answer: Building access to a resource is one complexity, that is in practice limited. Allowing multiple/complex parameters for the analysis of a resource is another one, that takes place in the definition of the DataSHIELD R server analysis functions, not in the definition of the resource. Comment 7: In the shell and ssh resource the Opal administrator is managing the actions that may be performed by the resource handler (researcher in general). When you host Opal on different infrastructure than the resource and is managed by someone who is not in charge of managing the resource the list of possible commands can be freely added to the resource. This possible allows Answer: The example of the PLINK resource (i.e. running PLINK through SSH) is put in practive by a using a SSH server in a docker container: the possible actions are limited to the available commands and data in this container for the user that connects to the SSH server (which itself has limited rights). Extra security could be added with setting up an AppArmor constrained environment. This illustrates that the security enforcement is possible but needs to be thought ahead of deploying access to such a resource. Comment 8: Maybe less dependencies in the resourcer package? Answer: We have tried to find a balance between showing out-of-the-box capabilities of various types of resources and not overloading the package. Note that most of the dependencies are suggestions and are not required at installation time. Since the review of the paper, several additional resource extensions have been developed to access less common or more specific resources (Dremio, HL7 FHIR, S3). Comment 9: When the interface of a dependency is changed the package needs to be changed as well. Answer: The resourcer package dependencies are well established packages (DBI, tidyverse etc.) that should not change in the near future. More experimental ones (ODBC, aws.s3) are proposed as additional packages. Comment 10: Think of a way to delegate the authorisation and authentication back to the resource owner. We would propose to change the way of passing credentials in the resource. Answer: The resource owner also owns the resource definition in the Opal server. Comment 11: Is it possible to restrict the usage of certain resources? For example, prohibit the use of resources. Answer: The resource usage requires user/group permission to be set up in Opal. In addition to that, the credentials used to access the resource can be permanently or temporarily invalidated by the resource owner. Comment 12: It would be nice to have some way of enforcing the metadata which is tight toi the data to be correct. This is often the problem with longitudinal data that metadata over all columns should be correct. It is likely that this also yields for big data structures that are analysed in a pooled manner. How do you handle multiple versions of VCF for example? Answer: This is a data harmonization issue that is usually addressed before the analysis. Data format validation, if needed, can also be performed by the analyst using DataSHIELD functions. Considering having different versions of VCF we aim to harmonize data by first doing imputation and solving issues concerning genomic strand and file format (PMID: 25495213). "],
["session-info.html", "23 Session Info", " 23 Session Info sessionInfo() R version 4.0.2 (2020-06-22) Platform: x86_64-w64-mingw32/x64 (64-bit) Running under: Windows 10 x64 (build 19041) Matrix products: default locale: [1] LC_COLLATE=Spanish_Spain.1252 LC_CTYPE=Spanish_Spain.1252 LC_MONETARY=Spanish_Spain.1252 [4] LC_NUMERIC=C LC_TIME=Spanish_Spain.1252 attached base packages: [1] stats4 parallel stats graphics grDevices utils datasets methods base other attached packages: [1] dsExposomeClient_0.1.0 dsGeoClient_0.1.0 [3] dsOmicsClient_1.0.0 ssh_0.7.0 [5] resourcer_1.0.1 dsBaseClient_6.0.1 [7] airway_1.8.0 SummarizedExperiment_1.18.2 [9] DelayedArray_0.14.1 matrixStats_0.57.0 [11] Homo.sapiens_1.3.1 TxDb.Hsapiens.UCSC.hg19.knownGene_3.2.2 [13] org.Hs.eg.db_3.11.4 GO.db_3.11.4 [15] OrganismDbi_1.30.0 GenomicFeatures_1.40.1 [17] AnnotationDbi_1.50.3 tweeDEseqCountData_1.26.0 [19] forcats_0.5.0 stringr_1.4.0 [21] dplyr_1.0.2 purrr_0.3.4 [23] readr_1.3.1 tidyr_1.1.2 [25] tibble_3.0.3 ggplot2_3.3.2 [27] tidyverse_1.3.0 kableExtra_1.2.1 [29] BiocStyle_2.16.0 GenomicRanges_1.40.0 [31] GenomeInfoDb_1.24.2 IRanges_2.22.2 [33] S4Vectors_0.26.1 snpStats_1.38.0 [35] Matrix_1.2-18 survival_3.2-3 [37] Biobase_2.48.0 BiocGenerics_0.34.0 [39] DSOpal_1.1.0 DSI_1.1.0 [41] R6_2.4.1 progress_1.2.2 [43] opalr_1.5.1 httr_1.4.2 loaded via a namespace (and not attached): [1] circlize_0.4.10 readxl_1.3.1 backports_1.1.9 corrplot_0.84 [5] BiocFileCache_1.12.1 sn_1.6-2 splines_4.0.2 BiocParallel_1.22.0 [9] TH.data_1.0-10 digest_0.6.25 htmltools_0.5.0 fansi_0.4.1 [13] magrittr_1.5 memoise_1.1.0 credentials_1.3.0 Biostrings_2.56.0 [17] modelr_0.1.8 sandwich_2.5-1 askpass_1.1 prettyunits_1.1.1 [21] jpeg_0.1-8.1 colorspace_1.4-1 blob_1.2.1 rvest_0.3.6 [25] rappdirs_0.3.1 ggrepel_0.8.2 haven_2.3.1 xfun_0.16 [29] crayon_1.3.4 RCurl_1.98-1.2 jsonlite_1.7.0 graph_1.66.0 [33] zoo_1.8-8 glue_1.4.2 gtable_0.3.0 zlibbioc_1.34.0 [37] XVector_0.28.0 webshot_0.5.2 shape_1.4.4 scales_1.1.1 [41] mvtnorm_1.1-1 DBI_1.1.0 bibtex_0.4.2.2 Rcpp_1.0.5 [45] metap_1.4 plotrix_3.7-8 viridisLite_0.3.0 tmvnsim_1.0-2 [49] bit_4.0.4 RColorBrewer_1.1-2 TFisher_0.2.0 ellipsis_0.3.1 [53] farver_2.0.3 pkgconfig_2.0.3 XML_3.99-0.5 dbplyr_1.4.4 [57] utf8_1.1.4 labeling_0.3 tidyselect_1.1.0 rlang_0.4.7 [61] munsell_0.5.0 cellranger_1.1.0 tools_4.0.2 cli_2.0.2 [65] generics_0.0.2 RSQLite_2.2.0 broom_0.7.0 mathjaxr_1.0-1 [69] evaluate_0.14 yaml_2.2.1 knitr_1.29 bit64_4.0.4 [73] fs_1.5.0 RBGL_1.64.0 mime_0.9 xml2_1.3.2 [77] biomaRt_2.44.1 compiler_4.0.2 rstudioapi_0.11 png_0.1-7 [81] curl_4.3 reprex_0.3.0 stringi_1.4.6 highr_0.8 [85] lattice_0.20-41 multtest_2.44.0 vctrs_0.3.3 mutoss_0.1-12 [89] pillar_1.4.6 lifecycle_0.2.0 BiocManager_1.30.10 GlobalOptions_0.1.2 [93] Rdpack_1.0.0 bitops_1.0-6 gbRd_0.4-11 rtracklayer_1.48.0 [97] bookdown_0.20 gridExtra_2.3 codetools_0.2-16 MASS_7.3-52 [101] assertthat_0.2.1 openssl_1.4.2 withr_2.2.0 GenomicAlignments_1.24.0 [105] Rsamtools_2.4.0 mnormt_2.0.2 multcomp_1.4-13 GenomeInfoDbData_1.2.3 [109] hms_0.5.3 grid_4.0.2 rmarkdown_2.3 numDeriv_2016.8-1.1 [113] lubridate_1.7.9 "],
["contributors.html", "24 Contributors", " 24 Contributors Juan R Gonzalez, Associate Research Professor at Barcelona Institute for Global Health (ISGlobal) and Adjunct Professor at Department of Mathematics at Autonomous University of Barcelona (UAB). Yannick Marcon, Software Engineer at Epigeny. Tom Bishop, Senior Data Scientist at the MRC Epidemiology Unit, University of Cambridge. Demetris Avraam, Data Scientist at the Population Health Sciences Institute, Newcastle University. Xavier Escribà-Montagut, PhD researcher at Barcelona Institute for Global Health (ISGlobal) "]
]
