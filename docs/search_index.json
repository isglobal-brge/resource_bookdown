[
["index.html", "Orchestrating non-disclosive big data analyses of data from different resources with R and DataSHIELD Welcome", " Orchestrating non-disclosive big data analyses of data from different resources with R and DataSHIELD 2020-06-10 Welcome This is the website for a book that provides users with some common workflows for the non-disclosive analysis of biomedical data with R and DataSHIELD from different resources. This book will teach you how to use the resourcer R package to perform any statistcal analysis from different studies having data in different formats (e.g., CSV, SPSS, R class, …). In particular, we focus on illustrating how to deal with Big Data by providing several examples from omic and geographical settings. To this end, we use cutting-edge Bioconductor tools to perform transcriptomic, epigenomic and genomic data analyses. Serveral R packages are used to perform analysis of geospatial data. We also provide examples of how performing non-disclosive analyses using secure SHELL commands by allowing the use of specific software that properly deals with Big Data outside R. This material serves as an online companion for the manuscript “Orchestrating non-disclosive big data analyses of shared data from different resources with R and DataSHIELD”. While we focus here in genomic and geospatial data, dozens of data analysis aplications interested in performing non-disclosive analysis having data in any specific format could be carried out. By learning the grammar of DataSHIELD workflows, we hope to provide you a starting point for the exploration of your own data, whether it be omic, geospatial or otherwise. This book is organized into five parts. In the Preamble, we introduce the book, provides a tutorial for key data infrastructure useful for omic and geospatial data and a general overview for learning how Opal and DataSHIELD allows performing non-disclosive data analyses from multiple studies simultaneously. So far, DataSHIELD uses tables from repository data in Opal which have some limitations to perfom Big Data analyses. The second part, Focus Topic, dive into information for for non-disclosive analyses using any type of resource which is one of the key advances provided in this work. It allow, among others, perform big data analyses using genomic or geospatial information where thousand of sensitive data have to be managed. The third part, Resources Extensions, provides examples illustrating how to extend existing resources. We describe how to create functions to deal with omic data in Variant Calling Format (VCF files) which specifies the format of a text file used in bioinformatics for storing gene sequence variations. It also shows how to perform genomic data analysis using secure shell commands. This can be consider as an illustrative example for using shell programs to perform Big Data analyses that could be extend to other frameworks of Big Data such as “Apache Spark”. The fourth part, Workflows, provides primarily code detailing the analysis of various datasets throughout the book. Initially, we provide examples from transcriptomic, epigenomic and genomic association studies, as well as examples linking geospatial data where data confidenciallity is an important issue. The fifth part, Developers, provides information about how to develop DataSHIELD packages using specific R packages devoted to Big Data analyses as well as using shell programs. It also provides some useful trips and trick that developers or end users may face when using our proposed infrastructure. Finally, the Appendix highlights our contributors. If you would like to cite this work, please use the reference “Orchestrating non-disclosive big data analyses of data from different resources with R and DataSHIELD”. "],
["introduction.html", "1 Introduction 1.1 What you will learn 1.2 Preliminaries 1.3 Acknowledgments", " 1 Introduction 1.1 What you will learn The goal of this book is to provide a solid foundation to non-disclosive data analysis using R and DataSHIELD/Opal through the resourcer R package. We illustrate how to preform such data analyses in two setting (omics and geospatial) where the use of the resources allows to handle Big Data problems. Nontheless, we also present workflows of how to perform statistical analyses using data in different formats than Opal tables. We aim to tackle key concepts covered in the manuscript, “Orchestrating non-disclosive big data analyses of shared data from different resources with R and DataSHIELD”, with each workflow covering these in varying detail, as well as essential preliminaries that are important for following along with the workflows on your own. 1.2 Preliminaries For those unfamiliar with R (and those looking to learn more), in the R section we provide some links to R books. Also there are several R on-line courses such as this one at DataCamp or this at CodeAcademy that can help to introduce the main R concepts. Nonetheless, we assume that the readers of this book are already familiar with R. For those interested in omic data analyses we recommend the book Omic association analysis with R and Bioconductor which provides a global overview of how to perform genomic, transcriptomic, epigenomic and multi-omic data analyses using R and Bioconductor packages. Bioconductor support provides the primary way to contact with both Bioconductor developers and users and it is a great way to search for answers to your questions. Bioconductor courses provides excellent material to learn most of the Bioconductor basics as well as other advanced methods and R related topics. [Omic data infrastructure][Bioconductor data infrastructures] deserves a section here since knowing common data containers are an essential part of Bioconductor workflows that used in our DataSHIELD packages designed for omic data analyses. This enable interoperability across packages, allowing for “plug and play” usage of cutting-edge methods. Geospatial data also requires specific data managment that is describe the [Geospatial Workflow] {#GIS} 1.3 Acknowledgments First, we would like to thank OBiBa and DataSHIELD developers for providing such as impresive framework for non-disclosive data analysis. We would also like to thank all Bioconductor contributors who made available their packages for dealing with different omic and performing state-of-the-arge data analyses. This has allow us to easily create DataSHIELD packages without the need of re-programming most of the omic association analyses. We also thank R package developers since their work will permit to implement other DataSHIELD packages specifically designed to address other biomedical, epidemiological or social science problems as we did with geospatial data. Finally, we would like to thank the Bioconductor core team for inspiring us to write this book. We follow the two succesful works describing how to Orchestrate High-Throughput Genomic and Single Cell Analysis with Bioconductor. "],
["learnR.html", "2 Learning R", " 2 Learning R R is a free, open-source software and programming language developed in 1995 at the University of Auckland as an environment for statistical computing and graphics (Ikaha and Gentleman, 1996). Since then R has become one of the dominant software environments for data analysis and is used by a variety of scientific disiplines, including biomedicine, environmental epidemioloy, social sciences, ecology, genetics and geoinformatics among others. CRAN Tasks provides an excellent overview of existing R packages for a given discipline (see for instance Genetics Task View ; Envirometrics Task View; Spatial Task View). R offers numerous advantages, such as: Free and Open source Reproducible Research repeatable: code + output in a single document easier the re-analyses scalable: applicable to small or large datasets extensible: several Getting help Numerous Discipline Specific R Groups Numerous Local R User Groups (including R-Ladies Groups) Stack Overflow Learning Resources R books (Free Online) R Books While some people find the use of a commandline environment daunting, it is becoming a necessary skill for scientists as the volume and variety of data has grown. Additionaly GUI interfaces can easily be implemented in R (see this review) being Shiny a widely used R package that makes it easy to build interactive web apps straight from R. "],
["BioC.html", "3 Bioconductor Data Structures 3.1 SNP Array Data 3.2 Expression Sets 3.3 Genomic Ranges 3.4 Summarized Experiments 3.5 Ranged Summarized Experiments 3.6 Multi Data Set", " 3 Bioconductor Data Structures Bioconductor promotes the statistical analysis and comprehension of current and emerging high-throughput biological assays. Bioconductor is based on packages written primarily in the R programming language. Bioconductor is committed to open source, collaborative, distributed software development and literate, reproducible research. Most Bioconductor components are distributed as R packages. The functional scope of Bioconductor packages includes the analysis of DNA microarray, sequence, flow, SNP, and other data. Bioconductor provides several data infrastructures for efficiently managing omic data. See this paper for a global overview. Here we provide a quick introduction for the most commonly used ones. We also recommend to learn how to deal with GenomicRanges which helps to efficiently manage genomic data information. 3.1 SNP Array Data SNP array data are normally stored in PLINK format (or VCF for NGS data). PLINK data are normally stored in three files .ped, .bim, .fam. The advantage is that SNP data are stored in binary format in the BED file (Homozygous normal 01, Heterozygous 02, Homozygous variant 03, missing 00). FAM file: one row per individual - identification information: Family ID, Individual ID Paternal ID, Maternal ID, Sex (1=male; 2=female; other=unknown), Phenotype. BIM file: one row per SNP (rs id, chromosome, position, allele 1, allele 2). BED file: one row per individual. Genotypes in columns. Data are easily loaded into R by using read.plink function require(snpStats) snps &lt;- read.plink(&quot;data/obesity&quot;) # there are three files obesity.fam, obesity.bim, obesity.bed names(snps) [1] &quot;genotypes&quot; &quot;fam&quot; &quot;map&quot; Genotypes is a snpMatrix object geno &lt;- snps$genotypes geno A SnpMatrix with 2312 rows and 100000 columns Row names: 100 ... 998 Col names: MitoC3993T ... rs28600179 Annotation is a data.frame object annotation &lt;- snps$map head(annotation) chromosome snp.name cM position allele.1 allele.2 MitoC3993T NA MitoC3993T NA 3993 T C MitoG4821A NA MitoG4821A NA 4821 A G MitoG6027A NA MitoG6027A NA 6027 A G MitoT6153C NA MitoT6153C NA 6153 C T MitoC7275T NA MitoC7275T NA 7275 T C MitoT9699C NA MitoT9699C NA 9699 C T 3.2 Expression Sets ExpressionSet Alt ExpressionSet Description Biobase is part of the Bioconductor project and contains standardized data structures to represent genomic data. The ExpressionSet class is designed to combine several different sources of information into a single convenient structure. An ExpressionSet can be manipulated (e.g., subsetted, copied) conveniently, and is the input or output from many Bioconductor functions. The data in an ExpressionSet consists of expression data from microarray experiments, `meta-data’ describing samples in the experiment, annotations and meta-data about the features on the chip and information related to the protocol used for processing each sample Print library(tweeDEseqCountData) data(pickrell) pickrell.eset ExpressionSet (storageMode: lockedEnvironment) assayData: 52580 features, 69 samples element names: exprs protocolData: none phenoData sampleNames: NA18486 NA18498 ... NA19257 (69 total) varLabels: num.tech.reps population study gender varMetadata: labelDescription featureData featureNames: ENSG00000000003 ENSG00000000005 ... LRG_99 (52580 total) fvarLabels: gene fvarMetadata: labelDescription experimentData: use &#39;experimentData(object)&#39; Annotation: Get experimental data (e.g., gene expression) genes &lt;- exprs(pickrell.eset) genes[1:4,1:4] NA18486 NA18498 NA18499 NA18501 ENSG00000000003 0 0 0 0 ENSG00000000005 0 0 0 0 ENSG00000000419 22 105 40 55 ENSG00000000457 22 100 107 53 Get phenotypic data (e.g. covariates, disease status, outcomes, …) pheno &lt;- pData(pickrell.eset) head(pheno) num.tech.reps population study gender NA18486 2 YRI Pickrell male NA18498 2 YRI Pickrell male NA18499 2 YRI Pickrell female NA18501 2 YRI Pickrell male NA18502 2 YRI Pickrell female NA18504 2 YRI Pickrell male pheno$gender [1] male male female male female male female male female male [11] female male female male female male female female male female [21] male female female male female male female female male female [31] female male female male female female female female male female [41] male male female female male female female male female female [51] male female male male female female male female male female [61] male female female male female female female male female Levels: female male This also works pickrell.eset$gender [1] male male female male female male female male female male [11] female male female male female male female female male female [21] male female female male female male female female male female [31] female male female male female female female female male female [41] male male female female male female female male female female [51] male female male male female female male female male female [61] male female female male female female female male female Levels: female male Subsetting (everything is synchronized) eSet.male &lt;- pickrell.eset[, pickrell.eset$gender==&quot;male&quot;] eSet.male ExpressionSet (storageMode: lockedEnvironment) assayData: 52580 features, 29 samples element names: exprs protocolData: none phenoData sampleNames: NA18486 NA18498 ... NA19239 (29 total) varLabels: num.tech.reps population study gender varMetadata: labelDescription featureData featureNames: ENSG00000000003 ENSG00000000005 ... LRG_99 (52580 total) fvarLabels: gene fvarMetadata: labelDescription experimentData: use &#39;experimentData(object)&#39; Annotation: Finally, the fData function gets the probes’ annotation in a data.frame. Let us first illustrate how to provide an annotation to an ExpressionSet object require(Homo.sapiens) geneSymbols &lt;- rownames(genes) annot &lt;- select(Homo.sapiens, geneSymbols, columns=c(&quot;TXCHROM&quot;, &quot;SYMBOL&quot;), keytype=&quot;ENSEMBL&quot;) annotation(pickrell.eset) &lt;- &quot;Homo.sapiens&quot; fData(pickrell.eset) &lt;- annot pickrell.eset ExpressionSet (storageMode: lockedEnvironment) assayData: 52580 features, 69 samples element names: exprs protocolData: none phenoData sampleNames: NA18486 NA18498 ... NA19257 (69 total) varLabels: num.tech.reps population study gender varMetadata: labelDescription featureData featureNames: 1 2 ... 59330 (59330 total) fvarLabels: ENSEMBL SYMBOL TXCHROM fvarMetadata: labelDescription experimentData: use &#39;experimentData(object)&#39; Annotation: Homo.sapiens probes &lt;- fData(pickrell.eset) head(probes) ENSEMBL SYMBOL TXCHROM 1 ENSG00000000003 TSPAN6 chrX 2 ENSG00000000005 TNMD chrX 3 ENSG00000000419 DPM1 chr20 4 ENSG00000000457 SCYL3 chr1 5 ENSG00000000460 C1orf112 chr1 6 ENSG00000000938 FGR chr1 3.3 Genomic Ranges GenomicRanges GRanges(): genomic coordinates to represent annotations (exons, genes, regulatory marks, …) and data (called peaks, variants, aligned reads) GRangesList(): genomic coordinates grouped into list elements (e.g., paired-end reads; exons grouped by transcript) Operations intra-range: act on each range independently e.g., shift() inter-range: act on all ranges in a GRanges object or GRangesList element e.g., reduce(); disjoin() between-range: act on two separate GRanges or GRangesList objects e.g., findOverlaps(), nearest() gr &lt;- GRanges(&quot;chr1&quot;, IRanges(c(10, 20, 22), width=5), &quot;+&quot;) gr GRanges object with 3 ranges and 0 metadata columns: seqnames ranges strand &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; [1] chr1 10-14 + [2] chr1 20-24 + [3] chr1 22-26 + ------- seqinfo: 1 sequence from an unspecified genome; no seqlengths # shift move all intervals 3 base pair towards the end shift(gr, 3) GRanges object with 3 ranges and 0 metadata columns: seqnames ranges strand &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; [1] chr1 13-17 + [2] chr1 23-27 + [3] chr1 25-29 + ------- seqinfo: 1 sequence from an unspecified genome; no seqlengths # inter-range range(gr) GRanges object with 1 range and 0 metadata columns: seqnames ranges strand &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; [1] chr1 10-26 + ------- seqinfo: 1 sequence from an unspecified genome; no seqlengths # two Granges: knowing the intervals that overlap with a targeted region snps &lt;- GRanges(&quot;chr1&quot;, IRanges(c(11, 17), width=1)) snps GRanges object with 2 ranges and 0 metadata columns: seqnames ranges strand &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; [1] chr1 11 * [2] chr1 17 * ------- seqinfo: 1 sequence from an unspecified genome; no seqlengths gr.ov &lt;- findOverlaps(snps, gr) gr.ov Hits object with 1 hit and 0 metadata columns: queryHits subjectHits &lt;integer&gt; &lt;integer&gt; [1] 1 1 ------- queryLength: 2 / subjectLength: 3 # recover the overlapping intervals gr[subjectHits(gr.ov)] GRanges object with 1 range and 0 metadata columns: seqnames ranges strand &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; [1] chr1 10-14 + ------- seqinfo: 1 sequence from an unspecified genome; no seqlengths #coverage: summarizes the times each base is covered by an interval coverage(gr) RleList of length 1 $chr1 integer-Rle of length 26 with 6 runs Lengths: 9 5 5 2 3 2 Values : 0 1 0 1 2 1 # get counts countOverlaps(gr, snps) [1] 1 0 0 This table shows the common operations of GenomicRanges 3.4 Summarized Experiments SummarizedExperiment Alt SummarizedExperiment Comprehensive data structure that can be used to store expression and methylation data from microarrays or read counts from RNA-seq experiments, among others. Can contain slots for one or more omic datasets, feature annotation (e.g. genes, transcripts, SNPs, CpGs), individual phenotypes and experimental details, such as laboratory and experimental protocols. As in an ExpressionSet a SummarizedExperiment, the rows of omic data are features and columns are subjects. Coordinate feature x sample ‘assays’ with row (feature) and column (sample) descriptions. ‘assays’ (similar to ‘exprs’ in ExpressionSetobjects) can be any matrix-like object, including very large on-disk representations such as HDF5Array ‘assays’ are annotated using GenomicRanges It is being deprecated 3.5 Ranged Summarized Experiments SummarizedExperiment is extended to RangedSummarizedExperiment, a child class that contains the annotation data of the features in a GenomicRanges object. An example dataset, stored as a RangedSummarizedExperiment is available in the airway package. This data represents an RNA sequencing experiment. library(airway) data(airway) airway class: RangedSummarizedExperiment dim: 64102 8 metadata(1): &#39;&#39; assays(1): counts rownames(64102): ENSG00000000003 ENSG00000000005 ... LRG_98 LRG_99 rowData names(0): colnames(8): SRR1039508 SRR1039509 ... SRR1039520 SRR1039521 colData names(9): SampleName cell ... Sample BioSample Some aspects of the object are very similar to ExpressionSet, although with slightly different names and types. colData contains phenotype (sample) information, like pData for ExpressionSet. It returns a DataFrame instead of a data.frame: colData(airway) DataFrame with 8 rows and 9 columns SampleName cell dex albut Run avgLength &lt;factor&gt; &lt;factor&gt; &lt;factor&gt; &lt;factor&gt; &lt;factor&gt; &lt;integer&gt; SRR1039508 GSM1275862 N61311 untrt untrt SRR1039508 126 SRR1039509 GSM1275863 N61311 trt untrt SRR1039509 126 SRR1039512 GSM1275866 N052611 untrt untrt SRR1039512 126 SRR1039513 GSM1275867 N052611 trt untrt SRR1039513 87 SRR1039516 GSM1275870 N080611 untrt untrt SRR1039516 120 SRR1039517 GSM1275871 N080611 trt untrt SRR1039517 126 SRR1039520 GSM1275874 N061011 untrt untrt SRR1039520 101 SRR1039521 GSM1275875 N061011 trt untrt SRR1039521 98 Experiment Sample BioSample &lt;factor&gt; &lt;factor&gt; &lt;factor&gt; SRR1039508 SRX384345 SRS508568 SAMN02422669 SRR1039509 SRX384346 SRS508567 SAMN02422675 SRR1039512 SRX384349 SRS508571 SAMN02422678 SRR1039513 SRX384350 SRS508572 SAMN02422670 SRR1039516 SRX384353 SRS508575 SAMN02422682 SRR1039517 SRX384354 SRS508576 SAMN02422673 SRR1039520 SRX384357 SRS508579 SAMN02422683 SRR1039521 SRX384358 SRS508580 SAMN02422677 You can still use $ to get a particular column: airway$cell [1] N61311 N61311 N052611 N052611 N080611 N080611 N061011 N061011 Levels: N052611 N061011 N080611 N61311 The measurement data are accessed by assay and assays. A SummarizedExperiment can contain multiple measurement matrices (all of the same dimension). You get all of them by assays and you select a particular one by assay(OBJECT, NAME) where you can see the names when you print the object or by using assayNames. In this case there is a single matrix called counts: assayNames(airway) [1] &quot;counts&quot; assays(airway) List of length 1 names(1): counts head(assay(airway, &quot;counts&quot;)) SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 ENSG00000000003 679 448 873 408 1138 ENSG00000000005 0 0 0 0 0 ENSG00000000419 467 515 621 365 587 ENSG00000000457 260 211 263 164 245 ENSG00000000460 60 55 40 35 78 ENSG00000000938 0 0 2 0 1 SRR1039517 SRR1039520 SRR1039521 ENSG00000000003 1047 770 572 ENSG00000000005 0 0 0 ENSG00000000419 799 417 508 ENSG00000000457 331 233 229 ENSG00000000460 63 76 60 ENSG00000000938 0 0 0 Annotation is a GenomicRanges object rowRanges(airway) GRangesList object of length 64102: $ENSG00000000003 GRanges object with 17 ranges and 2 metadata columns: seqnames ranges strand | exon_id exon_name &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;integer&gt; &lt;character&gt; [1] X 99883667-99884983 - | 667145 ENSE00001459322 [2] X 99885756-99885863 - | 667146 ENSE00000868868 [3] X 99887482-99887565 - | 667147 ENSE00000401072 [4] X 99887538-99887565 - | 667148 ENSE00001849132 [5] X 99888402-99888536 - | 667149 ENSE00003554016 ... ... ... ... . ... ... [13] X 99890555-99890743 - | 667156 ENSE00003512331 [14] X 99891188-99891686 - | 667158 ENSE00001886883 [15] X 99891605-99891803 - | 667159 ENSE00001855382 [16] X 99891790-99892101 - | 667160 ENSE00001863395 [17] X 99894942-99894988 - | 667161 ENSE00001828996 ... &lt;64101 more elements&gt; ------- seqinfo: 722 sequences (1 circular) from an unspecified genome Subset for only rows (e.g. features) which are in the interval 1 to 1Mb of chromosome 1 roi &lt;- GRanges(seqnames=&quot;1&quot;, ranges=IRanges(start=1, end=1e6)) subsetByOverlaps(airway, roi) class: RangedSummarizedExperiment dim: 79 8 metadata(1): &#39;&#39; assays(1): counts rownames(79): ENSG00000177757 ENSG00000185097 ... ENSG00000272512 ENSG00000273443 rowData names(0): colnames(8): SRR1039508 SRR1039509 ... SRR1039520 SRR1039521 colData names(9): SampleName cell ... Sample BioSample 3.6 Multi Data Set MultiDataSet Alt MultiDataSet Designed to encapsulate different types of datasets (including all classes in Bioconductor) It properly deals with non-complete cases situations Subsetting is easily performed in both: samples and features (using GenomicRanges) It allows to: – perform integration analysis with third party packages; – create new methods and functions for omic data integration; – encapsule new unimplemented data from any biological experiment MultiAssayExperiment is another infrastructure (created by BioC developers) that can be used to manage multi-omic data "],
["opal.html", "4 Opal 4.1 Introduction 4.2 Data Management 4.3 Security 4.4 R Integration 4.5 Demo", " 4 Opal 4.1 Introduction Opal is OBiBa’s core database application for epidemiological studies. Participant data, collected by questionnaires, medical instruments, sensors, administrative databases etc. can be integrated and stored in a central data repository under a uniform model. Opal is a web application that can import, process, copy data and has advanced features for cataloging the data (fully described, annotatted and searchable data dictionaries) as recommended by the Maelstrom Research group at McGill University, Canada. Opal is typically used in a research center to analyze the data acquired at assessment centres. Its ultimate purpose is to achieve seamless data-sharing among epidemiological studies. Opal is the reference implementation of the DataSHIELD infrastructure. More information on Opal can be seen in Opal description on OBiBa. Opal provides the following main features: Use of MongoDB, Mysql, MariaDB and/or PostgreSQL as database software backends, Import data from various file formats (CSV, SPSS, SAS, Stata etc.) and from SQL databases, Export data to various file formats (CSV, SPSS, SAS, Stata etc.) and to SQL databases, Plugin architecture to extend import/export capabilities for instance by connecting to data source software such as REDCap, LimeSurvey etc., Store data about any type of “entity”, such as subject, sample, geographic area, etc., Store data of any type (e.g., texts, numbers, geo-localisation, images, videos, etc.), Advanced authentication and authorization features, Reporting using R markdown, Data analysis plugins using R, Web services can be accessed using R, Python, Java, Javascript, DataSHIELD middleware reference implementation (configuration, access controls, R session management). 4.2 Data Management In Opal the data sets are represented by tables which are grouped by projects. A table has variables (columns) and entity values (rows). Opal has also a concept of views, which are logical tables where variable derivation scripts are defined. The storage of the data and of the meta-data (data dictionaries) is done in a managed database (can be a SQL database such as MySQL, MariaDB or PostgreSQL, or a document-oriented database such as MongoDB). Detailed concepts and tutorials for tables can be found here: Variables and Data Identifiers Mappings Data Harmonization 4.3 Security All Opal operations are accessible through web services that require authentication and proper authorization. The permissions can be granted to a specific user or a group of users, can be applied to a project or to a table and have different levels: read-only meta-data (access to the data dictionary without access to the individual-level data), read-only, or write permissions. The programmatic authentication can make use of username/password credentials, token or 2-way SSL authentication methods. Opal can also integrate with the hosting institution’s users registry using the OpenID Connect standard. 4.4 R Integration Opal connects to a R server to perform different kind of operations: data import/export (using R packages), data analysis (by transfering data from Opal’s database into a R server session and using R packages). The R server is based on the Rserve R package. The user R sessions that are running in this R server are managed by Opal. This Opal/R integration works well for small to mid-size datasets (usually less than 10M data points). For bigger datasets, extracting and transferring data from the database to the R server is time, CPU and memory consuming. In this work, we will present a more flexible data description paradigm called resources that will enable Opal to manage access to Big Data sets, complex data structures and computation units for analysis purpose, while still having the security and the analysis features provided by Opal. 4.5 Demo To have a closer look at Opal you can try the demo site with the credentials: username: administrator password: password In order to make the reader familiar with Opal we recommend to visit the Opal online documentation. "],
["datashield.html", "5 DataSHIELD 5.1 Introduction 5.2 DataSHIELD R Interface (DSI) 5.3 DataSHIELD/Opal Implementation", " 5 DataSHIELD 5.1 Introduction Some research projects require pooling data from several studies to obtain sample sizes large and diverse enough for detecting interactions. Unfortunately, important ethico-legal constraints often prevent or impede the sharing of individual-level data across multiple studies. DataSHIELD aims to address this issue. DataSHIELD is a method that enables advanced statistical analysis of individual-level data from several sources without actually pooling the data from these sources together. DataSHIELD facilitates important research in settings where: a co-analysis of individual-level data from several studies is scientifically necessary but governance restrictions prevent the release or sharing of some of the required data, and/or render data access unacceptably slow, equivalent governance concerns prevent or hinder access to a single dataset, a research group wishes to actively share the information held in its data with others but does not wish to cede control of the governance of those data and/or the intellectual property they represent by physically handing over the data themselves, a dataset which is to be remotely analysed, or included in a multi-study co-analysis, contains data objects (e.g. images) which are too large to be physically transferred to the site of analysis. A typical DataSHIELD infrastructure (see Figure 5.1) is composed of one central analysis node (the DataSHIELD client) connected to one or several data analysis nodes (the DataSHIELD servers). In each of these server nodes, there is a R server application which can be accessed only through a DataSHIELD compliant middleware application. This middleware application acts as a broker for managing R server sessions in a multi-user environment, assigning data and launching analysis in the R server. The analysis execution environment is then fully controlled: users must be authenticated, must have the proper permissions to access the data of interest and can only perform some predefined assignment and aggregation operations. Importantly, the operations that are permitted are designed to prevent the user having access to individual data items while still allowing useful work to be done with the data. For example, users can fit a generalised linear model to a dataset and receive information about the model coefficients, but are not given the residuals, as these could be used to reconstruct the original data.The reference implementation of this DataSHIELD infrastructure is based on the Opal data repository. Figure 5.1: Typical DataSHIELD infrastructure, including one central analysis node (client) and several data nodes (servers). The client node interacts programmatically in R with the server nodes using the DataSHIELD Interface implemented as the DSI R package. The DSI defines prototype functions to authenticate the user and to perform assignment and aggregation operations in each of the R servers sitting in the server nodes. The reference implementation of DSI is the DSOpal R package. An alternate implementation of DSI is the DSLite R package which mainly targets DataSHIELD developers by offering a pure R implementation of the whole DataSHIELD infrastructure. 5.2 DataSHIELD R Interface (DSI) The DataSHIELD Interface (DSI) defines a set of S4 classes and generic methods that can be implemented for accessing a data repository supporting the DataSHIELD infrastructure: controlled R commands to be executed on the server side are garanteeing that non disclosive information is returned to client side. 5.2.1 Classes Structure The DSI S4 classes are: Class Description DSObject A common base class for all DSI, DSDriver A class to drive the creation of a connection object, DSConnection Allows the interaction with the remote server; DataSHIELD operations such as aggregation and assignment return a result object; DataSHIELD setup status check can be performed (dataset access, configuration comparision), DSResult Wraps access to the result, which can be fetched either synchronously or asynchronously depending on the capabilities of the data repository server. All classes are virtual: they cannot be instantiated directly and instead must be subclassed. See DSOpal for a reference implementation of DSI based on the Opal data repository. These S4 classes and generic methods are meant to be used for implementing connection to a DataSHIELD-aware data repository. 5.2.2 Higher Level Functions In addition to these S4 classes, DSI provides functions to handle a list of remote data repository servers: Functions Description datashield.login Create DSConnection objects to the data repositories, using the DSDriver specification. datashield.logout Destroy the DSConnections objects. datashield.aggregate, datashield.assign Typical DataSHIELD operations on DSConnection objects, which result will be fetched through DSResult objects. datashield.connections, datashield.connections_default, datashield.connections_find Management of the list of DSConnection objects that will be discovered and used by the client-side analytic functions. datashield.workspaces, datashield.workspace_save, datashield.workspace_rm Manage R images of the remote DataSHIELD sessions (to speed up data analysis sessions). datashield.symbols, datashield.symbol_rm Minimalistic management of the R symbols living in the remote DataSHIELD sessions. datashield.tables, datashield.table_status List the tables and their accessibility across a set of data repositories. datashield.resources, datashield.resource_status List the resources and their accessibility across a set of data repositories. datashield.pkg_status, datashield.method_status, datashield.methods Utility functions to explore the DataSHIELD setup across a set of data repositories. These datashield.* functions are meant to be used by DataSHIELD packages developers and users. 5.2.3 Options Some options can be set to modify the behavior of the DSI: Option Description datashield.env The R environment in which the DSConnection object list is to be looked for. Default value is the Global Environment: globalenv(). datashield.progress A logical to enable the visibility of the progress bars. Default value is TRUE. datashield.progress.clear A logical to make the progress bar disappear after it has been completed. Default value is FALSE. datashiel.error.stop A logical to alter error handling behavior: if TRUE an error is raised when at least one server has failed, otherwise a warning message is issued. Default value is TRUE. 5.3 DataSHIELD/Opal Implementation Opal is a web application that is accessible through web services. It is implementing the DataSHIELD method thanks to the following built-in features: integration with a R server, where the DataSHIELD operations will take place, secure data management, with fine-grain permissions (to restrict access to individual level data), web services API, that allows to run Opal operations from a R script. In addition to these features, Opal manages the DataSHIELD configuration which consists of declaring the set of the allowed aggregation/assignment R functions and some R options. 5.3.1 Client The opalr R package is a general purpose Opal connection R library (authentication is required) that is used to perform various operations (authorization may be required). The DSOpal R package is an implementation of the DSI, built on top of opalr. All the DataSHIELD operations are transparently applied to one or more Opal server using the DSI higher-level functions. Opal also supports asynchronous function calls (submission of a R operation, then later retrieval of the result) which allows to operate on several DataSHIELD analysis nodes in parallel. 5.3.2 Server On the Opal’s R server side, some DataSHIELD-compliant R packages can be managed using the Opal’s web interface: installation, removal of DataSHIELD-compliant R packages and automatic DataSHIELD configuration discovery. Opal guarantees that only the allowed functions can be called. The DataSHIELD-compliant R package guarantees that only aggregated results are returned to the client. The term ‘aggregated’ here means that the data in the R server will go through a function that summarizes individual-level data into a non-disclosive form. For example, obtaining the length of a vector, or obtaining the summary statistics of a vector (min, max, mean, etc.). These DataSHIELD functions that are customisable. That is, administrators of the Opal server can add, remove, modify or create completely custom aggregating methods that are proposed to DataSHIELD clients. Figure 5.2: DataSHIELD configuration in Opal When performing a DataSHIELD analysis session, a typical workflow on a single Opal analysis node is the following: authentication of the user (autorization to use DataSHIELD service is required), creation and initialization of a R server session, assignment of Opal-managed data into the R server session (data access authorization is required), processing of the incoming R operation requests (aggregation and assignment function calls authorization is required) that are forwarded to the R server session; non-disclosive aggregated result is then returned to the R client. 5.3.3 Demo Readers can read the DataSHIELD page in Opal documentation to have a global overview about how to use DataSHIELD functions. It describes how to perform basic statistical analyses, linear and generalized linear models and some data visualization. A complete description of how DataSHIELD works, with lots of materials, examples, courses and real data analyses can be obtained in the DataSHIELD Wiki. The following is a simple illustration of how to analyze some data available in the Opal demo site. The Projects page gives access to the different projects avaialble in this Opal server. If we select the SURVIVAL project we see that there are three tables: Figure 5.3: Tables available in the SURVIVAL project from our Opal example First we build a connection object with user credentials and the location of the server for each of the study: library(DSOpal) builder &lt;- newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;administrator&quot;, password = &quot;password&quot;) builder$append(server = &quot;study2&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;administrator&quot;, password = &quot;password&quot;) builder$append(server = &quot;study3&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;administrator&quot;, password = &quot;password&quot;) logindata &lt;- builder$build() logindata server url table resource driver user 1 study1 https://opal-demo.obiba.org OpalDriver administrator 2 study2 https://opal-demo.obiba.org OpalDriver administrator 3 study3 https://opal-demo.obiba.org OpalDriver administrator password token options 1 password 2 password 3 password Then we perform login into each of the analysis servers and assign for each of them a different table with the same R symbol name: conns &lt;- datashield.login(logindata) datashield.assign.table(conns, symbol = &quot;D&quot;, table = list(study1 = &quot;SURVIVAL.EXPAND_WITH_MISSING1&quot;, study2 = &quot;SURVIVAL.EXPAND_WITH_MISSING2&quot;, study3 = &quot;SURVIVAL.EXPAND_WITH_MISSING3&quot;)) datashield.symbols(conns) $study1 [1] &quot;D&quot; $study2 [1] &quot;D&quot; $study3 [1] &quot;D&quot; # TODO show up some basic analysis Finaly we clean up the R server sessions: datashield.logout(conns) "],
["resources.html", "6 The Resources 6.1 Concept 6.2 Types 6.3 Definition", " 6 The Resources Developing and implementing new algorithms to perform advanced data analyses under DataSHIELD framework is a current active line of research. However, the analysis of big data within DataSHIELD has some limitations. Some of them are related to how data is managed in the Opal’s database and others are related to how to perform statistical analyses of big data within the R environment. Opal databases are for general purpose and do not properly manage large amounts of information and, second, it requires moving data from original repositories into Opal which is inefficient (this is a time, CPU and memory consuming operation) and is difficult to maintain when data are updated. We have overcome the problem related to DataSHIELD big data management by developing a new data infrastructure within Opal: the resources. 6.1 Concept Resources are datasets or computation units which location is described by a URL and access is protected by credentials. When assigned to a R/DataSHIELD server session, remote big/complex datasets or high performance computers are made accessible to data analysts. Instead of storing the data in Opal’s database, only the way to access them is to be defined: the datasets are kept in their original format and location (a SQL database, a SPSS file, R object, etc.) and are read directly from the R/DataSHIELD server-side session. Then as soon as there is a R reader for the dataset or a connector for the analysis services, a resource can be defined. Opal takes care of the DataSHIELD permissions (a DataSHIELD user cannot see the resource’s credentials) and of the resources assignment to a R/DataSHIELD session (see Figure 6.1) Figure 6.1: Resources: a new DataSHIELD infrastructure 6.2 Types The data format refers to the intrinsic structure of the data. A very common family of data formats is the tabular format which is made of rows (entities, records, observations etc.) and columns (variables, fields, vectors etc.). Examples of tabular formats are the delimiter-separated values formats (CSV, TSV etc.), the spreadsheet data formats (Microsoft Excel, LibreOffice Calc, Google Sheets etc.), some proprietary statistical software data formats (SPSS, SAS, Stata etc.), the database tables that can be stored in structured database management systems that are row-oriented (MySQL, MariaDB, PostgreSQL, Oracle, SQLite etc.) or column-oriented (Apache Cassandra, Apache Parquet, MariaDB ColumnStore, BigTable etc.), or in semi-structured database management systems such as the documented-oriented databases (MongoDB, Redis, CouchDB, Elasticsearch etc.). When the data model is getting complex (data types and objects relationships), a domain-specific data format is sometimes designed to handle this complexity so that statistical analysis and data retrieval can be executed as efficiently as possible. Examples of domain-specific data formats are encountered in the omic or geospatial fields of research that are described in the Workflows part: Omic and Geospatial. A data format can also include some additional features such as data compression, encoding or encryption. Each data format requires an appropriate reader software library or application to extract the information or perform data aggregation or filtering operations. We have prepared a test environment, with the Opal implementation of Resources and an appropriate R/DataSHIELD configuration that is available at: opal-demo.obiba.org. This figure illustrate the resources which are available for the test project and can serve as a starting example of the different types of resources that can be dealt with Figure 6.2: Resources from a test enviroment available at https://opal-demo.obiba.org As it can be seen, the data storage can simply be a file to be accessed directly from the host’s file system or to be downloaded from a remote location. More advanced data storage systems are software applications that expose an interface to query, extract or analyse the data. These applications can make use of a standard programming interface (e.g. SQL) or expose specific web services (e.g. based on the HTTP communication protocol) or provide a software library (in different programming languages) to access the data. These different ways of accessing the data are not exclusive from each other. In some cases the micro-data cannot be extracted, only computation services that return aggregated data are provided. The data storage system can also apply security rules, requiring authentication and proper authorisations to access or analyse the data. 6.3 Definition We call resource this data or computation access description. A resource will have the following properties: the location of the data or of the computation services, the data format (if this information cannot be inferred from the location property), the access credentials (if some apply). The resource location description will make use of the web standard described in the RFC 3986 “Uniform Resource Identifier (URI): Generic Syntax”. More specifically, the Uniform Resource Locator (URL) specification is what we need for defining the location of the data or computation resource: the term Uniform allows to describe the resource the same way, independently of its type, location and usage context; the term Resource does not limit the scope of what might be a resource, e.g. a document, a service, a collection of resources, or even abstract concepts (operations, relationships, etc.); the term Locator both identifies the resource and provides a means of locating it by describing its access mechanism (e.g. the network location). The URL syntax is composed of several parts: a scheme, that describes how to access the resource, e.g. the communication protocols “https” (secured HTTP communication), “ssh” (secured shell, for issuing commands on a remote server), or “s3” (for accessing Amazon Web Service S3 file store services), an authority (optional), e.g. a server name address, a path that identifies/locates the resource in a hierarchical way and that can be altered by query parameters. The resource’s data format might be inferred from the path part of the URL, by using the file name suffix for instance. Nevertheless, sometimes it is not possible to identify the data format because the path could make sense only for the data storage system, for example when a file store designates a document using an obfuscated string identifier or when a text-based data format is compressed as a zip archive. The format property can provide this information. Despite the authority part of the URL can contain some user information (such as the username and password), it is discouraged to use this capability for security considerations. The resource’s credentials property will be used instead, and will be composed of an identifier sub-property and a secret sub-property, which can be used for authenticating with a username/password, or an access token, or any other credentials encoded string. The advantage of separating the credentials property from the resource location property is that a user with limited permissions could have access to the resource’s location information while the credentials are kept secret. Once a resource has been formally defined, it should be possible to build programmatically a connection object that will make use of the described data or computation services. This resource description is not bound to a specific programmatic language (the URL property is a web standard, other properties are simple strings) and does not enforce the use of a specific software application for building, storing and interpreting a resource object. Next Section describes the resourcer package which is an R implementation of the data and computation resources description and connection. There the reader can see some examples of how dealing with different resources in DataSHIELD. "],
["resourcer.html", "7 The resourcer Package 7.1 Introduction 7.2 File Resources 7.3 Database Resources 7.4 Computation Resources 7.5 Interacting with R Resources 7.6 Extending Resources 7.7 Resource Forms 7.8 Examples of Resources", " 7 The resourcer Package 7.1 Introduction The resourcer package is an R implementation of the data and computation resources description and connection. It is reusing many existing R packages for reading various data formats and connecting to external data storage or computation servers. The resourcer package role is to interpret a resource description object to build the appropriate resource connection object. Because the bestiary of resources is very wide, the resourcer package provides a framework for dynamically extending the interpretation capabilities to new types of resources. This framework uses the object-oriented paradigm provided by the R6 library. It is meant to access resources identified by a URL in a uniform way whether it references a dataset (stored in a file, a SQL table, a MongoDB collection etc.) or a computation unit (system commands, web services etc.). Usually some credentials will be defined, and an additional data format information can be provided to help dataset coercing to a data.frame object. The main concepts are: Class Description resource Access to a resource (dataset or computation unit) is described by an object with URL, optional credentials and optional data format properties. ResourceResolver A ResourceClient factory based on the URL scheme and available in a resolvers registry. ResourceClient Realizes the connection with the dataset or the computation unit described by a resource. FileResourceGetter Connect to a file described by a resource. DBIResourceConnector Establish a DBI connection. 7.2 File Resources These are resources describing a file. If the file is in a remote location, it must be downloaded before being read. The data format specification of the resource helps to find the appropriate file reader. 7.2.1 File Getter The file locations supported by default are: file, local file system, http(s), web address, basic authentication, gridfs, MongoDB file store, scp, file copy through SSH, opal, Opal file store. This can be easily applied to other file locations by extending the FileResourceGetter class. An instance of the new file resource getter is to be registered so that the FileResourceResolver can operate as expected. resourcer::registerFileResourceGetter(MyFileLocationResourceGetter$new()) 7.2.2 File Data Format The data format specified within the resource object, helps at finding the appropriate file reader. Currently supported data formats are: the data formats that have a reader in tidyverse: readr (csv, csv2, tsv, ssv, delim), haven (spss, sav, por, dta, stata, sas, xpt), readxl (excel, xls, xlsx). This can be easily applied to other data file formats by extending the FileResourceClient class. the R data format that can be loaded in a child R environment from which object of interest will be retrieved. Usage example that reads a local SPSS file: # make a SPSS file resource res &lt;- resourcer::newResource( name = &quot;CNSIM1&quot;, url = &quot;file:///data/CNSIM1.sav&quot;, format = &quot;spss&quot; ) # coerce the csv file in the opal server to a data.frame df &lt;- as.data.frame(res) To support other file data format, extend the FileResourceClient class with the new data format reader implementation. Associate factory class, an extension of the ResourceResolver class is also to be implemented and registered. resourcer::registerResourceResolver(MyFileFormatResourceResolver$new()) 7.3 Database Resources 7.3.1 DBI Connectors DBI is a set of virtual classes that are are used to abstract the SQL database connections and operations within R. Then any DBI implementation can be used to access to a SQL table. Which DBI connector to be used is an information that can be extracted from the scheme part of the resource’s URL. For instance a resource URL starting with postgres:// will require the RPostgres driver. To separate the DBI connector instanciation from the DBI interface interactions in the SQLResourceClient, a DBIResourceConnector registry is to be populated. The currently supported SQL database connectors are: mariadb MariaDB connector, mysql MySQL connector, postgres or postgresql Postgres connector, presto, presto+http or presto+https Presto connector, spark, spark+http or spark+https Spark connector. To support another SQL database having a DBI driver, extend the DBIResourceConnector class and register it: resourcer::registerDBIResourceConnector(MyDBResourceConnector$new()) 7.3.2 Use dplyr Having the data stored in the database allows to handle large (common SQL databases) to big (PrestoDB, Spark) datasets using dplyr which will delegate as much as possible operations to the database. 7.3.3 Document Databases NoSQL databases can be described by a resource. The nodbi can be used here. Currently only connection to MongoDB database is supported using URL scheme mongodb or mongodb+srv. 7.4 Computation Resources Computation resources are resources on which tasks/commands can be triggerred and from which resulting data can be retrieved. Example of computation resource that connects to a server through SSH: # make an application resource on a ssh server res &lt;- resourcer::newResource( name = &quot;supercomp1&quot;, url = &quot;ssh://server1.example.org/work/dir?exec=plink,ls&quot;, identity = &quot;sshaccountid&quot;, secret = &quot;sshaccountpwd&quot; ) # get ssh client from resource object client &lt;- resourcer::newResourceClient(res) # does a ssh::ssh_connect() # execute commands files &lt;- client$exec(&quot;ls&quot;) # exec &#39;cd /work/dir &amp;&amp; ls&#39; # release connection client$close() # does ssh::ssh_disconnect(session) 7.5 Interacting with R Resources As the ResourceClient is just a connector to a resource, to make this useful some data conversion functions are defined by default: R data.frame, which is the most common representation of tabular data in R. A data frame, as defined in R base, is an object stored in memory that may be not suitable for large to big datasets. dplyr tbl, which is another representation of tabular data that nicely integrates with the DBI: filtering, mutation and aggregation operations can be delegated to the underlying SQL database, reducing the R memory and computation footprint. Useful functions are also provided to perform join operations on relational datasets. A data.frame can be accessed as a tbl and vice versa. In the case when the resource is a R object, the RDataFileResourceClient offers the ability to get the internal raw data object. Then complex data structures, optimized for a specific research domain can be accessed with the most appropriate tools. When the resource is a computation service provider, the interaction with the resource client will consist of issuing commands/requests with parameters and getting the result from it either as a response object or as a file to be downloaded. Another way of interacting with a resource is to get the internal connection object (a database connector, a SSH connector etc.) from the ResourceClient and then apply any kind of operations that are defined for it. The general purpose of a resource is not to substitute itself to the underlying library, it is to facilitate the access to the related data and services. 7.6 Extending Resources There are several ways to extend the Resources handling. These are based on different R6 classes having a isFor(resource) function: If the resource is a file located at a place not already handled, write a new FileResourceGetter subclass and register an instance of it with the function registerFileResourceGetter(). If the resource is a SQL engine having a DBI connector defined, write a new DBIResourceConnector subclass and register an instance of it with the function registerDBIResourceConnector(). If the resource is in a domain specific web application or database, write a new ResourceResolver subclass and register an instance of it with the function registerResourceResolver(). This ResourceResolver object will create the appropriate ResourceClient object that matches your needs. The design of the URL that will describe your new resource should not overlap an existing one, otherwise the different registries will return the first instance for which the isFor(resource) is TRUE. In order to distinguish resource locations, the URL’s scheme can be extended, for instance the scheme for accessing a file in a Opal server is opal+https so that the credentials be applied as needed by Opal. In order to simplify the usage of these resource client classes, the resourcer package combines several software design patterns: The factory pattern, a classic creational design pattern. It is realized by the ResourceResolver R6 class which is responsible for making a ResourceClient object matching a provided resource object. The registry pattern, “a well-known object that other objects can use to find common objects and services” as described by M. Fowler. It is basically a global list of objects to iterate to find the appropriate one. In the resourcer package there are several registries: (1) the registry of ResourceResolver objects (the ResourceClient factories), (2) the registry of FileResourceGetter objects and (3) the registry of DBIResourceConnector objects. The self-registration pattern, which consists of delegating the registration of new services to their provider. The package event mechanism of R is used so that a R package self-registers its resource components in the previously mentioned registries when the package is loaded at runtime. For implementing ResourceClient factories the resourcer package provides different ResourceResolver classes (see Figure 7.1): for file resources, which will discriminate the resources based on the URL property (checking if any FileResourceGetter exists for that resource) and the data format property (for getting additional information about how to read the data). The FileResourceGetter can be extended to new file locations: as an example, the aws.resourcer R package is able to get resource files from Amazon Web Service S3 file stores. for database resources, which will discriminate the resources based on the scheme part of the URL (checking if any DBIResourceConnector can be found or whether a nodbi connector can be created). The DBIResourceConnector can be extended to new DBI implementations. For instance, using bigrquery R package, it would be easy to implement access to a resource stored in a Google’s BigQuery database. for command-based computation resources, which will discriminate the resources based on the scheme part of the URL, indicating how to issue commands (local shell or secure remote shell). Figure 7.1: ResourceResolver class diagram Additional ResourceResolver and ResourceClient extensions could be implemented for accessing domain specific applications which would expose data extraction and/or analysis services, the only requirement is that a R connection API exists for the considered application. The process of creating a new ResourceClient instance from a resource object consists of iterating over a registry of ResourceResolver instances and finding the one that can handle the resource object by inspecting its properties (URL, data format etc.). The URL property inspection can imply a lookup in the additional registries: if the resource is a data file, the file resource resolvers will check in the FileResourceGetter registry whether there is one that applies; if the resource is a DBI-compatible database, the SQL resource resolver will check in the DBIResourceConnector registry whether there is one that applies. This workflow is simply triggered by a resourcer’s function call. 7.7 Resource Forms As it can be error prone to define a new resource, when a URL is complex, or when there is a limited choice of formats or when credentials can be on different types, it is recommended to declare the resources forms and factory functions within the R package. This resource declaration is to be done in javascript, as this is a very commonly used language for building graphical user interfaces. These files are expected to be installed at the root of the package folder (then in the source code of the R package, they will be declared in the inst/resources folder), so that an external application can lookup statically the packages having declared some resources. The configuration file inst/resources/resource.js is a javascript file which contains an object with the properties: settings, a JSON object that contains the description and the documentation of the web forms (based on the json-schema specification). asResource, a javascript function that will convert the data captured from one of the declared web forms into a data structure representing the resource object. As an example (see also resourcer’s resource.js): var myPackage = { settings: { &quot;title&quot;: &quot;MyPackage resources&quot;, &quot;description&quot;: &quot;MyPackage resources are for etc.&quot;, &quot;web&quot;: &quot;https://github.com/org/myPackage&quot;, &quot;categories&quot;: [ { &quot;name&quot;: &quot;my-format&quot;, &quot;title&quot;: &quot;My data format&quot;, &quot;description&quot;: &quot;Data are files in my format, that will be read by myPackage etc.&quot; } ], &quot;types&quot;: [ { &quot;name&quot;: &quot;my-format-http&quot;, &quot;title&quot;: &quot;My data format - HTTP&quot;, &quot;description&quot;: &quot;Data are files in my format, that will be downloaded from a HTTP server etc.&quot;, &quot;tags&quot;: [&quot;my-format&quot;, &quot;http&quot;], &quot;parameters&quot;: {}, &quot;credentials&quot;: {} } ] }, asResource: function(type, name, params, credentials) { // make a resource object from arguments, using type to drive // what params/credentials properties are to be used // a basic example of resource object: return { &quot;name&quot;: name, &quot;url&quot;: params.url, &quot;format&quot;: params.format, &quot;identity&quot;: credentials.username, &quot;secret&quot;: credentials.password }; } } The specifications for the settings object is the following: Property Type Description title string The title of the set of resources. description string The description of the set of resources. web string A web link that describes the resources. categories array of object A list of category objects which are used to categorize the declared resources in terms of resource location, format, usage etc. types array of object A list of type objects which contains a description of the parameters and credentials forms for each type of resource. Where the category object is: Property Type Description name string The name of the category that will be applied to each resource type, must be unique. title string The title of the category. description string The description of the category. And the type object is: Property Type Description name string The identifying name of the resource, must be unique. title string The title of the resource. description string The description of the resource form. tags array of string The tag names that are applied to the resource form. parameters object The form that will be used to capture the parameters to build the url and the format properties of the resource (based on the json-schema specification). Some specific fields can be used: _package to capture the R package name or _packages to capture an array of R package names to be loaded prior to the resource assignment. credentials object The form that will be used to capture the access credentials to build the identity and the secret properties of the resource (based on the json-schema specification). The asResource function is a javascript function which signature is function(type, name, params, credentials) where: type, the form name used to capture the resource parameters and credentials, name, the name to apply to the resource, params, the captured parameters, credentials, the captured credentials. The name of the root object must follow the pattern: &lt;R package&gt; (note that any dots (.) in the R package name are to be replaced by underscores (_)). A real example of how to create this file for the `{r Githubpkg(“isglobal-brge”, “dsOmics”)} package (described in this Section) can be found here. 7.8 Examples of Resources Let us illustrate how to deal with different types of resources within DataSHIELD. To this end, let use our Opal test example available at https://opal-demo.obiba.org which has the following reources Figure 7.2: Resources from a test enviroment available at https://opal-demo.obiba.org Let us start by illustrating how to get a simple TSV file (brge.txt) into the R server. This file is located at a GitHub repository: https://raw.githubusercontent.com/isglobal-brge/brgedata/master/inst/extdata/brge.txt and it is not necesary to be moved from there. This is one of the main strenght of the resources implementation. 7.8.1 TSV File into a tibble or data.frame This code describes how to get the resource (a TSV file) as a data.frame into the R Server. Note that this is a secure access since user name and password must be provided library(DSOpal) library(dsBaseClient) # access to the &#39;brge&#39; resource (NOTE: RSRC.brge is need since the project # is called test) builder &lt;- newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.brge&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() # the resource is loaded into R as the object &#39;res&#39; conns &lt;- datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) # the resource is assigned to a data.frame # Assign to the original R class (e.g ExpressionSet) datashield.assign.expr(conns, symbol = &quot;dat&quot;, expr = quote(as.resource.data.frame(res))) ds.class(&quot;dat&quot;) $study1 [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; # logout the connection datashield.logout(conns) 7.8.2 R Data File into a R object Now let us describe how to get an specific type of R object into de R server. Our Opal test contains a resource called GSE80970 which is in a local machine. The resource is an R object of class ExpressionSet which is normally used to jointly capsulate gene expression, metadata and annotation. In general, we can retrieve any R object in their original format and if a method to coerce the specific object into a data.frame exists, we can also retrieve it as a tibble/data.frame. # prepare login data and resource to assign builder &lt;- newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.GSE80970&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() # login and assign resource (to &#39;res&#39; symbol) conns &lt;- datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) # coerce ResourceClient objects to a data.frame called &#39;DF&#39; # NOTE: as.data.frame exists for `ExpressionSet` objects datashield.assign.expr(conns, symbol = &quot;DF&quot;, expr = quote(as.resource.data.frame(res))) ds.class(&quot;DF&quot;) $study1 [1] &quot;data.frame&quot; # we can also coerce ResourceClient objects to their original format. # This will allow the analyses with specific R/Bioconductor packages datashield.assign.expr(conns, symbol = &quot;ES&quot;, expr = quote(as.resource.object(res))) ds.class(&quot;ES&quot;) $study1 [1] &quot;ExpressionSet&quot; attr(,&quot;package&quot;) [1] &quot;Biobase&quot; # logout the connection datashield.logout(conns) "],
["omic-extension.html", "8 VCF files to GDS to peform GWAS with Bioconductor", " 8 VCF files to GDS to peform GWAS with Bioconductor Genomic data can be stored in different formats. PLINK and VCF files are commonly used in genetic epidemiology studies. In order to deal with this type of data, we have extended the resources available at the resourcer package to VCF files. NOTE: PLINK files can be translated into VCF files using different pipelines. In R you can use SeqArray to get VCF files. We use the Genomic Data Storage (GDS) format which efficiently manage VCF files into the R environment. This extension requires to create a Client and a Resolver function for the resourcer that are located into the dsOmics package. The client function uses snpgdsVCF2GDS function implemented in SNPrelate to coerce the VCF file to a GDS object. Then the GDS object is loaded into R as an object of class GdsGenotypeReader from GWASTools package that facilitates downstream analyses. The opal API server allows to incorporate this new type of resource as illustrated in Figure 8.1. Figure 8.1: Description of how a VCF file can be added to the opal resources It is important to notice that the URL should contain the tag method=biallelic.only&amp;snpfirstdim=TRUE since these are required parameters of snpgdsVCF2GDS function. This is an example: https://raw.githubusercontent.com/isglobal-brge/scoreInvHap/master/inst/extdata/example.vcf?method=biallelic.only&amp;snpfirstdim=TRUE In that case we indicate that only biallelic SNPs are considered (‘method=biallelic.only’) and that genotypes are stored in the individual-major mode, (i.e., list all SNPs for the first individual, and then list all SNPs for the second individual, etc) (‘snpfirstdim=TRUE’). "],
["shell-extension.html", "9 Secure shell programs: GWAS with PLINK", " 9 Secure shell programs: GWAS with PLINK GWAS can also be performed using program that are executed using shell commands. This is the case of PLINK, one of the state-of-the-art programs to run GWAS and other genomic data analyses such gene-enviroment interactions or polygenic risc score analyses that requires efficient and scalable pipelines. The resources also allow the use of secure SSH service to run programs on a remote server accessible through ssh containing data and analysis tools where R is just used for launching the analyses and aggregating results. This feature allow us to create functions to analyze data using specific shell programs. Here we describe how PLINK program can be use to perform GWAS [Yannick some overview about how this is created should be described] We use this following code to illustrate how analyses should be performed using the resourcer package. This code could be considered as the base code for creating a DataSHIELD package for the OPAL server as performed in plinkDS() function implemented in the dsOmics package We access the ssh resource called brge_plink (Figure 6.2) using the resourcer package as follows: library(resourcer) brge_plink &lt;- resourcer::newResource(url=&quot;ssh://plink-demo.obiba.org:2222/home/master/brge?exec=ls,plink,plink1&quot;, identity = &quot;master&quot;, secret = &quot;master&quot;) client &lt;- resourcer::newResourceClient(brge_plink) This creates an object of this class: class(client) [1] &quot;SshResourceClient&quot; &quot;CommandResourceClient&quot; &quot;ResourceClient&quot; [4] &quot;R6&quot; These are the actions we can do with an SshResourceClient object names(client) [1] &quot;.__enclos_env__&quot; &quot;clone&quot; &quot;close&quot; [4] &quot;exec&quot; &quot;removeTempDir&quot; &quot;tempDir&quot; [7] &quot;uploadFile&quot; &quot;downloadFile&quot; &quot;getConnection&quot; [10] &quot;getAllowedCommands&quot; &quot;initialize&quot; &quot;asTbl&quot; [13] &quot;asDataFrame&quot; &quot;getResource&quot; For this specific resource (e.g. PLINK) we can execute these shell commands client$getAllowedCommands() [1] &quot;ls&quot; &quot;plink&quot; &quot;plink1&quot; For instance client$exec(&quot;ls&quot;, &quot;-la&quot;) $status [1] 0 $output [1] &quot;total 92992&quot; [2] &quot;dr-xr-xr-x 2 master master 4096 Apr 29 09:50 .&quot; [3] &quot;drwxr-xr-x 6 master master 4096 Jun 9 08:30 ..&quot; [4] &quot;-r--r--r-- 1 master master 57800003 Apr 29 09:50 brge.bed&quot; [5] &quot;-r--r--r-- 1 master master 2781294 Apr 29 09:50 brge.bim&quot; [6] &quot;-r--r--r-- 1 master master 45771 Apr 29 09:50 brge.fam&quot; [7] &quot;-r--r--r-- 1 master master 34442346 Apr 29 09:50 brge.gds&quot; [8] &quot;-r--r--r-- 1 master master 59802 Apr 29 09:50 brge.phe&quot; [9] &quot;-r--r--r-- 1 master master 72106 Apr 29 09:50 brge.txt&quot; $error character(0) $command [1] &quot;cd /home/master/brge &amp;&amp; ls -la&quot; attr(,&quot;class&quot;) [1] &quot;resource.exec&quot; Then, to avoid multiple accesses to the resource, it is recommended to create a temporal directory to save our results tempDir &lt;- client$tempDir() tempDir [1] &quot;/tmp/ssh-1851&quot; client$exec(&quot;ls&quot;, tempDir) $status [1] 0 $output character(0) $error character(0) $command [1] &quot;cd /home/master/brge &amp;&amp; ls /tmp/ssh-1851&quot; attr(,&quot;class&quot;) [1] &quot;resource.exec&quot; Then, we can use R to launch the shell commands client$exec(&#39;plink1&#39;, c(&#39;--bfile&#39;, &#39;brge&#39;, &#39;--freq&#39;, &#39;--out&#39;, paste0(tempDir, &#39;/out&#39;), &#39;--noweb&#39;)) $status [1] 0 $output [1] &quot;&quot; [2] &quot;@----------------------------------------------------------@&quot; [3] &quot;| PLINK! | v1.07 | 10/Aug/2009 |&quot; [4] &quot;|----------------------------------------------------------|&quot; [5] &quot;| (C) 2009 Shaun Purcell, GNU General Public License, v2 |&quot; [6] &quot;|----------------------------------------------------------|&quot; [7] &quot;| For documentation, citation &amp; bug-report instructions: |&quot; [8] &quot;| http://pngu.mgh.harvard.edu/purcell/plink/ |&quot; [9] &quot;@----------------------------------------------------------@&quot; [10] &quot;&quot; [11] &quot;Skipping web check... [ --noweb ] &quot; [12] &quot;Writing this text to log file [ /tmp/ssh-1851/out.log ]&quot; [13] &quot;Analysis started: Wed Jun 10 07:15:04 2020&quot; [14] &quot;&quot; [15] &quot;Options in effect:&quot; [16] &quot;\\t--bfile brge&quot; [17] &quot;\\t--freq&quot; [18] &quot;\\t--out /tmp/ssh-1851/out&quot; [19] &quot;\\t--noweb&quot; [20] &quot;&quot; [21] &quot;Reading map (extended format) from [ brge.bim ] &quot; [22] &quot;100000 markers to be included from [ brge.bim ]&quot; [23] &quot;Reading pedigree information from [ brge.fam ] &quot; [24] &quot;2312 individuals read from [ brge.fam ] &quot; [25] &quot;2312 individuals with nonmissing phenotypes&quot; [26] &quot;Assuming a disease phenotype (1=unaff, 2=aff, 0=miss)&quot; [27] &quot;Missing phenotype value is also -9&quot; [28] &quot;725 cases, 1587 controls and 0 missing&quot; [29] &quot;1097 males, 1215 females, and 0 of unspecified sex&quot; [30] &quot;Reading genotype bitfile from [ brge.bed ] &quot; [31] &quot;Detected that binary PED file is v1.00 SNP-major mode&quot; [32] &quot;Before frequency and genotyping pruning, there are 100000 SNPs&quot; [33] &quot;2312 founders and 0 non-founders found&quot; [34] &quot;6009 heterozygous haploid genotypes; set to missing&quot; [35] &quot;Writing list of heterozygous haploid genotypes to [ /tmp/ssh-1851/out.hh ]&quot; [36] &quot;7 SNPs with no founder genotypes observed&quot; [37] &quot;Warning, MAF set to 0 for these SNPs (see --nonfounders)&quot; [38] &quot;Writing list of these SNPs to [ /tmp/ssh-1851/out.nof ]&quot; [39] &quot;Writing allele frequencies (founders-only) to [ /tmp/ssh-1851/out.frq ] &quot; [40] &quot;&quot; [41] &quot;Analysis finished: Wed Jun 10 07:15:08 2020&quot; [42] &quot;&quot; $error character(0) $command [1] &quot;cd /home/master/brge &amp;&amp; plink1 --bfile brge --freq --out /tmp/ssh-1851/out --noweb&quot; attr(,&quot;class&quot;) [1] &quot;resource.exec&quot; The results can be retrieve as an R object outs &lt;- client$exec(&#39;ls&#39;, tempDir)$output outs [1] &quot;out.frq&quot; &quot;out.hh&quot; &quot;out.log&quot; &quot;out.nof&quot; client$downloadFile(paste0(tempDir, &#39;/out.frq&#39;)) NULL ans &lt;- readr::read_table(&quot;out.frq&quot;) ans # A tibble: 100,000 x 6 CHR SNP A1 A2 MAF NCHROBS &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 MitoC3993T T C 0.0149 4572 2 0 MitoG4821A A G 0.00175 4564 3 0 MitoG6027A A G 0.00434 4614 4 0 MitoT6153C C T 0.0106 4616 5 0 MitoC7275T T C 0.000866 4618 6 0 MitoT9699C C T 0.0732 4604 7 0 MitoA10045G G A 0.00780 4616 8 0 MitoG10311A A G 0.00261 4604 9 0 MitoA11252G G A 0.182 4518 10 0 MitoT11900C C T 0.000868 4608 # … with 99,990 more rows Finally temporal directories are removed and session closed client$removeTempDir() client$close() "],
["setup.html", "10 Setup 10.1 Providing DataSHIELD packages in the opal server 10.2 Required R Packages in the client site (e.g. local machine)", " 10 Setup As describe in a previous Chapter the resourcer R package allows to deal with the main data sources (using tidyverse, DBI, dplyr, sparklyr, MongoDB, AWS S3, SSH etc.) and is easily extensible to new ones including specific data infrastructure in R or Bioconductor. So far ExpressionSet and RangedSummarizedExperiment objects saved in .rdata files are accesible through the resourcer package. The dsOmics package contains a new extension that deals with VCF (Variant Calling Format) files which are coerced to a GDS (Genomic Data Storage) format (VCF2GDS). In order to achive this resourcer extension, two R6 classes have been implemented: GDSFileResourceResolver class which handles file-base resources with data in GDS or VCF formats. This class is responsible for creating a GDSFileResourceClient object instance from an assigned resource. GDSFileResourceClient class which is responsible for getting the referenced file and making a connection (created by GWASTools) to the GDS file (will also convert the VCF file to a GDS file on the fly, using SNPRelate). For the subsequent analysis, it’s this connection handle to the GDS file that will be used. 10.1 Providing DataSHIELD packages in the opal server Required DataSHIELD packages must be uploaded in the opal server through the Administration site by accessing to DataSHIELD tab. In our case, both dsBase and dsOmics and resourcer packages must be installed as is illustrated in the figure. Figure 10.1: Installed packages in the test opal server The tab +Add package can be used to install a new package. The figure depicts how dsOmics was intalled into the opal server Figure 10.2: Description how dsOmics package was intalled into the test opal server For reproducing this book the following packages must be installed in the opal server From CRAN: - resourcer From Github: - datashield/dsBase - datashield/dsGeo (tombisho/dsGeo) - isglobal-brge/dsOmics Note that the dsGeo package imports the sp, rgeos and rgdal R packages. rgeos and rgdal in turn require some additional libraries which can be installed as follows (on Ubuntu systems - see the notes of rgeos and rgdal for other operating systems): sudo apt-get update sudo apt-get install libgdal-dev libproj-dev libgeos++dev 10.2 Required R Packages in the client site (e.g. local machine) Using DataSHIELD also requires some R packages to be install from the client site. So far, the following R packages must be installed (in their development version): install.packages(&quot;DSOpal&quot;, dependencies = TRUE) install.packages(&quot;dsBaseClient&quot;, repos = c(&quot;https://cloud.r-project.org&quot;, &quot;https://cran.obiba.org&quot;), dependencies = TRUE) devtools::install_github(&quot;isglobal-brge/dsOmicsClient&quot;, dependencies = TRUE) devtools::install_github(&quot;tombisho/dsGeoClient&quot;, dependencies = TRUE) The package dependencies are then loaded as follows: library(DSOpal) library(dsBaseClient) library(dsOmicsClient) library(dsGeoClient) "],
["basic-statistical-analyses.html", "11 Basic statistical analyses 11.1 Analysis from a single study 11.2 Analysis from a multiple studies", " 11 Basic statistical analyses Let us start by illustrating how to peform simple statistical data analyses using different resources. Here, we will use data from three studies that are avaialbe in our OPAL test repository. The three databases are called CNSIM1, CNSIM2, CNSIM3 that are avaialble as three different resources: mySQL database, SPSS file and CSV file (see Figure 6.2). This example mimics real situations were different hospital or research centers manage their own databases containing harmonized data. Data correspond to three simulated datasets with different number of observations of 11 harmonized variables. They contain synthetic data based on a model derived from the participants of the 1958 Birth Cohort, as part of the obesity methodological development project. This dataset does contain some NA values. The available variables are: Variable Description Type Note LAB_TSC Total Serum Cholesterol numeric mmol/L LAB_TRIG Triglycerides numeric mmol/L LAB_HDL HDL Cholesterol numeric mmol/L LAB_GLUC_ADJUSTED Non-Fasting Glucose numeric mmol/L PM_BMI_CONTINUOUS Body Mass Index (continuous) numeric kg/m2 DIS_CVA History of Stroke factor 0 = Never had stroke; 1 = Has had stroke MEDI_LPD Current Use of Lipid Lowering Medication (from categorical assessment item) factor 0 = Not currently using lipid lowering medication; 1 = Currently using lipid lowering medication DIS_DIAB History of Diabetes factor 0 = Never had diabetes; 1 = Has had diabetes DIS_AMI History of Myocardial Infarction factor 0 = Never had myocardial infarction; 1 = Has had myocardial infarction GENDER Gender factor 0 = Female PM_BMI_CATEGORICAL Body Mass Index (categorical) factor 1 = Less than 25 kg/m2; 2 = 25 to 30 kg/m2; 3 = Over 30 kg/m2 The analyses that are described here, can also be found in the DataSHIELD Tutorial where these resources where uploaded into the Opal server as three tables, a much worse approach since data have to be moved from original repositories. 11.1 Analysis from a single study Let us start by illustrating how to analyze one data set (CNSIM2). library(DSOpal) library(dsBaseClient) # prepare login data and resource to assign builder &lt;- DSI::newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.CNSIM1&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() # login and assign resource conns &lt;- DSI::datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) # coerce ResourceClient objects to a data.frame called &#39;D&#39; datashield.assign.expr(conns, symbol = &quot;D&quot;, expr = quote(as.resource.data.frame(res, strict=TRUE))) Then we can inspect the type of data we have ds.class(&quot;D&quot;) $study1 [1] &quot;data.frame&quot; ds.colnames(&quot;D&quot;) $study1 [1] &quot;id&quot; &quot;LAB_TSC&quot; &quot;LAB_TRIG&quot; [4] &quot;LAB_HDL&quot; &quot;LAB_GLUC_ADJUSTED&quot; &quot;PM_BMI_CONTINUOUS&quot; [7] &quot;DIS_CVA&quot; &quot;MEDI_LPD&quot; &quot;DIS_DIAB&quot; [10] &quot;DIS_AMI&quot; &quot;GENDER&quot; &quot;PM_BMI_CATEGORICAL&quot; Perform some data descriptive analyses ds.table(&quot;D$DIS_DIAB&quot;) Data in all studies were valid Study 1 : No errors reported from this study $output.list $output.list$TABLE_rvar.by.study_row.props study D$DIS_DIAB 1 0 1 1 1 $output.list$TABLE_rvar.by.study_col.props study D$DIS_DIAB 1 0 0.98613037 1 0.01386963 $output.list$TABLE_rvar.by.study_counts study D$DIS_DIAB 1 0 2133 1 30 $output.list$TABLES.COMBINED_all.sources_proportions D$DIS_DIAB 0 1 0.9860 0.0139 $output.list$TABLES.COMBINED_all.sources_counts D$DIS_DIAB 0 1 2133 30 $validity.message [1] &quot;Data in all studies were valid&quot; ds.table(&quot;D$DIS_DIAB&quot;, &quot;D$GENDER&quot;) Data in all studies were valid Study 1 : No errors reported from this study $output.list $output.list$TABLE.STUDY.1_row.props D$GENDER D$DIS_DIAB 0 1 0 0.502 0.498 1 0.700 0.300 $output.list$TABLE.STUDY.1_col.props D$GENDER D$DIS_DIAB 0 1 0 0.9810 0.9920 1 0.0192 0.0084 $output.list$TABLES.COMBINED_all.sources_row.props D$GENDER D$DIS_DIAB 0 1 0 0.502 0.498 1 0.700 0.300 $output.list$TABLES.COMBINED_all.sources_col.props D$GENDER D$DIS_DIAB 0 1 0 0.9810 0.9920 1 0.0192 0.0084 $output.list$TABLE_STUDY.1_counts D$GENDER D$DIS_DIAB 0 1 0 1071 1062 1 21 9 $output.list$TABLES.COMBINED_all.sources_counts D$GENDER D$DIS_DIAB 0 1 0 1071 1062 1 21 9 $validity.message [1] &quot;Data in all studies were valid&quot; Or even some statistical modelling. In this case we want to assess whether sex (GENDER) or trigrycerids (LAB_TRIG) are risk factors for diabetes (DIS_DIAB) mod &lt;- ds.glm(DIS_DIAB ~ LAB_TRIG + GENDER, data = &quot;D&quot; , family=&quot;binomial&quot;) mod$coeff Estimate Std. Error z-value p-value low0.95CI.LP (Intercept) -5.1696619 0.4549328 -11.363572 6.349427e-30 -6.0613138 LAB_TRIG 0.3813891 0.1037611 3.675647 2.372471e-04 0.1780211 GENDER -0.2260851 0.4375864 -0.516664 6.053908e-01 -1.0837387 high0.95CI.LP P_OR low0.95CI.P_OR high0.95CI.P_OR (Intercept) -4.2780099 0.005654338 0.002325913 0.01368049 LAB_TRIG 0.5847570 1.464317247 1.194850574 1.79455494 GENDER 0.6315685 0.797650197 0.338328242 1.88055787 As usual the connection must be closed datashield.logout(conns) 11.2 Analysis from a multiple studies Now, let us illustrate …. library(DSOpal) library(dsBaseClient) # prepare login data and resources to assign builder &lt;- DSI::newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.CNSIM1&quot;, driver = &quot;OpalDriver&quot;) builder$append(server = &quot;study2&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.CNSIM2&quot;, driver = &quot;OpalDriver&quot;) builder$append(server = &quot;study3&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.CNSIM3&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() # login and assign resources conns &lt;- datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) # assigned objects are of class ResourceClient (and others) ds.class(&quot;res&quot;) $study1 [1] &quot;SQLResourceClient&quot; &quot;ResourceClient&quot; &quot;R6&quot; $study2 [1] &quot;TidyFileResourceClient&quot; &quot;FileResourceClient&quot; &quot;ResourceClient&quot; [4] &quot;R6&quot; $study3 [1] &quot;TidyFileResourceClient&quot; &quot;FileResourceClient&quot; &quot;ResourceClient&quot; [4] &quot;R6&quot; # coerce ResourceClient objects to data.frames # (DataSHIELD config allows as.resource.data.frame() assignment function for the purpose of the demo) datashield.assign.expr(conns, symbol = &quot;D&quot;, expr = quote(as.resource.data.frame(res, strict = TRUE))) ds.class(&quot;D&quot;) $study1 [1] &quot;data.frame&quot; $study2 [1] &quot;data.frame&quot; $study3 [1] &quot;data.frame&quot; # do usual dsBase analysis ds.summary(&#39;D$LAB_HDL&#39;) $study1 $study1$class [1] &quot;numeric&quot; $study1$length [1] 2163 $study1$`quantiles &amp; mean` 5% 10% 25% 50% 75% 90% 95% Mean 0.875240 1.047400 1.300000 1.581000 1.844500 2.090000 2.210900 1.569416 $study2 $study2$class [1] &quot;numeric&quot; $study2$length [1] 3088 $study2$`quantiles &amp; mean` 5% 10% 25% 50% 75% 90% 95% Mean 0.850280 1.032200 1.294000 1.563000 1.840000 2.077000 2.225000 1.556648 $study3 $study3$class [1] &quot;numeric&quot; $study3$length [1] 4128 $study3$`quantiles &amp; mean` 5% 10% 25% 50% 75% 90% 95% Mean 0.876760 1.039200 1.304000 1.589000 1.856000 2.098800 2.244200 1.574687 # vector types are not necessarily the same depending on the data reader that was used ds.class(&#39;D$GENDER&#39;) $study1 [1] &quot;integer&quot; $study2 [1] &quot;haven_labelled&quot; &quot;vctrs_vctr&quot; &quot;double&quot; $study3 [1] &quot;numeric&quot; ds.asFactor(&#39;D$GENDER&#39;, &#39;GENDER&#39;) $all.unique.levels [1] &quot;0&quot; &quot;1&quot; $return.message [1] &quot;Data object &lt;GENDER&gt; correctly created in all specified data sources&quot; ds.summary(&#39;GENDER&#39;) $study1 $study1$class [1] &quot;factor&quot; $study1$length [1] 2163 $study1$categories [1] &quot;0&quot; &quot;1&quot; $study1$`count of &#39;0&#39;` [1] 1092 $study1$`count of &#39;1&#39;` [1] 1071 $study2 $study2$class [1] &quot;factor&quot; $study2$length [1] 3088 $study2$categories [1] &quot;0&quot; &quot;1&quot; $study2$`count of &#39;0&#39;` [1] 1585 $study2$`count of &#39;1&#39;` [1] 1503 $study3 $study3$class [1] &quot;factor&quot; $study3$length [1] 4128 $study3$categories [1] &quot;0&quot; &quot;1&quot; $study3$`count of &#39;0&#39;` [1] 2091 $study3$`count of &#39;1&#39;` [1] 2037 mod &lt;- ds.glm(&quot;DIS_DIAB ~ LAB_TRIG + GENDER&quot;, data = &quot;D&quot; , family=&quot;binomial&quot;) mod$coeff Estimate Std. Error z-value p-value low0.95CI.LP (Intercept) -4.7792110 0.21081170 -22.670521 8.755236e-114 -5.1923944 LAB_TRIG 0.3035931 0.05487436 5.532514 3.156737e-08 0.1960414 GENDER -0.4455989 0.20797931 -2.142516 3.215202e-02 -0.8532309 high0.95CI.LP P_OR low0.95CI.P_OR high0.95CI.P_OR (Intercept) -4.36602770 0.00833261 0.005527953 0.01254229 LAB_TRIG 0.41114488 1.35471774 1.216577226 1.50854390 GENDER -0.03796695 0.64044060 0.426036242 0.96274475 datashield.logout(conns) "],
["Omic.html", "12 Omic data analysis 12.1 Types of analyses implemented 12.2 Differential gene expression (DGE) analysis 12.3 Epigenome-wide association analysis (EWAS) 12.4 GWAS with Bioconductor 12.5 GWAS with PLINK", " 12 Omic data analysis In this part we will provide some real data anlyses of omic data including transcriptomic, epigenomic and genomic data that covers how to perform three of the widely used data analyses: differential gene expression (DGE), epigenome-wide association (EWAS) and genome-wide association (GWAS) analyses. We provide examples of how to perform data analyses using Bioconductor packages. For genomic data we also illustrate how to carry out analyses using PLINK. 12.1 Types of analyses implemented The Figure 12.1 describes the different types of omic association analyses that can be performed using DataSHIELD client functions implemented in the dsOmicsClient package. Basically, data (omic and phenotypes/covariates) can be stored in different sites (http, ssh, AWS S3, local, …) and are managed with Opal through the resourcer package and their extensions implemented in dsOmics. Figure 12.1: Non-disclosive omic data analysis with DataSHIELD and Bioconductor. The figure illustrates how the resourcer package is used to get access to omic data through the Opal servers. Then DataSHIELD is used in the client side to perform non-disclosive data analyses. Then, dsOmicsClient package allows different types of analyses: pooled and meta-analysis. Both methods are based on fitting different generalized linear models (GLMs) for each feature when assesing association between omic data and the phenotype/trait/condition of interest. Of course non-disclosive omic data analysis from a single study can also be performed. The pooled approach (Figure 12.2) is recommended when the user wants to analyze omic data from different sources and obtain results as if the data were located in a single computer. It should be noticed that this can be very time consuming when analyzing multiple features since it calls repeatedly to a base function in DataSHIELD (ds.glm) and that it cannot be recommended when data are not properly harmonized (e.g. gene expression normalized using different methods, GWAS data having different platforms, …). Also when it is necesary to remove unwanted variability (for transcriptomic and epigenomica analysis) or control for population stratification (for GWAS analysis), this approach cannot be used since we need to develop methods to compute surrogate variables (to remove unwanted variability) or PCAs (to to address population stratification) in a non-disclosive way. The meta-analysis approach Figure 12.3 overcomes the limitations raised when performing pooled analyses. First, the computation issue is addressed by using scalable and fast methods to perform data analysis at whole-genome level at each server. The transcriptomic and epigenomic data analyses make use of the widely used limma package that uses ExpressionSet or RangedSummarizedExperiment Bioc infrastructures to deal with omic and phenotypic (e.g covariates). The genomic data are analyzed using GWASTools and GENESIS that are designed to perform quality control (QC) and GWAS using GDS infrastructure. Next, we describe how both approaches are implemented: Pooled approach: Figure 12.2 illustrate how this analysis is performed. This corresponds to generalized linear models (glm) on data from single or multiple sources. It makes use of ds.glm() function which is a DataSHIELD function that uses an approach that is mathematically equivalent to placing all individual-level data froma all sources in one central warehouse and analysing those data using the conventional glm() function in R. The user can select one (or multiple) features (i.e., genes, transcripts, CpGs, SNPs, …) Figure 12.2: Non-disclosive omic data analysis with DataSHIELD and Bioconductor. The figure illustrates how to perform single pooled omic data analysis. The analyses are performed by using a generalized linear model (glm) on data from one or multiple sources. It makes use of ds.glm(), a DataSHIELD function, that uses an approach that is mathematically equivalent to placing all individual-level data from all sources in one central warehouse and analysing those data using the conventional glm() function in R. Meta-analysis: Figure 12.3 illustrate how this analysis is performed. This corresponds to perform a genome-wide analysis at each server using functions that are specifically design to that purpose and that are scalable. Then the results of each server can be meta-analyzed using method that meta-analyze either effects or p-values. Figure 12.3: Non-disclosive omic data analysis with DataSHIELD and Bioconductor. The figure illustrates how to perform anlyses at genome-wide level from one or multiple sources. It runs standard Bioconductor functions at each server independently to speed up the analyses and in the case of having multiple sources, results can be meta-analyzed uning standar R functions. 12.2 Differential gene expression (DGE) analysis Let us start by illustrating a simple example where a researcher may be interested in perfoming differential gene expression anaylis (DGE) having data in a single repository (e.g. one study). To this end, we will use bulk transcriptomic data from TCGA project. We have uploaded to the opal server a resource called tcga_liver whose URL is http://duffel.rail.bio/recount/TCGA/rse_gene_liver.Rdata which is available through the recount project. This resource contains the RangeSummarizedExperiment with the RNAseq profiling of liver cancer data from TCGA. Next, we illustrate how a differential expression analysis to compare RNAseq profiling of women vs men (variable gdc_cases.demographic.gender). The DGE analysis is normally performed using limma package. In that case, as we are analyzing RNA-seq data, limma + voom method will be required. Let us start by creating the connection to the opal server: builder &lt;- newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.tcga_liver&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() conns &lt;- datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) Then, let us coerce the resource to a RangedSummarizedExperiment which is the type of object that is available in the recount project. datashield.assign.expr(conns, symbol = &quot;rse&quot;, expr = quote(as.resource.object(res))) ds.class(&quot;rse&quot;) $study1 [1] &quot;RangedSummarizedExperiment&quot; attr(,&quot;package&quot;) [1] &quot;SummarizedExperiment&quot; The number of features and samples can be inspected by ds.dim(&quot;rse&quot;) $`dimensions of rse in study1` [1] 58037 424 $`dimensions of rse in combined studies` [1] 58037 424 And the names of the features using the same function used in the case of analyzing an ExpressionSet name.features &lt;- ds.featureNames(&quot;rse&quot;) lapply(name.features, head) $study1 [1] &quot;ENSG00000000003.14&quot; &quot;ENSG00000000005.5&quot; &quot;ENSG00000000419.12&quot; [4] &quot;ENSG00000000457.13&quot; &quot;ENSG00000000460.16&quot; &quot;ENSG00000000938.12&quot; Also the covariate names can be inspected by name.vars &lt;- ds.featureData(&quot;rse&quot;) lapply(name.vars, head, n=15) $study1 [1] &quot;project&quot; [2] &quot;sample&quot; [3] &quot;experiment&quot; [4] &quot;run&quot; [5] &quot;read_count_as_reported_by_sra&quot; [6] &quot;reads_downloaded&quot; [7] &quot;proportion_of_reads_reported_by_sra_downloaded&quot; [8] &quot;paired_end&quot; [9] &quot;sra_misreported_paired_end&quot; [10] &quot;mapped_read_count&quot; [11] &quot;auc&quot; [12] &quot;sharq_beta_tissue&quot; [13] &quot;sharq_beta_cell_type&quot; [14] &quot;biosample_submission_date&quot; [15] &quot;biosample_publication_date&quot; We can visualize the levels of the variable having gender information ds.table(&quot;rse$gdc_cases.demographic.gender&quot;) Data in all studies were valid Study 1 : No errors reported from this study $output.list $output.list$TABLE_rvar.by.study_row.props study rse$gdc_cases.demographic.gender 1 female 1 male 1 $output.list$TABLE_rvar.by.study_col.props study rse$gdc_cases.demographic.gender 1 female 0.3372642 male 0.6627358 $output.list$TABLE_rvar.by.study_counts study rse$gdc_cases.demographic.gender 1 female 143 male 281 $output.list$TABLES.COMBINED_all.sources_proportions rse$gdc_cases.demographic.gender female male 0.337 0.663 $output.list$TABLES.COMBINED_all.sources_counts rse$gdc_cases.demographic.gender female male 143 281 $validity.message [1] &quot;Data in all studies were valid&quot; The differential expression analysis is then performed by: ans.gender &lt;- ds.limma(model = ~ gdc_cases.demographic.gender, Set = &quot;rse&quot;, type.data = &quot;RNAseq&quot;, sva = FALSE) Notice that we have set type.data='RNAseq' to consider that our data are counts obtained from a RNA-seq experiment. By indicating so, the differential analysis is performed by using voom + limma as previously mention. As usual, we close the DataSHIELD session by: datashield.logout(conns) 12.3 Epigenome-wide association analysis (EWAS) EWAS requires basically the same statistical methods as those used in DGE. It should be notice that the pooled analysis we are going to illustrate here can also be performed with transcriptomic data since each study must have different range values. If so, gene expression harmonization should be performed, for instance, by standardizing the data at each study. For EWAS where methylation is measured using beta values (e.g CpG data are in the range 0-1) this is not a problem. In any case, adopting the meta-analysis approach could be a safe option. We have downloaded data from GEO corresponding to the accesion number GSE66351 which includes DNA methylation profiling (Illumina 450K array) of 190 individuals. Data corresponds to CpGs beta values measured in the superior temporal gyrus and prefrontal cortex brain regions of patients with Alzheimer’s. Data have been downloaded using GEOquery package that gets GEO data as ExpressionSet objects. Researchers who are not familiar with ExpressionSets can read this Section. Notice that data are encoded as beta-values that ensure data harmonization across studies. In order to illustrate how to perform data analyses using federated data, we have split the data into two ExpressionSets having 100 and 90 samples as if they were two different studies. Figure 6.2 shows the two resources defined for both studies (GSE66351_1 and GSE66351_2) In order to perform omic data analyses, we need first to login and assign resources to DataSHIELD. This can be performed using the as.resource.object() function builder &lt;- DSI::newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.GSE66351_1&quot;, driver = &quot;OpalDriver&quot;) builder$append(server = &quot;study2&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.GSE66351_2&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() conns &lt;- DSI::datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) # Assign to the original R class (e.g ExpressionSet) datashield.assign.expr(conns, symbol = &quot;methy&quot;, expr = quote(as.resource.object(res))) Now, we can see that the resources are actually loaded into the R servers as their original class ds.class(&quot;methy&quot;) $study1 [1] &quot;ExpressionSet&quot; attr(,&quot;package&quot;) [1] &quot;Biobase&quot; $study2 [1] &quot;ExpressionSet&quot; attr(,&quot;package&quot;) [1] &quot;Biobase&quot; Then, some Bioconductor-type functions can be use to return non-disclosive information of ExpressionSets from each server to the client, using similar functions as those defined in the dsBaseClient package. For example, feature names can be returned by fn &lt;- ds.featureNames(&quot;methy&quot;) lapply(fn, head) $study1 [1] &quot;cg00000029&quot; &quot;cg00000108&quot; &quot;cg00000109&quot; &quot;cg00000165&quot; &quot;cg00000236&quot; [6] &quot;cg00000289&quot; $study2 [1] &quot;cg00000029&quot; &quot;cg00000108&quot; &quot;cg00000109&quot; &quot;cg00000165&quot; &quot;cg00000236&quot; [6] &quot;cg00000289&quot; Experimental phenotypes variables can be obtained by ds.varLabels(&quot;methy&quot;) $study1 [1] &quot;title&quot; &quot;geo_accession&quot; [3] &quot;status&quot; &quot;submission_date&quot; [5] &quot;last_update_date&quot; &quot;type&quot; [7] &quot;channel_count&quot; &quot;source_name_ch1&quot; [9] &quot;organism_ch1&quot; &quot;characteristics_ch1&quot; [11] &quot;characteristics_ch1.1&quot; &quot;characteristics_ch1.2&quot; [13] &quot;characteristics_ch1.3&quot; &quot;characteristics_ch1.4&quot; [15] &quot;characteristics_ch1.5&quot; &quot;characteristics_ch1.6&quot; [17] &quot;characteristics_ch1.7&quot; &quot;characteristics_ch1.8&quot; [19] &quot;molecule_ch1&quot; &quot;extract_protocol_ch1&quot; [21] &quot;label_ch1&quot; &quot;label_protocol_ch1&quot; [23] &quot;taxid_ch1&quot; &quot;hyb_protocol&quot; [25] &quot;scan_protocol&quot; &quot;description&quot; [27] &quot;data_processing&quot; &quot;platform_id&quot; [29] &quot;contact_name&quot; &quot;contact_email&quot; [31] &quot;contact_phone&quot; &quot;contact_laboratory&quot; [33] &quot;contact_institute&quot; &quot;contact_address&quot; [35] &quot;contact_city&quot; &quot;contact_zip/postal_code&quot; [37] &quot;contact_country&quot; &quot;supplementary_file&quot; [39] &quot;supplementary_file.1&quot; &quot;data_row_count&quot; [41] &quot;age&quot; &quot;braak_stage&quot; [43] &quot;brain_region&quot; &quot;cell type&quot; [45] &quot;diagnosis&quot; &quot;donor_id&quot; [47] &quot;sentrix_id&quot; &quot;sentrix_position&quot; [49] &quot;Sex&quot; $study2 [1] &quot;title&quot; &quot;geo_accession&quot; [3] &quot;status&quot; &quot;submission_date&quot; [5] &quot;last_update_date&quot; &quot;type&quot; [7] &quot;channel_count&quot; &quot;source_name_ch1&quot; [9] &quot;organism_ch1&quot; &quot;characteristics_ch1&quot; [11] &quot;characteristics_ch1.1&quot; &quot;characteristics_ch1.2&quot; [13] &quot;characteristics_ch1.3&quot; &quot;characteristics_ch1.4&quot; [15] &quot;characteristics_ch1.5&quot; &quot;characteristics_ch1.6&quot; [17] &quot;characteristics_ch1.7&quot; &quot;characteristics_ch1.8&quot; [19] &quot;molecule_ch1&quot; &quot;extract_protocol_ch1&quot; [21] &quot;label_ch1&quot; &quot;label_protocol_ch1&quot; [23] &quot;taxid_ch1&quot; &quot;hyb_protocol&quot; [25] &quot;scan_protocol&quot; &quot;description&quot; [27] &quot;data_processing&quot; &quot;platform_id&quot; [29] &quot;contact_name&quot; &quot;contact_email&quot; [31] &quot;contact_phone&quot; &quot;contact_laboratory&quot; [33] &quot;contact_institute&quot; &quot;contact_address&quot; [35] &quot;contact_city&quot; &quot;contact_zip/postal_code&quot; [37] &quot;contact_country&quot; &quot;supplementary_file&quot; [39] &quot;supplementary_file.1&quot; &quot;data_row_count&quot; [41] &quot;age&quot; &quot;braak_stage&quot; [43] &quot;brain_region&quot; &quot;cell type&quot; [45] &quot;diagnosis&quot; &quot;donor_id&quot; [47] &quot;sentrix_id&quot; &quot;sentrix_position&quot; [49] &quot;Sex&quot; attr(,&quot;class&quot;) [1] &quot;dsvarLabels&quot; &quot;list&quot; 12.3.1 Single CpG analysis Once the methylation data have been loaded into the opal server, we can perform different type of analyses using functions from the dsOmicsClient package. Let us start by illustrating how to analyze a single CpG from two studies by using an approach that is mathematically equivalent to placing all individual-level. ans &lt;- ds.lmFeature(feature = &quot;cg07363416&quot;, model = ~ diagnosis + Sex, Set = &quot;methy&quot;, datasources = conns) ans Estimate Std. Error p-value cg07363416 0.03459886 0.02504291 0.1670998 attr(,&quot;class&quot;) [1] &quot;dsLmFeature&quot; &quot;matrix&quot; 12.3.2 Multiple CpG analysis The same analysis can be performed for all features (e.g. CpGs) just avoiding the feature argument. This process can be parallelized using mclapply function from the multicore package. ans &lt;- ds.lmFeature(model = ~ diagnosis + Sex, Set = &quot;methy&quot;, datasources = conns, mc.cores = 20) This method corresponds to the pooled analysis approach and can be very time consiming since the function repeatedly calls the DataSHIELD function ds.glm(). We can adopt another strategy that is to run a glm of each feature independently at each study using limma package (which is really fast) and then combine the results (i.e. meta-analysis approach). ans.limma &lt;- ds.limma(model = ~ diagnosis + Sex, Set = &quot;methy&quot;, datasources = conns) Then, we can visualize the top genes at each study (i.e server) by lapply(ans.limma, head) $study1 # A tibble: 6 x 10 id logFC CI.L CI.R AveExpr t P.Value adj.P.Val B SE &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg1313… -0.147 -0.191 -0.103 0.380 -6.62 1.90e-9 0.000466 10.6 0.0122 2 cg2385… -0.0569 -0.0741 -0.0397 0.200 -6.58 2.32e-9 0.000466 10.4 0.00520 3 cg1377… -0.0820 -0.107 -0.0570 0.437 -6.50 3.27e-9 0.000466 10.0 0.0135 4 cg1270… -0.0519 -0.0678 -0.0359 0.145 -6.45 4.25e-9 0.000466 9.76 0.00872 5 cg2472… -0.0452 -0.0593 -0.0312 0.139 -6.39 5.47e-9 0.000466 9.51 0.00775 6 cg0281… -0.125 -0.165 -0.0860 0.247 -6.33 7.31e-9 0.000466 9.23 0.0163 $study2 # A tibble: 6 x 10 id logFC CI.L CI.R AveExpr t P.Value adj.P.Val B SE &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg0404… -0.101 -0.135 -0.0669 0.345 -5.91 6.21e-8 0.0172 7.18 0.0128 2 cg0766… -0.0431 -0.0577 -0.0284 0.776 -5.85 8.22e-8 0.0172 6.90 0.00390 3 cg2709… -0.0688 -0.0924 -0.0452 0.277 -5.79 1.07e-7 0.0172 6.64 0.0147 4 cg0893… -0.0461 -0.0627 -0.0296 0.166 -5.55 2.98e-7 0.0360 5.64 0.00791 5 cg1834… -0.0491 -0.0671 -0.0311 0.157 -5.42 5.07e-7 0.0489 5.12 0.00848 6 cg0218… -0.0199 -0.0272 -0.0125 0.0947 -5.36 6.70e-7 0.0538 4.84 0.0155 The annotation can be added by using the argument annotCols. It should be a vector with the columns of the annotation available in the ExpressionSet or RangedSummarizedExperiment that want to be showed. The columns of the annotation can be obtained by ds.fvarLabels(&quot;methy&quot;) $study1 [1] &quot;ID&quot; &quot;Name&quot; [3] &quot;AddressA_ID&quot; &quot;AlleleA_ProbeSeq&quot; [5] &quot;AddressB_ID&quot; &quot;AlleleB_ProbeSeq&quot; [7] &quot;Infinium_Design_Type&quot; &quot;Next_Base&quot; [9] &quot;Color_Channel&quot; &quot;Forward_Sequence&quot; [11] &quot;Genome_Build&quot; &quot;CHR&quot; [13] &quot;MAPINFO&quot; &quot;SourceSeq&quot; [15] &quot;Chromosome_36&quot; &quot;Coordinate_36&quot; [17] &quot;Strand&quot; &quot;Probe_SNPs&quot; [19] &quot;Probe_SNPs_10&quot; &quot;Random_Loci&quot; [21] &quot;Methyl27_Loci&quot; &quot;UCSC_RefGene_Name&quot; [23] &quot;UCSC_RefGene_Accession&quot; &quot;UCSC_RefGene_Group&quot; [25] &quot;UCSC_CpG_Islands_Name&quot; &quot;Relation_to_UCSC_CpG_Island&quot; [27] &quot;Phantom&quot; &quot;DMR&quot; [29] &quot;Enhancer&quot; &quot;HMM_Island&quot; [31] &quot;Regulatory_Feature_Name&quot; &quot;Regulatory_Feature_Group&quot; [33] &quot;DHS&quot; &quot;RANGE_START&quot; [35] &quot;RANGE_END&quot; &quot;RANGE_GB&quot; [37] &quot;SPOT_ID&quot; $study2 [1] &quot;ID&quot; &quot;Name&quot; [3] &quot;AddressA_ID&quot; &quot;AlleleA_ProbeSeq&quot; [5] &quot;AddressB_ID&quot; &quot;AlleleB_ProbeSeq&quot; [7] &quot;Infinium_Design_Type&quot; &quot;Next_Base&quot; [9] &quot;Color_Channel&quot; &quot;Forward_Sequence&quot; [11] &quot;Genome_Build&quot; &quot;CHR&quot; [13] &quot;MAPINFO&quot; &quot;SourceSeq&quot; [15] &quot;Chromosome_36&quot; &quot;Coordinate_36&quot; [17] &quot;Strand&quot; &quot;Probe_SNPs&quot; [19] &quot;Probe_SNPs_10&quot; &quot;Random_Loci&quot; [21] &quot;Methyl27_Loci&quot; &quot;UCSC_RefGene_Name&quot; [23] &quot;UCSC_RefGene_Accession&quot; &quot;UCSC_RefGene_Group&quot; [25] &quot;UCSC_CpG_Islands_Name&quot; &quot;Relation_to_UCSC_CpG_Island&quot; [27] &quot;Phantom&quot; &quot;DMR&quot; [29] &quot;Enhancer&quot; &quot;HMM_Island&quot; [31] &quot;Regulatory_Feature_Name&quot; &quot;Regulatory_Feature_Group&quot; [33] &quot;DHS&quot; &quot;RANGE_START&quot; [35] &quot;RANGE_END&quot; &quot;RANGE_GB&quot; [37] &quot;SPOT_ID&quot; attr(,&quot;class&quot;) [1] &quot;dsfvarLabels&quot; &quot;list&quot; Then we can run the analysis and obtain the output with the chromosome and gene symbol by: ans.limma.annot &lt;- ds.limma(model = ~ diagnosis + Sex, Set = &quot;methy&quot;, annotCols = c(&quot;CHR&quot;, &quot;UCSC_RefGene_Name&quot;), datasources = conns) lapply(ans.limma.annot, head) $study1 # A tibble: 6 x 12 id CHR UCSC_RefGene_Na… logFC CI.L CI.R AveExpr t P.Value &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg13… 2 &quot;ECEL1P2&quot; -0.147 -0.191 -0.103 0.380 -6.62 1.90e-9 2 cg23… 2 &quot;MTA3&quot; -0.0569 -0.0741 -0.0397 0.200 -6.58 2.32e-9 3 cg13… 17 &quot;&quot; -0.0820 -0.107 -0.0570 0.437 -6.50 3.27e-9 4 cg12… 19 &quot;MEX3D&quot; -0.0519 -0.0678 -0.0359 0.145 -6.45 4.25e-9 5 cg24… 19 &quot;ISOC2;ISOC2;IS… -0.0452 -0.0593 -0.0312 0.139 -6.39 5.47e-9 6 cg02… 2 &quot;ECEL1P2&quot; -0.125 -0.165 -0.0860 0.247 -6.33 7.31e-9 # … with 3 more variables: adj.P.Val &lt;dbl&gt;, B &lt;dbl&gt;, SE &lt;dbl&gt; $study2 # A tibble: 6 x 12 id CHR UCSC_RefGene_Na… logFC CI.L CI.R AveExpr t P.Value &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg04… 11 &quot;CD6&quot; -0.101 -0.135 -0.0669 0.345 -5.91 6.21e-8 2 cg07… 6 &quot;MUC21&quot; -0.0431 -0.0577 -0.0284 0.776 -5.85 8.22e-8 3 cg27… 11 &quot;CD6&quot; -0.0688 -0.0924 -0.0452 0.277 -5.79 1.07e-7 4 cg08… 1 &quot;&quot; -0.0461 -0.0627 -0.0296 0.166 -5.55 2.98e-7 5 cg18… 3 &quot;RARRES1;RARRES… -0.0491 -0.0671 -0.0311 0.157 -5.42 5.07e-7 6 cg02… 8 &quot;&quot; -0.0199 -0.0272 -0.0125 0.0947 -5.36 6.70e-7 # … with 3 more variables: adj.P.Val &lt;dbl&gt;, B &lt;dbl&gt;, SE &lt;dbl&gt; Then, the last step is to meta-analyze the results. Different methods can be used to this end. We have implemented a method that meta-analyze the p-pvalues of each study as follows: ans.meta &lt;- metaPvalues(ans.limma) ans.meta # A tibble: 481,868 x 4 id study1 study2 p.meta &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg13138089 0.00000000190 0.00000763 4.78e-13 2 cg25317941 0.0000000179 0.00000196 1.12e-12 3 cg02812891 0.00000000731 0.00000707 1.63e-12 4 cg12706938 0.00000000425 0.0000161 2.14e-12 5 cg16026647 0.000000101 0.000000797 2.51e-12 6 cg12695465 0.00000000985 0.0000144 4.33e-12 7 cg21171625 0.000000146 0.00000225 9.78e-12 8 cg13772815 0.00000000327 0.000122 1.18e-11 9 cg00228891 0.000000166 0.00000283 1.38e-11 10 cg21488617 0.0000000186 0.0000299 1.62e-11 # … with 481,858 more rows This is a genreal method that can be used … We can verify that the results are pretty similar to those obtained using pooled analyses. Here we compute the association for two of the top-CpGs: res1 &lt;- ds.lmFeature(feature = &quot;cg13138089&quot;, model = ~ diagnosis + Sex, Set = &quot;methy&quot;, datasources = conns) res1 Estimate Std. Error p-value cg13138089 -0.1373348 0.01712405 1.057482e-15 attr(,&quot;class&quot;) [1] &quot;dsLmFeature&quot; &quot;matrix&quot; res2 &lt;- ds.lmFeature(feature = &quot;cg13772815&quot;, model = ~ diagnosis + Sex, Set = &quot;methy&quot;, datasources = conns) res2 Estimate Std. Error p-value cg13772815 -0.06786137 0.009128915 1.056225e-13 attr(,&quot;class&quot;) [1] &quot;dsLmFeature&quot; &quot;matrix&quot; We can create a QQ-plot by using the generic function plot (here not showed). In some cases inflation can be observed, so that, correction for cell-type or surrogate variables must be performed. We describe how we can do that in the next two sections. 12.3.3 Adjusting for Surrogate Variables The vast majority of omic studies require to control for unwanted variability. The surrogate variable analysis (SVA) can address this issue by estimating some hidden covariates that capture differences across individuals due to some artifacts such as batch effects or sample quality sam among others. The method is implemented in SVA package. Performing this type of analysis using the ds.lmFeature function is not allowed since estimating SVA would require to implement a non-disclosive method that computes SVA from the different servers. This will be a future topic of the dsOmicsClient. NOTE that, estimating SVA separately at each server would not be a good idea since the aim of SVA is to capture differences mainly due to experimental issues among ALL individuals. What we can do instead is to use the ds.limma function to perform the analyses adjusted for SVA at each study. ans.sva &lt;- ds.limma(model = ~ diagnosis + Sex, Set = &quot;methy&quot;, sva = TRUE, annotCols = c(&quot;CHR&quot;, &quot;UCSC_RefGene_Name&quot;)) ans.sva $study1 # A tibble: 481,868 x 12 id CHR UCSC_RefGene_Na… logFC CI.L CI.R AveExpr t P.Value &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg10… 19 &quot;GNG7&quot; -0.0547 -0.0721 -0.0373 0.338 -6.25 1.31e-8 2 cg13… 17 &quot;&quot; -0.0569 -0.0757 -0.0381 0.437 -6.00 3.91e-8 3 cg11… 19 &quot;PODNL1;PODNL1;… 0.0334 0.0223 0.0445 0.568 5.97 4.53e-8 4 cg27… 17 &quot;SLC47A2;SLC47A… 0.0274 0.0182 0.0365 0.548 5.95 4.91e-8 5 cg21… 6 &quot;&quot; -0.0453 -0.0609 -0.0297 0.799 -5.78 1.05e-7 6 cg23… 2 &quot;MTA3&quot; -0.0327 -0.0440 -0.0215 0.200 -5.77 1.09e-7 7 cg10… 16 &quot;SALL1;SALL1&quot; 0.0366 0.0240 0.0492 0.137 5.77 1.10e-7 8 cg24… 1 &quot;&quot; 0.0317 0.0208 0.0427 0.821 5.76 1.11e-7 9 cg13… 9 &quot;&quot; -0.0326 -0.0440 -0.0213 0.367 -5.72 1.32e-7 10 cg13… 2 &quot;ECEL1P2&quot; -0.106 -0.144 -0.0686 0.380 -5.61 2.15e-7 # … with 481,858 more rows, and 3 more variables: adj.P.Val &lt;dbl&gt;, B &lt;dbl&gt;, # SE &lt;dbl&gt; $study2 # A tibble: 481,868 x 12 id CHR UCSC_RefGene_Na… logFC CI.L CI.R AveExpr t P.Value &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg16… 12 &quot;LRP1&quot; -0.0418 -0.0548 -0.0289 0.388 -6.42 7.82e-9 2 cg25… 1 &quot;&quot; -0.0664 -0.0874 -0.0453 0.569 -6.27 1.55e-8 3 cg12… 11 &quot;NRXN2;NRXN2&quot; -0.0273 -0.0367 -0.0179 0.728 -5.81 1.13e-7 4 cg07… 3 &quot;&quot; -0.0393 -0.0529 -0.0258 0.160 -5.78 1.24e-7 5 cg07… 6 &quot;MUC21&quot; -0.0427 -0.0579 -0.0276 0.776 -5.61 2.63e-7 6 cg00… 1 &quot;CR1L&quot; -0.0568 -0.0771 -0.0365 0.350 -5.56 3.18e-7 7 cg11… 12 &quot;CNTN1;CNTN1&quot; -0.0443 -0.0601 -0.0284 0.152 -5.56 3.18e-7 8 cg03… 7 &quot;PGAM2;PGAM2&quot; -0.0442 -0.0600 -0.0283 0.211 -5.56 3.26e-7 9 cg07… 1 &quot;KCNAB2;KCNAB2&quot; 0.0611 0.0392 0.0831 0.659 5.54 3.47e-7 10 cg25… 17 &quot;WNK4&quot; -0.0392 -0.0533 -0.0251 0.512 -5.52 3.70e-7 # … with 481,858 more rows, and 3 more variables: adj.P.Val &lt;dbl&gt;, B &lt;dbl&gt;, # SE &lt;dbl&gt; attr(,&quot;class&quot;) [1] &quot;dsLimma&quot; &quot;list&quot; Then, data can be combined meta-anlyzed as follows: ans.meta.sv &lt;- metaPvalues(ans.sva) ans.meta.sv # A tibble: 481,868 x 4 id study1 study2 p.meta &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg00228891 0.00000397 0.000000318 3.58e-11 2 cg01301319 0.00000609 0.00000123 2.00e-10 3 cg22962123 0.00000106 0.00000767 2.17e-10 4 cg24302412 0.0000139 0.000000762 2.77e-10 5 cg02812891 0.000000408 0.0000327 3.47e-10 6 cg23859635 0.000000109 0.000190 5.29e-10 7 cg13138089 0.000000215 0.000105 5.74e-10 8 cg24938077 0.0000125 0.00000254 8.02e-10 9 cg13772815 0.0000000391 0.00132 1.27e- 9 10 cg21212881 0.000000340 0.000248 2.04e- 9 # … with 481,858 more rows The DataSHIELD session must by closed by: datashield.logout(conns) 12.4 GWAS with Bioconductor We have a GWAS example available at BRGE data repository that aims to find SNPs associated with asthma. We have genomic data in a VCF file (brge.vcf) along with several covariates and phenotypes in the file brge.txt (gender, age, obesity, smoking, country and asthma status). The same data is also available in PLINK format (brge.bed, brge.bim, brge.fam) with covariates in the file brge.phe. We have created a resource having the VCF file of our study on asthma as previously described. The name of the resource is brge_vcf the phenotypes are available in another resource called brge that is a .txt file (see 6.2). The GWAS analysis is then perform as follows. We first start by preparing login data builder &lt;- newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.brge_vcf&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() conns &lt;- datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) In this case we have to assign to different resources. One for the VCF (obesity_vcf) and another one for the phenotypic data (obesity). To this end, the datashield.assign.resource function is required before assigning any object to the specific resource. Notice that the VCF resource can be load into R as a GDS thanks to our extension of existing resources in the reourcer datashield.assign.resource(conns, symbol = &quot;vcf.res&quot;, resource = list(study1 = &quot;RSRC.brge_vcf&quot;)) datashield.assign.expr(conns, symbol = &quot;gds&quot;, expr = quote(as.resource.object(vcf.res))) datashield.assign.resource(conns, symbol = &quot;covars.res&quot;, resource = list(study1 = &quot;RSRC.brge&quot;)) datashield.assign.expr(conns, symbol = &quot;covars&quot;, expr = quote(as.resource.data.frame(covars.res))) These are the objects available in the Opal server ds.ls() $study1 $study1$environment.searched [1] &quot;R_GlobalEnv&quot; $study1$objects.found [1] &quot;covars&quot; &quot;covars.res&quot; &quot;gds&quot; &quot;res&quot; &quot;vcf.res&quot; We can use dsBaseClient functions to inspect the variables that are in the covars data.frame. The variables are ds.colnames(&quot;covars&quot;) $study1 [1] &quot;scanID&quot; &quot;gender&quot; &quot;obese&quot; &quot;age&quot; &quot;smoke&quot; &quot;country&quot; &quot;asthma&quot; The asthma variable has this number of individuals at each level (1: controls, 2: cases) ds.table(&quot;covars$asthma&quot;) Data in all studies were valid Study 1 : No errors reported from this study $output.list $output.list$TABLE_rvar.by.study_row.props study covars$asthma 1 0 1 1 1 $output.list$TABLE_rvar.by.study_col.props study covars$asthma 1 0 0.6864187 1 0.3135813 $output.list$TABLE_rvar.by.study_counts study covars$asthma 1 0 1587 1 725 $output.list$TABLES.COMBINED_all.sources_proportions covars$asthma 0 1 0.686 0.314 $output.list$TABLES.COMBINED_all.sources_counts covars$asthma 0 1 1587 725 $validity.message [1] &quot;Data in all studies were valid&quot; Then, an object of class GenotypeData must be created at the server side to perform genetic data analyses. This is a container defined in the GWASTools package for storing genotype and phenotypic data from genetic association studies. By doing that we will also verify whether individuals in the GDS (e.g VCF) and covariates files have the same individuals and are in the same order. This can be performed by ds.GenotypeData(x=&#39;gds&#39;, covars = &#39;covars&#39;, columnId = 1, newobj.name = &#39;gds.Data&#39;) The association analysis for a given SNP is performed by simply ds.glmSNP(snps.fit = &quot;rs11247693&quot;, model = asthma ~ gender + age, genoData=&#39;gds.Data&#39;) Estimate Std. Error p-value rs11247693 -0.1543215 0.2309585 0.5040196 attr(,&quot;class&quot;) [1] &quot;dsGlmSNP&quot; &quot;matrix&quot; The analysis of all available SNPs is performed when the argument snps.fit is missing. The function performs the analysis of the selected SNPs in a single repository or in multiple repositories as performing pooled analyses (it uses ds.glm DataSHIELD function). As in the case of transcriptomic data, analyzing all the SNPs in the genome (e.g GWAS) will be high time-consuming. We can adopt a similar approach as the one adopted using the limma at each server. That is, we run GWAS at each repository using specific and scalable packages available in R/Bioc. In that case we use the GWASTools and GENESIS packages. The complete pipeline is implemented in this function ans.bioC &lt;- ds.GWAS(&#39;gds.Data&#39;, model=asthma~age+country) This close the DataSHIELD session datashield.logout(conns) 12.5 GWAS with PLINK Here we illustrate how to perform the same GWAS analyses on the asthma using PLINK secure shell commands. This can be performed thanks to the posibility of having ssh resources as described here. It is worth to notice that this workflow and the new R functions implemented in dsOmicsClient could be used as a guideline to carry out similar analyses using existing analysis tools in genomics such as IMPUTE, SAMtools or BEDtools among many others. We start by assigning login resources library(DSOpal) library(dsBaseClient) library(dsOmicsClient) builder &lt;- newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.brge_plink&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() Then we assign the resource to a symbol (i.e. R object) called client which is a ssh resource conns &lt;- datashield.login(logins = logindata, assign = TRUE, symbol = &quot;client&quot;) ds.class(&quot;client&quot;) $study1 [1] &quot;SshResourceClient&quot; &quot;CommandResourceClient&quot; &quot;ResourceClient&quot; [4] &quot;R6&quot; Now, we are ready to run any PLINK command from the client site. Notice that in this case we want to assess association between the genotype data in bed format and use as phenotype the variable ‘obese’ that is in the file ‘obesity.phe’. The sentence in a PLINK command would be (NOTE: we avoid –out to indicate the output file since the file will be available in R as a tibble). plink --bfile obesity --assoc --pheno obesity.phe --pheno-name obese The arguments musth be encapsulated in a single character without the command ‘plink’ plink.arguments &lt;- &quot;--bfile brge --logistic --covar brge.phe --covar-name gender,age&quot; the analyses are then performed by ans.plink &lt;- ds.PLINK(&quot;client&quot;, plink.arguments) The object ans contains the PLINK results at each server as well as the outuput provided by PLINK lapply(ans.plink, names) $study1 [1] &quot;results&quot; &quot;plink.out&quot; head(ans.plink$study1$results) # A tibble: 6 x 9 CHR SNP BP A1 TEST NMISS OR STAT P &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 MitoC3993T 3993 T ADD 2286 0.752 -1.33 0.182 2 0 MitoC3993T 3993 T gender 2286 0.742 -3.27 0.00107 3 0 MitoC3993T 3993 T age 2286 1.00 0.565 0.572 4 0 MitoG4821A 4821 A ADD 2282 2.68 1.71 0.0879 5 0 MitoG4821A 4821 A gender 2282 0.740 -3.31 0.000940 6 0 MitoG4821A 4821 A age 2282 1.00 0.465 0.642 ans.plink$study$plink.out $status [1] 0 $output [1] &quot;&quot; [2] &quot;@----------------------------------------------------------@&quot; [3] &quot;| PLINK! | v1.07 | 10/Aug/2009 |&quot; [4] &quot;|----------------------------------------------------------|&quot; [5] &quot;| (C) 2009 Shaun Purcell, GNU General Public License, v2 |&quot; [6] &quot;|----------------------------------------------------------|&quot; [7] &quot;| For documentation, citation &amp; bug-report instructions: |&quot; [8] &quot;| http://pngu.mgh.harvard.edu/purcell/plink/ |&quot; [9] &quot;@----------------------------------------------------------@&quot; [10] &quot;&quot; [11] &quot;Skipping web check... [ --noweb ] &quot; [12] &quot;Writing this text to log file [ /tmp/ssh-7680/out.log ]&quot; [13] &quot;Analysis started: Wed Jun 10 07:26:36 2020&quot; [14] &quot;&quot; [15] &quot;Options in effect:&quot; [16] &quot;\\t--bfile brge&quot; [17] &quot;\\t--logistic&quot; [18] &quot;\\t--covar brge.phe&quot; [19] &quot;\\t--covar-name gender,age&quot; [20] &quot;\\t--noweb&quot; [21] &quot;\\t--out /tmp/ssh-7680/out&quot; [22] &quot;&quot; [23] &quot;Reading map (extended format) from [ brge.bim ] &quot; [24] &quot;100000 markers to be included from [ brge.bim ]&quot; [25] &quot;Reading pedigree information from [ brge.fam ] &quot; [26] &quot;2312 individuals read from [ brge.fam ] &quot; [27] &quot;2312 individuals with nonmissing phenotypes&quot; [28] &quot;Assuming a disease phenotype (1=unaff, 2=aff, 0=miss)&quot; [29] &quot;Missing phenotype value is also -9&quot; [30] &quot;725 cases, 1587 controls and 0 missing&quot; [31] &quot;1097 males, 1215 females, and 0 of unspecified sex&quot; [32] &quot;Reading genotype bitfile from [ brge.bed ] &quot; [33] &quot;Detected that binary PED file is v1.00 SNP-major mode&quot; [34] &quot;Reading 6 covariates from [ brge.phe ] with nonmissing values for 2199 individuals&quot; [35] &quot;Selected subset of 2 from 6 covariates&quot; [36] &quot;For these, nonmissing covariate values for 2312 individuals&quot; [37] &quot;Before frequency and genotyping pruning, there are 100000 SNPs&quot; [38] &quot;2312 founders and 0 non-founders found&quot; [39] &quot;6009 heterozygous haploid genotypes; set to missing&quot; [40] &quot;Writing list of heterozygous haploid genotypes to [ /tmp/ssh-7680/out.hh ]&quot; [41] &quot;7 SNPs with no founder genotypes observed&quot; [42] &quot;Warning, MAF set to 0 for these SNPs (see --nonfounders)&quot; [43] &quot;Writing list of these SNPs to [ /tmp/ssh-7680/out.nof ]&quot; [44] &quot;Total genotyping rate in remaining individuals is 0.994408&quot; [45] &quot;0 SNPs failed missingness test ( GENO &gt; 1 )&quot; [46] &quot;0 SNPs failed frequency test ( MAF &lt; 0 )&quot; [47] &quot;After frequency and genotyping pruning, there are 100000 SNPs&quot; [48] &quot;After filtering, 725 cases, 1587 controls and 0 missing&quot; [49] &quot;After filtering, 1097 males, 1215 females, and 0 of unspecified sex&quot; [50] &quot;Converting data to Individual-major format&quot; [51] &quot;Writing logistic model association results to [ /tmp/ssh-7680/out.assoc.logistic ] &quot; [52] &quot;&quot; [53] &quot;Analysis finished: Wed Jun 10 07:28:33 2020&quot; [54] &quot;&quot; $error character(0) $command [1] &quot;cd /home/master/brge &amp;&amp; plink1 --bfile brge --logistic --covar brge.phe --covar-name gender,age --noweb --out /tmp/ssh-7680/out&quot; attr(,&quot;class&quot;) [1] &quot;resource.exec&quot; We can compare the p-values obtained using PLINK with Bioconductor-based packages for the top-10 SNPs as follows: library(tidyverse) # get SNP p.values (additive model - ADD) res.plink &lt;- ans.plink$study1$results %&gt;% filter(TEST==&quot;ADD&quot;) %&gt;% arrange(P) # compare top-10 with Biocoductor&#39;s results snps &lt;- res.plink$SNP[1:10] plink &lt;- res.plink %&gt;% filter(SNP%in%snps) %&gt;% dplyr::select(SNP, P) bioC &lt;- ans.bioC$study1 %&gt;% filter(rs%in%snps) %&gt;% dplyr::select(rs, Score.pval) left_join(plink, bioC, by=c(&quot;SNP&quot; = &quot;rs&quot;)) # A tibble: 10 x 3 SNP P Score.pval &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 rs2267914 0.00000151 0.000000809 2 rs6097326 0.00000424 0.00000642 3 rs7153 0.00000440 0.00000508 4 rs3732410 0.00000817 0.00000940 5 rs7995146 0.0000170 0.0000195 6 rs6495788 0.0000213 0.0000278 7 rs1602679 0.0000268 0.0000264 8 rs11055608 0.0000270 0.0000161 9 rs7098143 0.0000313 0.0000214 10 rs7676164 0.0000543 0.0000537 As expected, the p-values are in the same order of magnitud having little variations due to the implemented methods of each software. We can do the same comparions of minor allele frequency (MAF) estimation performed with Bioconductor and PLINK. To this end, we need first to estimate MAF using PLINK plink.arguments &lt;- &quot;--bfile brge --freq&quot; ans.plink2 &lt;- ds.PLINK(&quot;client&quot;, plink.arguments) maf.plink &lt;- ans.plink2$study1$results plink &lt;- maf.plink %&gt;% filter(SNP%in%snps) %&gt;% dplyr::select(SNP, MAF) bioC &lt;- ans.bioC$study1 %&gt;% filter(rs%in%snps) %&gt;% dplyr::select(rs, freq) left_join(plink, bioC, by=c(&quot;SNP&quot; = &quot;rs&quot;)) # A tibble: 10 x 3 SNP MAF freq &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 rs7153 0.256 0.256 2 rs3732410 0.254 0.254 3 rs7676164 0.304 0.304 4 rs1602679 0.101 0.101 5 rs7098143 0.210 0.210 6 rs11055608 0.446 0.446 7 rs7995146 0.0527 0.0527 8 rs6495788 0.267 0.267 9 rs2267914 0.104 0.104 10 rs6097326 0.125 0.125 This close the DataSHIELD session datashield.logout(conns) "],
["GIS.html", "13 Geospatial data analysis 13.1 Introducing the analysis 13.2 Setting up the analysis 13.3 Data manipulation 13.4 Generating the final result", " 13 Geospatial data analysis In this section we will provide some realistic data anlyses of geographic data including Geospatial Positionning System (GPS) and geolocation data, which is then combined with phenotypic data. The objectives of the analyses are intended to mimic the real-life usage of data covered in REF BURGOINE ET AL. 13.1 Introducing the analysis In this example we consider GPS traces captured by individuals in some eastern districts of London, publically available from OpenStreetMap. For this example analysis, we will imagine these 819 GPS traces as commutes between individuals’ home and work. If these data were actual commutes, they would be highly sensitive as they identify an individual’s home and work location. Therefore it is less likely that commuting data would be readily accessible for traditional pooled analysis and a federated approach with DataSHIELD could be more practical. We also have real data on the location of 6100 fast food or takeaway outlets in this same area, sourced from the Food Standards Agency. This is shown in the figure below. We manufactured a corresponding data set on individuals’ BMI, age, sex, total household income, highest educational qualification (as proxies for individual level socioeconomic status) and smoking status. The purpose of the analysis is to test the association between exposure to takeaway food on a commute with BMI, using the other variables to adjust for confounding factors. Given that reducing obesity is a public health priority, this type of research could help inform local authority policies towards food outlet location, type and density. We illustrate how the tools available in the dsGeo package allow this question to be addressed. Figure 13.1: GPS traces in eastern London which could be imagined to be commutes between homes and workplaces Figure 13.2: Location of food outlets in eastern London 13.2 Setting up the analysis THE DATA ARE NOT CURRENTLY SPLIT, BUT COULD DO THIS AT A LATER DATE WHEN IT IS ALL WORKING OK In order to perform geospatial data analyses, we need first to login and assign resources to DataSHIELD. This can be performed using the as.resource.object() function. The data have been uploaded to the server and configured as resources. builder &lt;- DSI::newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.gps_data&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() conns &lt;- DSI::datashield.login(logins = logindata, assign = TRUE, symbol = &quot;gps.res&quot;) # Assign additional datasets as resources for the location of the food outlets / takeaways and # the participant phenotype data datashield.assign.resource(conns, symbol=&quot;takeaway.res&quot;, resource=&quot;RSRC.takeaway_gps&quot;) datashield.assign.resource(conns, symbol=&quot;participant.res&quot;, resource=&quot;RSRC.gps_participant&quot;) # Assign to the original R classes (e.g Spatial) datashield.assign.expr(conns, symbol = &quot;takeaway&quot;, expr = quote(as.resource.object(takeaway.res))) datashield.assign.expr(conns, symbol = &quot;journeys&quot;, expr = quote(as.resource.object(gps.res))) datashield.assign.expr(conns, symbol = &quot;participant&quot;, expr = quote(as.resource.data.frame(participant.res))) Now, we can see that the resources are loaded into the R servers as their original class ds.class(&quot;takeaway&quot;) $study1 [1] &quot;SpatialPointsDataFrame&quot; attr(,&quot;package&quot;) [1] &quot;sp&quot; ds.class(&quot;journeys&quot;) $study1 [1] &quot;SpatialLinesDataFrame&quot; attr(,&quot;package&quot;) [1] &quot;sp&quot; ds.class(&quot;participant&quot;) $study1 [1] &quot;data.frame&quot; In the same way that BioConductor provides convenient data structures for ’omics data, the sp package allows each row of data to have a corresponding set of geometries. So if a row of data has information about an individual (age, BMI, etc.) it can also have a set of many points defining geometries such as points, lines and polygons. # Standard ds.summary can be used on the &#39;data&#39; part of the spatial dataframe ds.summary(&#39;journeys$age&#39;) $study1 $study1$class [1] &quot;numeric&quot; $study1$length [1] 819 $study1$`quantiles &amp; mean` 5% 10% 25% 50% 75% 90% 95% Mean 41.23623 42.76502 46.61040 52.53669 58.73395 62.25656 63.72577 52.58792 We have provided an additional function to allow a summary of the geometries to be provided. ds.geoSummary(&#39;journeys&#39;) $study1 $class [1] &quot;SpatialLinesDataFrame&quot; attr(,&quot;package&quot;) [1] &quot;sp&quot; $bbox min max x 538171.2 556078.4 y 179796.4 208063.6 $is.projected [1] TRUE $proj4string [1] &quot;+init=epsg:27700 +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +datum=OSGB36 +units=m +no_defs +ellps=airy +towgs84=446.448,-125.157,542.060,0.1502,0.2470,0.8421,-20.4894&quot; $data id age sex inc fsmoke Min. : 1.0 Min. :40.02 Min. :0.0000 Min. : 2479 current:240 1st Qu.:218.5 1st Qu.:46.61 1st Qu.:0.0000 1st Qu.:40545 former :169 Median :426.0 Median :52.54 Median :0.0000 Median :50949 never :410 Mean :427.6 Mean :52.59 Mean :0.4872 Mean :50395 3rd Qu.:640.5 3rd Qu.:58.73 3rd Qu.:1.0000 3rd Qu.:59912 Max. :849.0 Max. :64.99 Max. :1.0000 Max. :99301 fedu BMI Secondary:156 Min. : 17.01 Higher :393 1st Qu.: 22.42 Advanced :270 Median : 24.34 Mean : 25.13 3rd Qu.: 26.60 Max. :116.72 attr(,&quot;class&quot;) [1] &quot;summary.Spatial&quot; 13.3 Data manipulation In our analysis we need to define how we determine whether an individual has been ‘exposed’ to a food outlet. First we will define a 10m buffer around the location of the food outlet. If an indivudal’s GPS trace falls within that buffered region, we will say that they are ‘exposed’ to that outlet. The food outlets are defined as points, and after the buffer is applied they become polygons. # Add a buffer to each takeaway - now they are polygons ds.gBuffer(input = &#39;takeaway&#39;, ip_width=10, newobj.name = &#39;take.buffer&#39;, by_id=TRUE) ds.geoSummary(&#39;take.buffer&#39;) $study1 $class [1] &quot;SpatialPolygonsDataFrame&quot; attr(,&quot;package&quot;) [1] &quot;sp&quot; $bbox min max x 538336.1 556245.1 y 179810.1 207683.1 $is.projected [1] TRUE $proj4string [1] &quot;+init=epsg:27700 +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +datum=OSGB36 +units=m +no_defs +ellps=airy +towgs84=446.448,-125.157,542.060,0.1502,0.2470,0.8421,-20.4894&quot; $data BusinessName BusinessTypeID Tesco : 43 Min. : 1 Subway : 32 1st Qu.: 1 Costa Coffee : 24 Median :4613 Sainsbury&#39;s : 22 Mean :4210 Londis : 19 3rd Qu.:7843 Domino&#39;s Pizza: 17 Max. :7846 (Other) :5943 attr(,&quot;class&quot;) [1] &quot;summary.Spatial&quot; To simplify the next step, we remove the data part of the SpatialPolygonsDataFrame to leave the polygons #extract the polygons ds.geometry(input_x = &#39;take.buffer&#39;, newobj.name = &#39;take.buffer.strip&#39;) ds.geoSummary(&#39;take.buffer.strip&#39;) $study1 $class [1] &quot;SpatialPolygons&quot; attr(,&quot;package&quot;) [1] &quot;sp&quot; $bbox min max x 538336.1 556245.1 y 179810.1 207683.1 $is.projected [1] TRUE $proj4string [1] &quot;+init=epsg:27700 +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +datum=OSGB36 +units=m +no_defs +ellps=airy +towgs84=446.448,-125.157,542.060,0.1502,0.2470,0.8421,-20.4894&quot; attr(,&quot;class&quot;) [1] &quot;summary.Spatial&quot; Now, we take the intersection of each individual’s commute with each food outlet as shown in the figure below. Figure 13.3: Food outlets with a buffer are shown by a cross surrounded by a circle. The line segments represent the GPS trace. In this case there is an intersection with 2 of the 3 food outlets. This results in a list of individuals, with each list element being a numeric vector containing the ids of the food outlet buffers that were intersected by the GPS trace. These are the food outlets that the individual was ‘exposed’ to. We then convert the numeric vectors to counts by applying the length function to the list. # Do the intersection of journeys with buffered takeaways ds.over(input_x = &#39;journeys&#39;, input_y = &#39;take.buffer.strip&#39;, newobj.name = &#39;my.over&#39;, retList = TRUE) ds.lapply(input = &#39;my.over&#39;,newobj.name = &#39;count.list&#39;, fun = &#39;length&#39;) The result of the lapply needs to be unlisted, and we check the result looks reasonable ds.unList(x.name = &#39;count.list&#39;, newobj=&#39;counts&#39;) $is.object.created [1] &quot;A data object &lt;counts&gt; has been created in all specified data sources&quot; $validity.check [1] &quot;&lt;counts&gt; appears valid in all sources&quot; ds.summary(&#39;counts&#39;) $study1 $study1$class [1] &quot;integer&quot; $study1$length [1] 819 $study1$`quantiles &amp; mean` 5% 10% 25% 50% 75% 90% 95% Mean 0.000000 0.000000 0.000000 0.000000 1.000000 5.000000 9.000000 1.882784 13.4 Generating the final result Finally we complete the association analysis and see that BMI is positively associated with the count of food outlets that an individual is exposed to on commutes (see the ‘counts’ coefficient). This type of analysis could be used to convey a public health message about density and location of takeaway food outlets. ds.dataFrame(x = c(&#39;participant&#39;, &#39;counts&#39;), newobj = &#39;geo.df&#39;) $is.object.created [1] &quot;A data object &lt;geo.df&gt; has been created in all specified data sources&quot; $validity.check [1] &quot;&lt;geo.df&gt; appears valid in all sources&quot; result &lt;- ds.glm(formula = &#39;BMI ~ age + sex + inc + fsmoke + fedu + counts&#39;, data = &#39;geo.df&#39;, family = &#39;gaussian&#39;) result$coefficients Estimate Std. Error z-value p-value low0.95CI (Intercept) 2.379500e+01 6.334955e-01 37.5614341 0.000000e+00 2.255337e+01 age 2.698037e-02 1.023046e-02 2.6372581 8.357921e-03 6.929033e-03 sex -2.960308e+00 1.447462e-01 -20.4517233 5.799002e-93 -3.244006e+00 inc -7.647467e-06 5.202464e-06 -1.4699701 1.415699e-01 -1.784411e-05 fsmokeformer 2.849097e-01 2.070037e-01 1.3763509 1.687130e-01 -1.208101e-01 fsmokenever 3.906555e-01 1.675560e-01 2.3314925 1.972740e-02 6.225183e-02 feduHigher 8.727664e-02 1.951124e-01 0.4473147 6.546479e-01 -2.951367e-01 feduAdvanced -1.417767e-01 2.076311e-01 -0.6828300 4.947143e-01 -5.487262e-01 counts 7.957629e-01 1.137431e-02 69.9614332 0.000000e+00 7.734697e-01 high0.95CI (Intercept) 2.503663e+01 age 4.703171e-02 sex -2.676611e+00 inc 2.549176e-06 fsmokeformer 6.906295e-01 fsmokenever 7.190592e-01 feduHigher 4.696899e-01 feduAdvanced 2.651727e-01 counts 8.180562e-01 "],
["dslite-datashield-implementation-on-local-datasets.html", "14 DSLite: DataSHIELD Implementation on Local Datasets 14.1 Development Environment Setup 14.2 DataSHIELD Development Flow 14.3 DataSHIELD Sessions 14.4 Debugging 14.5 Limitations", " 14 DSLite: DataSHIELD Implementation on Local Datasets DSLite is a serverless DataSHIELD Interface (DSI) implementation which purpose is to mimic the behavior of a distant (virtualized or barebone) data repository server (see DSOpal for instance). The datasets that are being analyzed must be fully accessible in the local environment and then the non-disclosive constraint of the analysis is not relevant for DSLite: some DSLite functionalities allows to inspect what is under the hood of the DataSHIELD computation nodes, making it a perfect tool for DataSHIELD analysis package developers. 14.1 Development Environment Setup 14.1.1 DataSHIELD Packages Both client and server side packages must be installed in your local R session. The entry point is still the client side package and DSLite will automatically load the corresponding server side package on DataSHIELD aggregate and assignment functions call, based on the DataSHIELD configuration. The minimum required packages should be: install.packages(c(&quot;resourcer&quot;, &quot;DSLite&quot;), dependencies = TRUE) install.packages(&quot;dsBase&quot;, repos = c(&quot;https://cloud.r-project.org&quot;, &quot;https://cran.obiba.org&quot;), dependencies = TRUE) 14.1.2 Test Datasets DSLite comes with a set of datasets that can be easily loaded. You can also provide your own to illustrate a specific data analysis function. 14.1.3 R Package Development Tools We recommend using the following tools to facilitate R package development: devtools, the collection of package development tools, usethis, automate package and project setup tasks that are otherwise performed manually, testthat, for unit testing, roxygen2, for writing documentation in-line with code, Rstudio, the R editor that integrates the tools mentioned above and more. 14.2 DataSHIELD Development Flow The typical development flow, using DSLite, is: Build and install your client and/or server side DataSHIELD packages. Create a new DSLiteServer object instance, refering test datasets. Use or alter the default DataSHIELD configuration. Test your DataSHIELD client/server functions. Debug DataSHIELD server nodes using DSLiteServer methods. 14.2.1 DSLiteServer After your client and/or server side DataSHIELD packages have been built and installed, a new DSLiteServer object instance must be created. Some DSLiteServer methods can be used to verify or modify the DSLiteServer behaviour: DSLiteServer$strict() DSLiteServer$home() See the R documentation of the DSLiteServer class for details. As an example: library(DSLite) # prepare test data in a light DS server data(&quot;CNSIM1&quot;) data(&quot;CNSIM2&quot;) data(&quot;CNSIM3&quot;) dslite.server &lt;- newDSLiteServer(tables=list(CNSIM1=CNSIM1, CNSIM2=CNSIM2, CNSIM3=CNSIM3)) # load corresponding DataSHIELD login data data(&quot;logindata.dslite.cnsim&quot;) The previous example can be simplified using the set-up functions based on the provided test datasets: library(DSLite) # load CNSIM test data logindata.dslite.cnsim &lt;- setupCNSIMTest() 14.2.2 DataSHIELD Configuration The DataSHIELD configuration (aggregate and assign functions, R options) is automatically discovered by inspecting the R packages installed and having some DataSHIELD settings defined, either in their DESCRIPTION file or in a DATASHIELD file. This default configuration extracting function is: DSLite::defaultDSConfiguration() The list of the DataSHIELD R packages to be inspected (or excluded) when building the default configuration can be specified as parameters of defaultDSConfiguration(). The DataSHIELD configuration can be specified at DSLiteServer creation time or afterwards with some DSLiteServer methods that can be used to verify or modify the DSLiteServer configuration: DSLiteServer$config() DSLiteServer$aggregateMethods() DSLiteServer$aggregateMethod() DSLiteServer$assignMethods() DSLiteServer$assignMethod() DSLiteServer$options() DSLiteServer$option() See the R documentation of the DSLiteServer class for details. As an example: # verify configuration dslite.server$config() 14.3 DataSHIELD Sessions The following figure illustrates a setup where a single DSLiteServer holds several data frames and is used by two different DataSHIELD Connection (DSConnection) objects. All these objects live in the same R environment (usually the Global Environment). The “server” is responsible for managing DataSHIELD sessions that are implemented as distinct R environments inside of which R symbols are assigned and R functions are evaluated. Using the R environment paradigm ensures that the different DataSHIELD execution context (client and servers) are contained and exclusive from each other. DSLite architecture After performing the login DataSHIELD phase, the DSLiteServer holds the different DataSHIELD server side sessions, i.e. R environments identified by an ID. These IDs are also stored within the DataSHIELD connection objects that are the result of the datashield.login() call. The folllowing example shows how to access these session IDs: # datashield logins and assignments conns &lt;- datashield.login(logindata.dslite.cnsim, assign=TRUE) # get the session ID of &quot;sim1&quot; node connection object conns$sim1@sid # the same ID is in the DSLiteServer dslite.server$hasSession(conns$sim1@sid) 14.4 Debugging Thanks to the DSLiteServer capability to have its configuration modified at any time, it is possible to add some debugging functions without polluting in the DataSHIELD package you are developping. For instance, this code adds an aggregate function print(): # add a print method to configuration dslite.server$aggregateMethod(&quot;print&quot;, function(x){ print(x) }) # and use it to print the D symbol datashield.aggregate(conns, quote(print(D))) Another option is to get a symbol value from the server into the client environment. This can be very helpful for complex data structures. The following example illustrates usage of a shortcut function that iterates over all the connection objects and get the corresponding symbol value: # get data represented by symbol D for each DataSHIELD connection data &lt;- getDSLiteData(conns, &quot;D&quot;) # get data represented by symbol D from a specific DataSHIELD connection data1 &lt;- getDSLiteData(conns$sim1, &quot;D&quot;) 14.5 Limitations 14.5.1 Function Parameters Parser The main difference with a regular DSI implementation (such as the one of DSOpal) is that the arguments of the DataSHIELD functional calls are not parsed in DSLite. The only R language element that is inspected and handled is the name of the functions, that are replaced by the ones defined in the DataSHIELD configuration. For instance the following expression, which includes a function call in the formula, is valid for the DSLiteServer but not for Opal: someregression(D$height ~ D$diameter + poly(D$length,3,raw=TRUE)) As a consequence, DataSHIELD R package development can take advantage of DSLite flexibility for speeding development but will never replace testing on a regular DataSHIELD infrastructure using DSOpal. 14.5.2 Server Side Environments For each of the DataSHIELD node, the server side code is evaluated within an environment that has no parent, i.e. detached from the global environment where the client code is executed. Some R functions have a parameter that allows to specify to which environment they apply, for instance assign(), get(), eval(), as.formula(), etc. Their env (or envir) parameter default value is parent.frame() which is the global environment when executed in Opal’s R server, because it is the parent frame of the package’s namespace where the function is defined. In DSLiteServer, the parent frame must be the environment where the server code is evaluated. In order to be consistent between these two execution contexts (Opal R server and DSLiteServer), you must specify the env (or envir) value explicitly to be parent.frame(), which is the parent frame of the block being executed (either the global environment in Opal context, or the environment defined in DSLiteServer). Example of a valid server side piece of code that assigns a value to a symbol in the DataSHIELD server’s environment (being the Opal R server’s global environment or a DSLiteServer’s environment): base::assign(x = &quot;D&quot;, value = someValue, envir = parent.frame()) See also the Advanced R - Environments documentation to learn more about environments. "],
["creating-datashield-packages.html", "15 Creating DataSHIELD packages", " 15 Creating DataSHIELD packages The procedure for the development of a client-side or a server-side package in DataSHIELD is similar as the procedure of creating packages in native R and RStudio. For the devevelopment of a package, the developer will need the R packages devtools and roxygen2. Having those packages installed and loaded in the R session, the developer should follow five basic steps: Create the package directory: this will create the R and the man folders and the DESCRIPTION file in a specified directory. Add functions: copy the R scripts of the developed functions in the R folder created at step (1). Add documentation: add comments at the beginning of each function to give information to the user on what the function does and how the arguments of the function are used. Those comments are compiled into the correct format for the package documentation. More details can be found in the roxygen2 documentation. Create the documentation: this automatically adds in the .Rd files to the man directory and a NAMESPACE file to the main package directory. (see the function document in devtools) Install the development package. In addition to those steps, the developer can make the package a GitHub repo and allows other developers to commit any further developments and improvements. For more details the reader can have a look on a Git/GitHub guide. After the development of a client-server pair of DataSHIELD packages the DataSHIELD Development Team can provide a testing framework where the developed functions and packages are tested including tests related to disclosure protection and if all tests are passed, then the package can be become publicly available for use by the DataSHIELD users. Details for the DataSHIELD testing framework can be found in the DataSHIELD Wiki. "],
["tips-and-tricks.html", "16 Tips and tricks 16.1 How to install R packages into OPAL server from R 16.2 How to check whether there are open R sesions in the OPAL server 16.3 How", " 16 Tips and tricks 16.1 How to install R packages into OPAL server from R 16.2 How to check whether there are open R sesions in the OPAL server 16.3 How "],
["session-info.html", "17 Session Info", " 17 Session Info sessionInfo() R version 3.6.3 (2020-02-29) Platform: x86_64-pc-linux-gnu (64-bit) Running under: Ubuntu 18.04.4 LTS Matrix products: default BLAS: /usr/lib/x86_64-linux-gnu/atlas/libblas.so.3.10.3 LAPACK: /usr/lib/x86_64-linux-gnu/atlas/liblapack.so.3.10.3 locale: [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C [3] LC_TIME=fr_FR.UTF-8 LC_COLLATE=en_US.UTF-8 [5] LC_MONETARY=fr_FR.UTF-8 LC_MESSAGES=en_US.UTF-8 [7] LC_PAPER=fr_FR.UTF-8 LC_NAME=C [9] LC_ADDRESS=C LC_TELEPHONE=C [11] LC_MEASUREMENT=fr_FR.UTF-8 LC_IDENTIFICATION=C attached base packages: [1] stats4 parallel stats graphics grDevices utils datasets [8] methods base other attached packages: [1] dsGeoClient_0.1.0 [2] dsOmicsClient_0.3.0 [3] ssh_0.6 [4] resourcer_1.0.1 [5] dsBaseClient_6.0.0 [6] DSOpal_1.1.0 [7] DSI_1.1.0 [8] R6_2.4.1 [9] progress_1.2.2 [10] opalr_1.4.0 [11] httr_1.4.1 [12] airway_1.4.0 [13] SummarizedExperiment_1.14.1 [14] DelayedArray_0.10.0 [15] BiocParallel_1.18.1 [16] matrixStats_0.54.0 [17] Homo.sapiens_1.3.1 [18] TxDb.Hsapiens.UCSC.hg19.knownGene_3.2.2 [19] org.Hs.eg.db_3.8.2 [20] GO.db_3.8.2 [21] OrganismDbi_1.26.0 [22] GenomicFeatures_1.36.4 [23] GenomicRanges_1.36.1 [24] GenomeInfoDb_1.20.0 [25] AnnotationDbi_1.46.1 [26] IRanges_2.18.3 [27] S4Vectors_0.22.1 [28] tweeDEseqCountData_1.22.0 [29] Biobase_2.44.0 [30] BiocGenerics_0.30.0 [31] snpStats_1.34.0 [32] Matrix_1.2-18 [33] survival_3.1-12 [34] forcats_0.4.0 [35] stringr_1.4.0 [36] dplyr_0.8.5 [37] purrr_0.3.4 [38] readr_1.3.1 [39] tidyr_1.0.3 [40] tibble_3.0.1 [41] ggplot2_3.3.0 [42] tidyverse_1.3.0 [43] kableExtra_1.1.0 [44] BiocStyle_2.12.0 loaded via a namespace (and not attached): [1] readxl_1.3.1 backports_1.1.7 sn_1.5-4 [4] splines_3.6.3 TH.data_1.0-10 digest_0.6.25 [7] htmltools_0.4.0 fansi_0.4.0 magrittr_1.5 [10] memoise_1.1.0 credentials_1.1 Biostrings_2.52.0 [13] modelr_0.1.7 sandwich_2.5-1 askpass_1.1 [16] prettyunits_1.1.1 jpeg_0.1-8 colorspace_1.4-1 [19] blob_1.2.1 rvest_0.3.5 haven_2.2.0 [22] xfun_0.13 crayon_1.3.4 RCurl_1.95-4.12 [25] jsonlite_1.6.1 graph_1.62.0 zoo_1.8-6 [28] glue_1.4.1 gtable_0.3.0 zlibbioc_1.30.0 [31] XVector_0.24.0 webshot_0.5.1 scales_1.0.0 [34] mvtnorm_1.0-10 DBI_1.1.0 bibtex_0.4.2 [37] Rcpp_1.0.4.6 plotrix_3.7-5 metap_1.3 [40] viridisLite_0.3.0 bit_1.1-15.2 TFisher_0.2.0 [43] ellipsis_0.3.1 pkgconfig_2.0.3 XML_3.98-1.19 [46] dbplyr_1.4.3 utf8_1.1.4 tidyselect_0.2.5 [49] rlang_0.4.6 munsell_0.5.0 cellranger_1.1.0 [52] tools_3.6.3 cli_2.0.2 generics_0.0.2 [55] RSQLite_2.2.0 broom_0.5.6 evaluate_0.14 [58] yaml_2.2.0 sys_3.3 knitr_1.28 [61] bit64_0.9-7 fs_1.4.1 RBGL_1.60.0 [64] nlme_3.1-147 mime_0.9 xml2_1.3.2 [67] biomaRt_2.40.5 compiler_3.6.3 rstudioapi_0.11 [70] curl_4.3 png_0.1-7 reprex_0.3.0 [73] stringi_1.4.6 highr_0.8 lattice_0.20-41 [76] multtest_2.42.0 vctrs_0.3.0 mutoss_0.1-12 [79] pillar_1.4.4 lifecycle_0.2.0 BiocManager_1.30.10 [82] Rdpack_0.11-1 bitops_1.0-6 gbRd_0.4-11 [85] rtracklayer_1.44.4 bookdown_0.19 codetools_0.2-16 [88] MASS_7.3-51.6 assertthat_0.2.1 openssl_1.4.1 [91] withr_2.1.2 GenomicAlignments_1.20.1 Rsamtools_2.0.3 [94] mnormt_1.5-5 multcomp_1.4-10 GenomeInfoDbData_1.2.1 [97] hms_0.5.3 grid_3.6.3 rmarkdown_2.1 [100] numDeriv_2016.8-1 lubridate_1.7.4 "],
["contributors.html", "18 Contributors", " 18 Contributors Juan R Gonzalez Yannick Marcon Tom Bishop Demetris Avraam "]
]
